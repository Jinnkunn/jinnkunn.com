[
  {
    "id": "8d6dfeef4c7f4d678b4899d2198877cb",
    "title": "Hi there!",
    "kind": "page",
    "routePath": "/",
    "text": "I am Jinkun Chen (he/him/his), a Ph.D. student (Post-Bachelor‚Äôs, fully funded) studying Computer Science at Dalhousie University under the supervision of Dr. Vlado Keselj. My research primarily focuses on Explainable AI, AI for Science, and Visualization, with a particular emphasis on fairness, on enabling open-ended co-evolution in LM-based agents, and on developing reliable long-term memory in both LLMs and LLM-based agents. While my work is not heavily centered on ocean data, it does occasionally touch on aspects of marine environment analysis as part of my works and broader research interests. Prior to my doctoral studies, I obtained a bachelor‚Äôs degree in Computer Science with First Class Honours from Dalhousie University under the supervision of Dr. Fernando Paulovich, who is now an Associate Professor at Eindhoven University of Technology.\nIn addition to my role as a Researcher Intern at Tsinghua University and Donut Labs, I also serve as an seasonal Instructor at Dalhousie University, a co-founder of Exorcat Technologies Ltd., and the founder of Jinnkunn Consulting Inc.\nI can be reached at jinkun.chen@dal.ca or i@jinkunchen.com. You can also find my profile on LinkedIn. If you are interested in reading my thoughts and insights, you can check my blog, or follow me on X at @_jinnkunn for more updates.\nIn addition to my academic pursuits, I have a diverse range of hobbies and experiences that enrich my life. For instance, I hold an Advanced Open Water (AOW) diver certification, am currently training as a fixed-wing aircraft pilot, and find creative expression through playing the Chinese Bamboo Flute. Additionally, I have prior experience serving as an anchor on Bilibili, a popular video-sharing platform in China.\nLastly, if you happen to speak Chinese, my Chinese name is ÈôàÊôãÂù§, which is pronounced as ch√©n j√¨n k≈´n. My family name is 'Chen (Èôà)', and we belong to the Yimen Chen lineage of the Yiling branch (‰πâÈó®ÈôàËçÜÂ∑ûÂ∫ÑÂ§∑ÈôµÊîØÁ≥ª), with 'Jin (Êôã)' as my generational designation.\nGoogle Scholar | ResearchGate | ORCID | Semantic Scholar\n2026\n2026\nUnified Minimax Optimization Framework for Propensity Score Estimation in Debiased Recommendation [oral] conference\nC. Zheng; H. Yang, J. Chen, S. Zhang, T. Xia conference: The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26)\n2025\n2025\nStatic Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations arXiv.org\nJ. Chen, S. Badshah, X. Yu, S. Han arXiv.org: Available at: https://arxiv.org/abs/2510.13982\nAddressing Correlated Latent Exogenous Variables in Debiased Recommender Systems [oral] conference arXiv.org\nS. Zhang, Y. Zhang, J. Chen, H. Sui conference: 31st SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) arXiv.org: Available at: https://www.arxiv.org/abs/2506.07517\n2024\n2024\nCAB-KWS : Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology [oral] conference journal arXiv.org\nW. Dai, Y. Jiang, Y. Liu, J. Chen, X. Sun, and J. Tao conference: 27th International Conference on Pattern Recognition (ICPR) journal: Pattern Recognition. Lecture Notes in Computer Science, vol 15303. Springer, Cham. dio: https://doi.org/10.1007/978-3-031-78122-3_7. arXiv.org: Available at: https://arxiv.org/abs/2409.00356.\nMaritime tracking data analysis and integration with AISDB journal arXiv.org\nG. Spadon, J. Kumar, J. Chen, M. Smith, C. Hilliard, S. Vela, R. Gehrmann, C. DiBacco, S. Matwin and R. Pelot journal: SoftwareX, vol 28, 101952. Elsevier. dio: 10.1016/j.softx.2024.101952 arXiv.org: Available at: https://arxiv.org/abs/2407.08082\nLong-form evaluation of model editing conference arXiv.org\nD. Rosati, R. Gonzales, J. Chen, X. Yu, Y. Kayani, F. Rudzicz and H. Sajjad conference: 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), Mexico City, Mexico, 2024, dio: 10.18653/v1/2024.naacl-long.208. arXiv.org: Available at: https://arxiv.org/abs/2402.09394\n2023 and Before\n2023 and Before\nAddressing Unintended Bias in Toxicity Detection: An LSTM and Attention-Based Approach conference\nW. Dai, J. Tao, X. Yan, Z. Feng and J. Chen conference: 2023 5th International Conference on Artificial Intelligence and Computer Applications (ICAICA), Dalian, China, 2023, pp. 375-379, doi: 10.1109/ICAICA58456.2023.10405429.\nTransitive Halifax: An Activity-Based Search Engine for Bus Routes conference\nJ. Chen, V. M. de Lira, F. V. Paulovich and A. Soares conference: 2021 22nd IEEE International Conference on Mobile Data Management (MDM), Toronto, ON, Canada, 2021, pp. 233-235, doi: 10.1109/MDM52706.2021.00045.\nFor the moment, there will only be a listing of teaching activities at Dalhousie University.\nArchived Course Pages | Rate My Professors\n2024/25 Winter Term Jan 2025 - April 2025 Marker for CSCI5408 (Data Management, Warehousing, and Analytics, Dr. Gabriel Spadon)\n2024/25 Fall Term Sep 2024 - Dec 2024 Instructor for CSCI3141 (Foundations/Data Science)\n2023/24 Winter Term Jan 2024 - April 2024 Teaching Assistant for CSCI2170 (Introduction to Server Side Scripting, Dr. Bonnie MacKay)\n2023/24 Fall Term Sep 2023 - Dec 2023 Teaching Assistant for CSCI3160 (Design User Interfaces, Dr. Derek Reilly)\n2022/23 Winter Term Jan 2023 - April 2023 Teaching Assistant for CSCI3160 (Design User Interfaces, Dr. Bonnie MacKay)\n2022/23 Summer Term May 2022 - Aug 2022 Marker for CSCI3151 (Foundations of Machine Learning, Dr. Sageev Oore)\nNo personal information (such as email) will be collected.\nThis page serves as a repository of archived course materials and resources.\n2024/25 Fall\n2024/25 Fall\nCSCI3141 - Foundations/Data Science - Dalhousie University\nFoundations/Data Science\nFoundations/Data Science\n2024/2025 Fall Faculty of Computer Science Dalhousie University, Halifax, Nova Scotia, Canada\nIn Memoriam: Dr. Luis Torgo\nThis course is deeply inspired by the teachings and work of Dr. Luis Torgo, whose invaluable contributions to the field of Data Science have shaped much of its content. Dr. Torgo was a distinguished scholar, educator, and mentor who dedicated his career to advancing our understanding of data analysis and predictive modeling. His passion for learning, teaching, and innovation continues to guide us.\nAs we navigate this course, we honor Dr. Torgo‚Äôs legacy and express our gratitude for his profound impact on the field and on all those fortunate enough to learn from him. His work remains a cornerstone of our studies, and his memory lives on through the knowledge he shared with the world.\nBrightSpace | Course Calendar\nOffice Hours Appointment | Anonymous Feedback\nCourse Description\nCourse Description\nThis course provides an introductory overview of the key steps in a typical data science project, using the R programming language and environment as the primary tool. It aims to: (i) introduce students to foundational data science methods and processes, which can be explored further at the graduate level; and (ii) familiarize students with one of the essential tools in data science, the R environment, and programming language.\nDesigned for students without a strong technical background in programming or computer science, the course is suitable for both Computer Science students and those from other disciplines. The approach is predominantly practical and case-study oriented, supplemented by essential theoretical concepts to provide a solid understanding.\nThroughout the course, students will gain hands-on experience with R, learning to navigate the stages of a data science project‚Äîfrom data importation and manipulation to analysis, model building, reporting, and deployment. They will learn to tackle the primary challenges encountered in data science using R, acquiring the skills and knowledge necessary to conduct data-driven projects effectively.\nSyllabus\nSyllabus\nCourse Calendar\nCourse Calendar\nView it in a separate page\nFor all of my teaching works, please refer to the Teaching page.\nRecent Works\nRecent Works\nIntern Donut Browser (Donut Labs), Remote Nov 2025 - Now\nPart-time Instructor Orange Education Ltd., Halifax, NS, Canada Sep 2019 - Now\nI have experience teaching a variety of Computer Science-related courses, and I'm proud to have been recognized for my hard work and dedication in this field. Specifically, I was awarded the prestigious Gold Medal Instructor title, which speaks to my ability to effectively engage with and educate my students on complex concepts related to Computer Science and Statistics. I am also in charge of some administration work, like interviewing potential instructors.c\nPassed Works\nPassed Works\nAIS Data Analyst, Project AISViz MERIDIAN, Dalhousie University, Halifax, NS, Canada May 2023 - Aug 2025\nOur main objective is to build an easy-to-use open-source toolbox for raw AIS data extraction, processing, visualization, and vessel modeling. We aim to make AIS data accessible and comprehensible for a broad spectrum of users, from government bodies and policymakers to university researchers, non-government organizations (NGOs), coastal communities, and the general public. AISviz was proposed to implement machine learning applications that work seamlessly with flexible data sources from terrestrial and satellite streams of AIS data. Besides, we intend to make the interaction with AIS data user-friendly, achieved through developing a Graphical User Interface (GUI). This approach will simplify data retrieval, integration, and basic manipulation processes. We also aim to enhance the applicability of AIS data by incorporating its integration with different maritime raster-based third-party data sources. The project and all its components will be made openly accessible, promoting collaboration, transparency, and public contribution. The project is under the auspices of MALNIS Lab and MERIDIAN, and funded by Fisheries and Oceans Canada (DFO) of the Government of Canada. GitHub | AISdb Documentation | AISViz Project Website\nResearch Assistant, Project Ballast Dalhousie University, Halifax, NS, Canada Feb 2021 - Mar 2023\nBallast project is one where I am serving as a Research Assistant at Institute for Big Data Analytics, Dalhousie University. It involves building a web-based application that examines commercial shipping activities as an environmental stressor. The project involves analyzing and visualizing GIS (Geographic Information System) data. This project is supported by the Ocean and Freshwater Science Contribution Program, which is hosted by Fisheries and Oceans Canada (DFO).\nPresident UNICEF Dalhousie, Halifax, NS, Canada Sep 2018 - Aug 2020\nUNICEF Dalhousie serves as the official representative of the United Nations Children's Fund (UNICEF) at Dalhousie University, as part of the UNICEF Canada Campus Clubs program. As a recognized campus club, UNICEF Dalhousie works to promote and support the mission of UNICEF, which is to protect and advocate for the rights of children and to help them reach their full potential. Through various initiatives and events, UNICEF Dalhousie aims to raise awareness and funds for UNICEF's programs and initiatives, and to engage the Dalhousie community in efforts to improve the lives of children around the world.\nTechnical Director Sino Gateway Media & Marketing Ltd., Halifax, NS, Canada Oct 2017 - Sep 2018\nI was actively involved in contributing to the design, development, and operation of Hafaquanzhidao(ÂìàÊ≥ïÂÖ®Áü•ÈÅì), which is an innovative information and life-sharing platform built as a WeChat mini app. This unique platform is catered towards the Chinese community residing in Halifax, offering them an exclusive digital space to share information and experiences related to their lives in the region. In addition, my team and I play a crucial role in one of the company's secondary revenue streams, which involves providing digital solutions to a diverse range of business clients.\nTechnical Director YiGuang Media Ltd., Halifax, NS, Canada Oct 2016 - Sep 2017\nOne of the company's primary endeavors during that period was managing an online forum called ChineseHalifax(ÂìàÊ≥ïÂçé‰∫∫ÁΩë), which catered to the needs of Chinese residents living in Halifax. My ability to get up to speed with the job quickly was due to the fact that the platform used the same technical stack that I used and operated in 2013 for an anime forum.\nThis list is not exhaustive, and maybe not up-to-date\nRSS Feed\nJinkun Chen (he/him/his) is a Ph.D. student in Computer Science at Dalhousie University, specializing in Explainable AI, Natural Language Processing (NLP), and Visualization. He earned a bachelor's degree in Computer Science with First-Class Honours from Dalhousie University. Jinkun is actively involved in research, working on advancing fairness, responsibility, trustworthiness, and explainability within Large Language Models (LLMs) and AI. In addition to his academic pursuits, Jinkun also serves as an AIS Data Analyst at MERIDIAN and is a valuable member of the HyperMatrix Lab and MALNIS Lab, all of which contribute to his research-related activities.\nüè¢ Academic Affiliations\nüè¢ Academic Affiliations\nModeling and Analytics on Predictive Systems (MAPS) Lab\nMaritime Risk and Safety Research Group (MARS)\nMERIDIAN, Institute for Big Data Analytics\nüë®‚Äçüéì Education\nüë®‚Äçüéì Education\nOn-Campus\nOn-Campus\nPhD of Computer Science (Post Bachelor‚Äôs) Dalhousie University Jan 2022 - Now\nBachelor of Computer Science (First-Class Honour) Dalhousie University Sept 2016 - Sept 2020 Minor in Math Certified in Data Science and Data Analytics Honour Advisor: Dr. Fernando Paulovich Honour Thesis Reader: Dr. Amilcar Soares\nOff-Campus\nOff-Campus\nProject Management Professional (PMP) May 2023 - Now\nPrivate Pilot License (PPL) Feb 2024 - Now\nüöÄ Certification\nüöÄ Certification\nTCPS2\nAdvanced Open Water Driver\nThis page helps verify my online identity and provides ways to connect with me.\nüåê Online Identities\nüåê Online Identities\nThese domains and email addresses are my official digital identities. If you receive communication from these sources, you can verify it's genuinely from me.\nüìß Email Addresses\nüìß Email Addresses\njinkun.chen@dal.ca ‚Äî Academic & Dalhousie\ni@jinkunchen.com ‚Äî Personal\njinkun.chen@exorcat.com ‚Äî Exorcat\njinkun@donutbrowser.ai ‚Äî Donut Browser\njinkunc@acm.org ‚Äî ACM\nüîó Domains\nüîó Domains\njinkunchen.com ‚Äî Primary website\nchenjinkun.com\njinnkunn.com\njinnkunn.eth ‚Äî Web3\njinkun.blue ‚Äî Web3\nüí¨ Social Media\nüí¨ Social Media\nI'm active on these platforms for professional networking and sharing research updates.\nLinkedIn ‚Äî Professional profile\nX/Twitter ‚Äî Research updates & thoughts\nGitHub ‚Äî Code & projects\n\"ÁôæÁä¨ÂêåÊßΩËÄåÈ£üÔºå‰∏ÄÁä¨‰∏çËá≥ÔºåÁôæÁä¨‰∏çÈ£ü\" \"A hundred dogs share the same trough; if one is absent, the rest will not eat.\" This ancient saying embodies the extraordinary unity of the Yimen Chen family.\nüìñ Introduction\nüìñ Introduction\nThe Yimen Chen (‰πâÈó®Èôà) lineage represents one of the most extraordinary examples of collective living and Confucian family values in Chinese history. For over three centuries (731-1062 CE), this remarkable family maintained unity across fifteen generations, with thousands of members sharing property, meals, and daily life under a single household system.\nAt its peak, the Yimen Chen family comprised over 3,900 individuals living together without private property or separate kitchens. Their achievement attracted imperial recognition from successive dynasties and praise from renowned scholars including Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi.\nThe family upheld core Confucian values of loyalty, filial piety, integrity, and righteousness. Their governance system, codified in the famous \"33 Family Rules,\" became a model for family management throughout China. The saying \"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" (\"All Chen families under heaven originate from Jiangzhou\") reflects their lasting influence, as descendants from the 1062 CE division now inhabit communities across China and beyond.\nüèõÔ∏è Historical Origins\nüèõÔ∏è Historical Origins\nFounded: 731 CE (Tang Dynasty, Kaiyuan 19th year) Location: Putang Village, Jiangzhou (present-day Yimen Village, De'an County, Jiangxi Province) Founder: Chen Wang (ÈôàÊó∫)\nThe lineage traces its ancestry to Chen Baxian (ÈôàÈú∏ÂÖà), the founding emperor of the Chen Dynasty (557-589 CE). Chen Wang, a descendant of Chen Baxian's brother Chen Tanxian, established the family settlement in what would become known as Yimen (Áæ©ÈñÄ, \"Righteous Gate\").\nüìú Imperial Recognition In 884 CE (Tang Dynasty, Zhonghe 4th year), Emperor Xizong personally bestowed the title \"Yimen Chen Clan\" (‰πâÈó®ÈôàÊ∞è) upon the family, marking the beginning of imperial recognition that would continue through successive dynasties.\nüìú Imperial Recognition In 884 CE (Tang Dynasty, Zhonghe 4th year), Emperor Xizong personally bestowed the title \"Yimen Chen Clan\" (‰πâÈó®ÈôàÊ∞è) upon the family, marking the beginning of imperial recognition that would continue through successive dynasties.\nüåü Remarkable Achievements\nüåü Remarkable Achievements\nThe Yimen Chen family created an unprecedented model of collective living that astonished the entire nation and was celebrated by emperors and scholars alike.\nKey Facts\nKey Facts\nDuration: 15 generations living together (332 years, approximately 731-1062 CE)\nPopulation: Grew from 700+ members (947-957 CE) to 3,900+ members by 1062 CE)\nShared Living: All members shared meals from a common kitchen and held no private property)\nCore Principles: \"ÂÆ§Êó†ÁßÅË¥¢ÔºåÂé®Êó†Âà´Áà®\" (No private wealth in rooms; no separate cooking in kitchens)\n‚öñÔ∏è Governance System\n‚öñÔ∏è Governance System\nThe Yimen Chen family established a sophisticated governance system characterized by fairness, education, and moral cultivation.\n33 Family Rules (ÈôàÊ∞èÂÆ∂Ê≥ï)\n33 Family Rules (ÈôàÊ∞èÂÆ∂Ê≥ï)\nThe family's 33 rules (ÈôàÊ∞èÂÆ∂Ê≥ï) were so well-regarded that in 1026 CE, Emperor Renzong of Song ordered them to be preserved in the Imperial Archives and distributed to all high officials as a model for family governance.\nThese rules emphasized:\nFilial piety and respect for elders\nCollective property and equitable distribution\nDiligence and frugality\nEducation and moral cultivation\n‰∏ú‰Ω≥‰π¶Èô¢ (Dongjia Academy)\n‰∏ú‰Ω≥‰π¶Èô¢ (Dongjia Academy)\nThe family founded the Dongjia Academy (‰∏ú‰Ω≥‰π¶Èô¢), one of China's earliest private higher education institutions. The academy was renowned for its extensive collection of books and calligraphy, described as unparalleled in the region.\n\"ÂÖ´ÁôæÂ§¥ÁâõËÄïÊó•ÊúàÔºå‰∏âÂçÉÁÅØÁÅ´ËØªÊñáÁ´†\" \"Eight hundred oxen plow day and night; three thousand lanterns illuminate the study of literature.\" ‚Äî Song Dynasty official Lv Duan, describing the family's prosperity and dedication to education\nüåç The Great Division (1062 CE)\nüåç The Great Division (1062 CE)\nIn 1062 CE (Song Dynasty, Jiayou 7th year), after the family population exceeded 3,900 members, Emperor Renzong‚Äîurged by prominent ministers including Wen Yanbo (ÊñáÂΩ¶Âçö) and Bao Zheng (ÂåÖÊãØ)‚Äîissued an imperial decree for the family to divide.\nThe Division Process\nThe Division Process\nThe division took nine months to plan (July 1062 to March 1063). The family property was divided into:\n291 estates spread across 16 provinces and 125 counties\nEach branch drew lots to determine their new locations\nBranches used a 12-character generational naming system bestowed by Emperor Taizong: \"Áü•ÂÆàÂÆó„ÄÅÂ∏åÂÖ¨Ê±ù„ÄÅÊâçÊÄùÂΩ¶„ÄÅÊâøÂª∂Áªß\"\n\"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" \"All Chen families under heaven originate from Jiangzhou\" This saying reflects the far-reaching influence of the Yimen Chen division. Descendants settled across China, with each family displaying a \"Yimen Shijia\" (‰πâÈó®‰∏ñÂÆ∂, \"Yimen Noble Family\") plaque above their doors.\n\"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" \"All Chen families under heaven originate from Jiangzhou\" This saying reflects the far-reaching influence of the Yimen Chen division. Descendants settled across China, with each family displaying a \"Yimen Shijia\" (‰πâÈó®‰∏ñÂÆ∂, \"Yimen Noble Family\") plaque above their doors.\nüåü Modern Legacy\nüåü Modern Legacy\nToday, the Yimen Chen lineage continues to be a source of pride for millions of descendants worldwide.\nNotable Descendants\nNotable Descendants\nMany prominent figures in modern Chinese history trace their ancestry to the Yimen Chen division, including:\nChen Duxiu (ÈôàÁã¨ÁßÄ) - Co-founder of the Chinese Communist Party\nChen Yi (ÈôàÊØÖ) - Marshal of the People's Republic of China\nChen Yun (Èôà‰∫ë) - Former Vice Premier of China\nChen Geng (ÈôàËµì) - General in the People's Liberation Army\nCultural Preservation\nCultural Preservation\nEfforts to preserve and celebrate the Yimen Chen heritage include:\nAnnual family reunions bringing together descendants from around the world\nRestoration of ancestral halls in Yimen Village, Jiangxi Province\nPublication of genealogical records documenting the 291 branches and their migrations\nAcademic research into the family's unique governance system and Confucian values\nHistorical Significance The Yimen Chen family stands as a unique chapter in Chinese history. A testament to the power of Confucian values, collective living, and family harmony. Their achievement of maintaining unity across 15 generations and 3,900+ members remains unparalleled in recorded history. Renowned scholars such as Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi all praised the Yimen Chen family in their writings, spreading their fame throughout the nation and establishing them as a model for family governance and moral cultivation.\nHistorical Significance The Yimen Chen family stands as a unique chapter in Chinese history. A testament to the power of Confucian values, collective living, and family harmony. Their achievement of maintaining unity across 15 generations and 3,900+ members remains unparalleled in recorded history. Renowned scholars such as Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi all praised the Yimen Chen family in their writings, spreading their fame throughout the nation and establishing them as a model for family governance and moral cultivation.\n2025/12/18\n2025/12/18\nI will be serving as a reviewer for ICML 2026.\n2025/11/11\n2025/11/11\nI‚Äôm pleased to share that I‚Äôve recently joined Donut Labs (Donut Browser AI), a startup supported by Sequoia Capital and several other leading investors with a total of 22 million US dollars in Pre Seed and Seed funding. I am contributing to the development of its agentic crypto trading browser, a system designed to perceive markets and reason like a quant. My work focuses on applying my research in LLM reasoning and agentic systems to real world trading environments.\n2025/11/07\n2025/11/07\nI‚Äôm pleased to share that our recent work, Unified Minimax Optimization Framework for Propensity Score Estimation in Debiased Recommendation, has been accepted for an oral presentation at AAAI 2026 Main Technical Track.\n2025/09/22\n2025/09/22\nI will be serving as a reviewer for ICLR 2026.\n2025/08/01\n2025/08/01\nI will be serving as a reviewer for AAAI 2026.\n2025/06/20\n2025/06/20\nI will be serving as a reviewer for the XLLM-Reason-Plan Workshop, and Workshop on AI Agents: Capabilities and Safety at COLM 2025.\n2025/05/21\n2025/05/21\nOur paper, \"Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems\", has been accepted for an oral presentation at KDD 2025.\n2024/12/30\n2024/12/30\nAs the new year approaches, I‚Äôm excited to share a personal milestone‚Äîmy research papers have been cited over 100 times üéâ! It‚Äôs a great way to wrap up the year and look ahead to more learning and contributions in 2025.\n2024/12/24\n2024/12/24\nüéâ Excited to share a significant milestone in my journey as an educator! Teaching CSCI3141 (Foundations/Data Science) as my first course as an Instructor at Dalhousie University was an incredibly rewarding experience. The course received a perfect 5/5 student rating on Rate My Professors and achieved a 0% drop rate throughout the term."
  },
  {
    "id": "c277f772b85a4e8d85938cd07ac85f94",
    "title": "Publications",
    "kind": "page",
    "routePath": "/publications",
    "text": "Google Scholar | ResearchGate | ORCID | Semantic Scholar\n2026\n2026\nUnified Minimax Optimization Framework for Propensity Score Estimation in Debiased Recommendation [oral] conference\nC. Zheng; H. Yang, J. Chen, S. Zhang, T. Xia conference: The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26)\n2025\n2025\nStatic Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations arXiv.org\nJ. Chen, S. Badshah, X. Yu, S. Han arXiv.org: Available at: https://arxiv.org/abs/2510.13982\nAddressing Correlated Latent Exogenous Variables in Debiased Recommender Systems [oral] conference arXiv.org\nS. Zhang, Y. Zhang, J. Chen, H. Sui conference: 31st SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) arXiv.org: Available at: https://www.arxiv.org/abs/2506.07517\n2024\n2024\nCAB-KWS : Contrastive Augmentation: An Unsupervised Learning Approach for Keyword Spotting in Speech Technology [oral] conference journal arXiv.org\nW. Dai, Y. Jiang, Y. Liu, J. Chen, X. Sun, and J. Tao conference: 27th International Conference on Pattern Recognition (ICPR) journal: Pattern Recognition. Lecture Notes in Computer Science, vol 15303. Springer, Cham. dio: https://doi.org/10.1007/978-3-031-78122-3_7. arXiv.org: Available at: https://arxiv.org/abs/2409.00356.\nMaritime tracking data analysis and integration with AISDB journal arXiv.org\nG. Spadon, J. Kumar, J. Chen, M. Smith, C. Hilliard, S. Vela, R. Gehrmann, C. DiBacco, S. Matwin and R. Pelot journal: SoftwareX, vol 28, 101952. Elsevier. dio: 10.1016/j.softx.2024.101952 arXiv.org: Available at: https://arxiv.org/abs/2407.08082\nLong-form evaluation of model editing conference arXiv.org\nD. Rosati, R. Gonzales, J. Chen, X. Yu, Y. Kayani, F. Rudzicz and H. Sajjad conference: 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), Mexico City, Mexico, 2024, dio: 10.18653/v1/2024.naacl-long.208. arXiv.org: Available at: https://arxiv.org/abs/2402.09394\n2023 and Before\n2023 and Before\nAddressing Unintended Bias in Toxicity Detection: An LSTM and Attention-Based Approach conference\nW. Dai, J. Tao, X. Yan, Z. Feng and J. Chen conference: 2023 5th International Conference on Artificial Intelligence and Computer Applications (ICAICA), Dalian, China, 2023, pp. 375-379, doi: 10.1109/ICAICA58456.2023.10405429.\nTransitive Halifax: An Activity-Based Search Engine for Bus Routes conference\nJ. Chen, V. M. de Lira, F. V. Paulovich and A. Soares conference: 2021 22nd IEEE International Conference on Mobile Data Management (MDM), Toronto, ON, Canada, 2021, pp. 233-235, doi: 10.1109/MDM52706.2021.00045."
  },
  {
    "id": "e3a47d49c220484ab33773c1d010dd69",
    "title": "Teaching",
    "kind": "page",
    "routePath": "/teaching",
    "text": "For the moment, there will only be a listing of teaching activities at Dalhousie University.\nArchived Course Pages | Rate My Professors\n2024/25 Winter Term Jan 2025 - April 2025 Marker for CSCI5408 (Data Management, Warehousing, and Analytics, Dr. Gabriel Spadon)\n2024/25 Fall Term Sep 2024 - Dec 2024 Instructor for CSCI3141 (Foundations/Data Science)\n2023/24 Winter Term Jan 2024 - April 2024 Teaching Assistant for CSCI2170 (Introduction to Server Side Scripting, Dr. Bonnie MacKay)\n2023/24 Fall Term Sep 2023 - Dec 2023 Teaching Assistant for CSCI3160 (Design User Interfaces, Dr. Derek Reilly)\n2022/23 Winter Term Jan 2023 - April 2023 Teaching Assistant for CSCI3160 (Design User Interfaces, Dr. Bonnie MacKay)\n2022/23 Summer Term May 2022 - Aug 2022 Marker for CSCI3151 (Foundations of Machine Learning, Dr. Sageev Oore)\nNo personal information (such as email) will be collected.\nThis page serves as a repository of archived course materials and resources.\n2024/25 Fall\n2024/25 Fall\nCSCI3141 - Foundations/Data Science - Dalhousie University\nFoundations/Data Science\nFoundations/Data Science\n2024/2025 Fall Faculty of Computer Science Dalhousie University, Halifax, Nova Scotia, Canada\nIn Memoriam: Dr. Luis Torgo\nThis course is deeply inspired by the teachings and work of Dr. Luis Torgo, whose invaluable contributions to the field of Data Science have shaped much of its content. Dr. Torgo was a distinguished scholar, educator, and mentor who dedicated his career to advancing our understanding of data analysis and predictive modeling. His passion for learning, teaching, and innovation continues to guide us.\nAs we navigate this course, we honor Dr. Torgo‚Äôs legacy and express our gratitude for his profound impact on the field and on all those fortunate enough to learn from him. His work remains a cornerstone of our studies, and his memory lives on through the knowledge he shared with the world.\nBrightSpace | Course Calendar\nOffice Hours Appointment | Anonymous Feedback\nCourse Description\nCourse Description\nThis course provides an introductory overview of the key steps in a typical data science project, using the R programming language and environment as the primary tool. It aims to: (i) introduce students to foundational data science methods and processes, which can be explored further at the graduate level; and (ii) familiarize students with one of the essential tools in data science, the R environment, and programming language.\nDesigned for students without a strong technical background in programming or computer science, the course is suitable for both Computer Science students and those from other disciplines. The approach is predominantly practical and case-study oriented, supplemented by essential theoretical concepts to provide a solid understanding.\nThroughout the course, students will gain hands-on experience with R, learning to navigate the stages of a data science project‚Äîfrom data importation and manipulation to analysis, model building, reporting, and deployment. They will learn to tackle the primary challenges encountered in data science using R, acquiring the skills and knowledge necessary to conduct data-driven projects effectively.\nSyllabus\nSyllabus\nCourse Calendar\nCourse Calendar\nView it in a separate page"
  },
  {
    "id": "4a048f29599144b18aeaeae8eda733df",
    "title": "Appointment",
    "kind": "page",
    "routePath": "/teaching/appointment",
    "text": ""
  },
  {
    "id": "6b59278ee8c3450081a792ae1be461fe",
    "title": "Feedback",
    "kind": "page",
    "routePath": "/teaching/feedback",
    "text": "No personal information (such as email) will be collected."
  },
  {
    "id": "17040d70fdf58035acc1e820aa3a4809",
    "title": "Archive",
    "kind": "page",
    "routePath": "/teaching/archive",
    "text": "This page serves as a repository of archived course materials and resources.\n2024/25 Fall\n2024/25 Fall\nCSCI3141 - Foundations/Data Science - Dalhousie University\nFoundations/Data Science\nFoundations/Data Science\n2024/2025 Fall Faculty of Computer Science Dalhousie University, Halifax, Nova Scotia, Canada\nIn Memoriam: Dr. Luis Torgo\nThis course is deeply inspired by the teachings and work of Dr. Luis Torgo, whose invaluable contributions to the field of Data Science have shaped much of its content. Dr. Torgo was a distinguished scholar, educator, and mentor who dedicated his career to advancing our understanding of data analysis and predictive modeling. His passion for learning, teaching, and innovation continues to guide us.\nAs we navigate this course, we honor Dr. Torgo‚Äôs legacy and express our gratitude for his profound impact on the field and on all those fortunate enough to learn from him. His work remains a cornerstone of our studies, and his memory lives on through the knowledge he shared with the world.\nBrightSpace | Course Calendar\nOffice Hours Appointment | Anonymous Feedback\nCourse Description\nCourse Description\nThis course provides an introductory overview of the key steps in a typical data science project, using the R programming language and environment as the primary tool. It aims to: (i) introduce students to foundational data science methods and processes, which can be explored further at the graduate level; and (ii) familiarize students with one of the essential tools in data science, the R environment, and programming language.\nDesigned for students without a strong technical background in programming or computer science, the course is suitable for both Computer Science students and those from other disciplines. The approach is predominantly practical and case-study oriented, supplemented by essential theoretical concepts to provide a solid understanding.\nThroughout the course, students will gain hands-on experience with R, learning to navigate the stages of a data science project‚Äîfrom data importation and manipulation to analysis, model building, reporting, and deployment. They will learn to tackle the primary challenges encountered in data science using R, acquiring the skills and knowledge necessary to conduct data-driven projects effectively.\nSyllabus\nSyllabus\nCourse Calendar\nCourse Calendar\nView it in a separate page"
  },
  {
    "id": "17040d70fdf580c29811f438d7cced08",
    "title": "2024/25 Fall",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall",
    "text": "Foundations/Data Science\nFoundations/Data Science\n2024/2025 Fall Faculty of Computer Science Dalhousie University, Halifax, Nova Scotia, Canada\nIn Memoriam: Dr. Luis Torgo\nThis course is deeply inspired by the teachings and work of Dr. Luis Torgo, whose invaluable contributions to the field of Data Science have shaped much of its content. Dr. Torgo was a distinguished scholar, educator, and mentor who dedicated his career to advancing our understanding of data analysis and predictive modeling. His passion for learning, teaching, and innovation continues to guide us.\nAs we navigate this course, we honor Dr. Torgo‚Äôs legacy and express our gratitude for his profound impact on the field and on all those fortunate enough to learn from him. His work remains a cornerstone of our studies, and his memory lives on through the knowledge he shared with the world.\nBrightSpace | Course Calendar\nOffice Hours Appointment | Anonymous Feedback\nCourse Description\nCourse Description\nThis course provides an introductory overview of the key steps in a typical data science project, using the R programming language and environment as the primary tool. It aims to: (i) introduce students to foundational data science methods and processes, which can be explored further at the graduate level; and (ii) familiarize students with one of the essential tools in data science, the R environment, and programming language.\nDesigned for students without a strong technical background in programming or computer science, the course is suitable for both Computer Science students and those from other disciplines. The approach is predominantly practical and case-study oriented, supplemented by essential theoretical concepts to provide a solid understanding.\nThroughout the course, students will gain hands-on experience with R, learning to navigate the stages of a data science project‚Äîfrom data importation and manipulation to analysis, model building, reporting, and deployment. They will learn to tackle the primary challenges encountered in data science using R, acquiring the skills and knowledge necessary to conduct data-driven projects effectively.\nSyllabus\nSyllabus\nCourse Calendar\nCourse Calendar\nView it in a separate page"
  },
  {
    "id": "17040d70fdf581d0b439cbbbc2871f5a",
    "title": "CSCI3141",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141",
    "text": "Foundations/Data Science\nFoundations/Data Science\n2024/2025 Fall Faculty of Computer Science Dalhousie University, Halifax, Nova Scotia, Canada\nIn Memoriam: Dr. Luis Torgo\nThis course is deeply inspired by the teachings and work of Dr. Luis Torgo, whose invaluable contributions to the field of Data Science have shaped much of its content. Dr. Torgo was a distinguished scholar, educator, and mentor who dedicated his career to advancing our understanding of data analysis and predictive modeling. His passion for learning, teaching, and innovation continues to guide us.\nAs we navigate this course, we honor Dr. Torgo‚Äôs legacy and express our gratitude for his profound impact on the field and on all those fortunate enough to learn from him. His work remains a cornerstone of our studies, and his memory lives on through the knowledge he shared with the world.\nBrightSpace | Course Calendar\nOffice Hours Appointment | Anonymous Feedback\nCourse Description\nCourse Description\nThis course provides an introductory overview of the key steps in a typical data science project, using the R programming language and environment as the primary tool. It aims to: (i) introduce students to foundational data science methods and processes, which can be explored further at the graduate level; and (ii) familiarize students with one of the essential tools in data science, the R environment, and programming language.\nDesigned for students without a strong technical background in programming or computer science, the course is suitable for both Computer Science students and those from other disciplines. The approach is predominantly practical and case-study oriented, supplemented by essential theoretical concepts to provide a solid understanding.\nThroughout the course, students will gain hands-on experience with R, learning to navigate the stages of a data science project‚Äîfrom data importation and manipulation to analysis, model building, reporting, and deployment. They will learn to tackle the primary challenges encountered in data science using R, acquiring the skills and knowledge necessary to conduct data-driven projects effectively.\nSyllabus\nSyllabus\nCourse Calendar\nCourse Calendar\nView it in a separate page"
  },
  {
    "id": "17040d70fdf5816ca4ccdb1e97184183",
    "title": "Timeline",
    "kind": "database",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline",
    "text": "Timeline\n‚è∞¬†Project Due\nAssignment Due\nüöÄ¬†Exam Period üöÄ\nMONDAY CLASS WILL BE HELD\nüí™¬†Final Exam\nFinal Exam Review üéÜ The Last Lecture\nAssignment4 Posted\nHierarchical Clustering & Outlier Detection\nOnline Q&A Session\nMidterm Exam Grade Released\nAssignment3 Due\nProject Dataset Selection Due\nüçÅ FALL STUDY BREAK üçÅ\nPartition-Based Clustering\nLab10 Sec01\nDescriptive Analytics\nLab10 Sec02\nAssignment3 Posted\nModel Evaluation and Selection\nLab9 Sec01\nTree-Based Models\nLab9 Sec02\nAssignment2 Due\nSVM\nLinear Models\nLab8 Sec01\nLab8 Sec02\nPredictive Analytics\nGet Data from Real-Time Data Feed\nLeb7 Sec01\nAssignment2 Posted\nLab7 Sec02\nLab6 Sec01\nWeb Scraping in R: rvest, httr, and jsonlite\nBasic Web Component Review\nLab6 Sec02\nAssignment 1 Due (Extended)\nüí™ Midterm Exam\nMidterm Review\nLab5 Sec02\nLab5 Sec01\nData Visualization\nLab3 Sec01\nLab4 Sec01\nData Summarization\nLab3 Sec02\nLab4 Sec02\nAssignment 1 Posted\nFinal Exam/Project Selection Due\nProject Description Posted\nData Pre-Processing\nData Manipulation\nLab2 Sec01\nLab2 Sec02\nData Structure\nIntroduction to R Language\nLab1 Sec01\nLab1 Sec02\nIntroduction to Data Science\nFirst Class, Let‚Äôs Know Each Other!"
  },
  {
    "id": "17040d70fdf581a3ad9ffb8cd457c8ab",
    "title": "‚è∞¬†Project Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/project-due",
    "text": ""
  },
  {
    "id": "17040d70fdf58153aaf1c5c9e9ec2d77",
    "title": "Assignment Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment-due",
    "text": ""
  },
  {
    "id": "17040d70fdf58106b9bce6d1bbd15b1c",
    "title": "üöÄ¬†Exam Period üöÄ",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/exam-period",
    "text": ""
  },
  {
    "id": "17040d70fdf581388aa0e28a9cb23fae",
    "title": "MONDAY CLASS WILL BE HELD",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/monday-class-will-be-held",
    "text": ""
  },
  {
    "id": "17040d70fdf5813884dccac1d2818b60",
    "title": "üí™¬†Final Exam",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/final-exam",
    "text": ""
  },
  {
    "id": "17040d70fdf5810e8858db811123b892",
    "title": "Final Exam Review üéÜ The Last Lecture",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/final-exam-review-the-last-lecture",
    "text": ""
  },
  {
    "id": "17040d70fdf581fd8f6ce952d0f0df63",
    "title": "Assignment4 Posted",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment4-posted",
    "text": ""
  },
  {
    "id": "17040d70fdf58111bed1cbe863c92fda",
    "title": "Hierarchical Clustering & Outlier Detection",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/hierarchical-clustering-outlier-detection",
    "text": ""
  },
  {
    "id": "17040d70fdf581219800f5c4ed98fc69",
    "title": "Online Q&A Session",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/online-q-a-session",
    "text": ""
  },
  {
    "id": "17040d70fdf58127aae8e1262df9ae53",
    "title": "Midterm Exam Grade Released",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/midterm-exam-grade-released",
    "text": ""
  },
  {
    "id": "17040d70fdf581718cf5da13c28f84dc",
    "title": "Assignment3 Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment3-due",
    "text": ""
  },
  {
    "id": "17040d70fdf5816fa153f1ff88601124",
    "title": "Project Dataset Selection Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/project-dataset-selection-due",
    "text": ""
  },
  {
    "id": "17040d70fdf581f4acd0d90682a33120",
    "title": "üçÅ FALL STUDY BREAK üçÅ",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/fall-study-break",
    "text": ""
  },
  {
    "id": "17040d70fdf581d9a37de6a211051ecf",
    "title": "Partition-Based Clustering",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/partition-based-clustering",
    "text": ""
  },
  {
    "id": "17040d70fdf581358e8af4964445c8cf",
    "title": "Lab10 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab10-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf5814392cafdfd98bf61d9",
    "title": "Descriptive Analytics",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/descriptive-analytics",
    "text": ""
  },
  {
    "id": "17040d70fdf581d59ca7cf7b66ef4af7",
    "title": "Lab10 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab10-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581229312caa2e7ac23a5",
    "title": "Assignment3 Posted",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment3-posted",
    "text": ""
  },
  {
    "id": "17040d70fdf581a1b051ce8bf6565abb",
    "title": "Model Evaluation and Selection",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/model-evaluation-and-selection",
    "text": ""
  },
  {
    "id": "17040d70fdf58145be88e89fd03e0eae",
    "title": "Lab9 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab9-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf58153bdaecdd752c6a2e1",
    "title": "Tree-Based Models",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/tree-based-models",
    "text": ""
  },
  {
    "id": "17040d70fdf5816b9cedd1127493caac",
    "title": "Lab9 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab9-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581408f14eebdba3567ad",
    "title": "Assignment2 Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment2-due",
    "text": ""
  },
  {
    "id": "17040d70fdf581ecbdf2dee3b6527938",
    "title": "SVM",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/svm",
    "text": ""
  },
  {
    "id": "17040d70fdf5811a9ff4ec0210165e9a",
    "title": "Linear Models",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/linear-models",
    "text": ""
  },
  {
    "id": "17040d70fdf5813a93c8e045411a2781",
    "title": "Lab8 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab8-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf58117bbb0d1570e43a9ed",
    "title": "Lab8 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab8-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf5810fa48cd576e93c61de",
    "title": "Predictive Analytics",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/predictive-analytics",
    "text": ""
  },
  {
    "id": "17040d70fdf58102a803cd7288ce1341",
    "title": "Get Data from Real-Time Data Feed",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/get-data-from-real-time-data-feed",
    "text": ""
  },
  {
    "id": "17040d70fdf581c4a39cc58c689cfece",
    "title": "Leb7 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/leb7-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf5814f9b72d8b34ea225c2",
    "title": "Assignment2 Posted",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment2-posted",
    "text": ""
  },
  {
    "id": "17040d70fdf581a3b051d5ba0b685ff2",
    "title": "Lab7 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab7-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581dd86f8ca5337659273",
    "title": "Lab6 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab6-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf58127a4baffcc5b51a7f6",
    "title": "Web Scraping in R: rvest, httr, and jsonlite",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/web-scraping-in-r-rvest-httr-and-jsonlite",
    "text": ""
  },
  {
    "id": "17040d70fdf581aaa7d9f85f1e5abd98",
    "title": "Basic Web Component Review",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/basic-web-component-review",
    "text": ""
  },
  {
    "id": "17040d70fdf581659f20ecc06d10ef63",
    "title": "Lab6 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab6-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf5818883e1e094e7460396",
    "title": "Assignment 1 Due (Extended)",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment-1-due-extended",
    "text": ""
  },
  {
    "id": "17040d70fdf581bd9e4fdf61a4c4d45b",
    "title": "üí™ Midterm Exam",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/midterm-exam",
    "text": ""
  },
  {
    "id": "17040d70fdf581968dc5f426656e912d",
    "title": "Midterm Review",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/midterm-review",
    "text": ""
  },
  {
    "id": "17040d70fdf5810cbfc9eec9b6072ff2",
    "title": "Lab5 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab5-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581db8cbfff9f7baae4f4",
    "title": "Lab5 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab5-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf581b99c08d4295a2d2a3e",
    "title": "Data Visualization",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/data-visualization",
    "text": ""
  },
  {
    "id": "17040d70fdf58121974ff96622f590c6",
    "title": "Lab3 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab3-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf581409642e9d4bdf00e86",
    "title": "Lab4 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab4-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf581e19905e613fb15e17f",
    "title": "Data Summarization",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/data-summarization",
    "text": ""
  },
  {
    "id": "17040d70fdf581c3a50fffef42d942b6",
    "title": "Lab3 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab3-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581968dcdd3a7e74ed9a7",
    "title": "Lab4 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab4-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf5819abb4fc0d3dc0b1ddd",
    "title": "Assignment 1 Posted",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/assignment-1-posted",
    "text": ""
  },
  {
    "id": "17040d70fdf581d8a90aee4405b3cb1c",
    "title": "Final Exam/Project Selection Due",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/final-exam-project-selection-due",
    "text": ""
  },
  {
    "id": "17040d70fdf581249a2edb3e4a5c2813",
    "title": "Project Description Posted",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/project-description-posted",
    "text": ""
  },
  {
    "id": "17040d70fdf5818b8ce1fb09379df096",
    "title": "Data Pre-Processing",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/data-pre-processing",
    "text": ""
  },
  {
    "id": "17040d70fdf581028ddce8d5c2bf48d7",
    "title": "Data Manipulation",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/data-manipulation",
    "text": ""
  },
  {
    "id": "17040d70fdf581d58183f2bf7ae89b2f",
    "title": "Lab2 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab2-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf58142857ceaf9150770f4",
    "title": "Lab2 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab2-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581c59ff0f7d93a19540d",
    "title": "Data Structure",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/data-structure",
    "text": ""
  },
  {
    "id": "17040d70fdf58157aae0d12ee6671134",
    "title": "Introduction to R Language",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/introduction-to-r-language",
    "text": ""
  },
  {
    "id": "17040d70fdf581928066d9f4df3a6334",
    "title": "Lab1 Sec01",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab1-sec01",
    "text": ""
  },
  {
    "id": "17040d70fdf58195b440c21f6efe3702",
    "title": "Lab1 Sec02",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/lab1-sec02",
    "text": ""
  },
  {
    "id": "17040d70fdf581d1bbbbd7b4ff5830c6",
    "title": "Introduction to Data Science",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/introduction-to-data-science",
    "text": ""
  },
  {
    "id": "17040d70fdf581f09a66c9411c1f5eb7",
    "title": "First Class, Let‚Äôs Know Each Other!",
    "kind": "page",
    "routePath": "/teaching/archive/2024-25-fall/csci3141/timeline/first-class-let-s-know-each-other",
    "text": ""
  },
  {
    "id": "ed62ebed814c41f684fd918936b64c09",
    "title": "Works",
    "kind": "page",
    "routePath": "/works",
    "text": "For all of my teaching works, please refer to the Teaching page.\nRecent Works\nRecent Works\nIntern Donut Browser (Donut Labs), Remote Nov 2025 - Now\nPart-time Instructor Orange Education Ltd., Halifax, NS, Canada Sep 2019 - Now\nI have experience teaching a variety of Computer Science-related courses, and I'm proud to have been recognized for my hard work and dedication in this field. Specifically, I was awarded the prestigious Gold Medal Instructor title, which speaks to my ability to effectively engage with and educate my students on complex concepts related to Computer Science and Statistics. I am also in charge of some administration work, like interviewing potential instructors.c\nPassed Works\nPassed Works\nAIS Data Analyst, Project AISViz MERIDIAN, Dalhousie University, Halifax, NS, Canada May 2023 - Aug 2025\nOur main objective is to build an easy-to-use open-source toolbox for raw AIS data extraction, processing, visualization, and vessel modeling. We aim to make AIS data accessible and comprehensible for a broad spectrum of users, from government bodies and policymakers to university researchers, non-government organizations (NGOs), coastal communities, and the general public. AISviz was proposed to implement machine learning applications that work seamlessly with flexible data sources from terrestrial and satellite streams of AIS data. Besides, we intend to make the interaction with AIS data user-friendly, achieved through developing a Graphical User Interface (GUI). This approach will simplify data retrieval, integration, and basic manipulation processes. We also aim to enhance the applicability of AIS data by incorporating its integration with different maritime raster-based third-party data sources. The project and all its components will be made openly accessible, promoting collaboration, transparency, and public contribution. The project is under the auspices of MALNIS Lab and MERIDIAN, and funded by Fisheries and Oceans Canada (DFO) of the Government of Canada. GitHub | AISdb Documentation | AISViz Project Website\nResearch Assistant, Project Ballast Dalhousie University, Halifax, NS, Canada Feb 2021 - Mar 2023\nBallast project is one where I am serving as a Research Assistant at Institute for Big Data Analytics, Dalhousie University. It involves building a web-based application that examines commercial shipping activities as an environmental stressor. The project involves analyzing and visualizing GIS (Geographic Information System) data. This project is supported by the Ocean and Freshwater Science Contribution Program, which is hosted by Fisheries and Oceans Canada (DFO).\nPresident UNICEF Dalhousie, Halifax, NS, Canada Sep 2018 - Aug 2020\nUNICEF Dalhousie serves as the official representative of the United Nations Children's Fund (UNICEF) at Dalhousie University, as part of the UNICEF Canada Campus Clubs program. As a recognized campus club, UNICEF Dalhousie works to promote and support the mission of UNICEF, which is to protect and advocate for the rights of children and to help them reach their full potential. Through various initiatives and events, UNICEF Dalhousie aims to raise awareness and funds for UNICEF's programs and initiatives, and to engage the Dalhousie community in efforts to improve the lives of children around the world.\nTechnical Director Sino Gateway Media & Marketing Ltd., Halifax, NS, Canada Oct 2017 - Sep 2018\nI was actively involved in contributing to the design, development, and operation of Hafaquanzhidao(ÂìàÊ≥ïÂÖ®Áü•ÈÅì), which is an innovative information and life-sharing platform built as a WeChat mini app. This unique platform is catered towards the Chinese community residing in Halifax, offering them an exclusive digital space to share information and experiences related to their lives in the region. In addition, my team and I play a crucial role in one of the company's secondary revenue streams, which involves providing digital solutions to a diverse range of business clients.\nTechnical Director YiGuang Media Ltd., Halifax, NS, Canada Oct 2016 - Sep 2017\nOne of the company's primary endeavors during that period was managing an online forum called ChineseHalifax(ÂìàÊ≥ïÂçé‰∫∫ÁΩë), which catered to the needs of Chinese residents living in Halifax. My ability to get up to speed with the job quickly was due to the fact that the platform used the same technical stack that I used and operated in 2013 for an anime forum.\nThis list is not exhaustive, and maybe not up-to-date"
  },
  {
    "id": "21040d70fdf5806aad08fe42515fe790",
    "title": "Blog",
    "kind": "page",
    "routePath": "/blog",
    "text": "RSS Feed"
  },
  {
    "id": "21040d70fdf580488cfbf71f19978866",
    "title": "List",
    "kind": "database",
    "routePath": "/blog/list",
    "text": "List\nContext Order and Reasoning Drift: Measuring Order Sensitivity from Token Probabilities\nMeasuring Reasoning Drift: A Token-Level Instability Signal\nWhen AI Reasoning Starts to Drift\nThe Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\nKey Challenges in Current LLM Memory Systems\nThe Surprising Impact of Memory Order on LLM Responses\nDo Language Model Embeddings Form an Approximate Abelian Group?"
  },
  {
    "id": "30040d70fdf58106bc92ee4e16cdbba2",
    "title": "Context Order and Reasoning Drift: Measuring Order Sensitivity from Token Probabilities",
    "kind": "page",
    "routePath": "/blog/list/context-order-and-reasoning-drift-measuring-order-sensitivity-from-token-probabilities",
    "text": "How the same evidence can yield different answers when you only change the order.\nPeople often treat retrieval order as a formatting detail.\nIn practice, context order can change the early next token distributions, and that can push decoding into a different trajectory.\nThis post is about how to measure that sensitivity using only token probabilities.\nThe phenomenon: same chunks, different outcomes\nThe phenomenon: same chunks, different outcomes\nIn many retrieval augmented setups, the model receives a question and a set of retrieved chunks.\nIf you permute those chunks but keep everything else fixed, you often get different answers.\nThere are two important points.\nFirst, order effects are not rare edge cases.\nThey show up even with greedy decoding because the model still makes a sequence of locally optimal choices.\nSecond, the effect is process level.\nYou can observe drift before the final answer diverges.\nWhat we can observe during decoding\nWhat we can observe during decoding\nAt each decoding step t, an autoregressive model produces a next token distribution p_t(\\cdot).\nMany APIs expose only top k candidates with log probabilities.\nThat is enough.\nWe can renormalize the logged support and compute lightweight signals from the resulting distribution.\nThe key idea is to measure two things.\nUncertainty at a step.\nDistributional change from one step to the next.\nA simple instability signal\nA simple instability signal\nDefine an entropy term H_t from p_t and a Jensen Shannon divergence term D_t between p_t and p_{t-1}.\nThen define an instability index\nwhere \\lambda is a mixing weight.\nI use \\lambda = 1 as a default.\nTo summarize a trace, use a peak statistic.\nS = \\max_t I_t\nFor early diagnostics, use prefix windows.\nS_w = \\max_{t \\le w} I_t\nThis is simple on purpose.\nThe goal is not a perfect theory of decoding.\nThe goal is a consistent metric you can compute across many runs.\nMeasuring order sensitivity\nMeasuring order sensitivity\nNow we can define a clean protocol.\nFix a question and its retrieved chunk set.\nGenerate multiple permutations of the chunk order.\nDecode with the same settings for each permutation and log top k token probabilities.\nCompute I_t, then summarize each run by S or S_w.\nQuantify sensitivity from variability across permutations.\nYou can report sensitivity in several equivalent ways.\n\\operatorname{Var}(S \\mid q, \\text{chunks})\n\\operatorname{mean}_{a<b}\\,\\lvert S(\\pi_a) - S(\\pi_b) \\rvert\nThe fraction of permutations that cross a chosen risk threshold.\nIf you also label runs as correct or wrong, you can test whether higher sensitivity correlates with a higher chance of failure.\nWhat this diagnostic is and is not\nWhat this diagnostic is and is not\nWhat it is.\nA way to measure how much the decoding path changes under context permutations.\nA method that works with logged token probabilities.\nA tool to compare retrieval and prompting strategies by process dynamics.\nWhat it is not.\nNot an intervention.\nNot a claim that you can eliminate order effects with a single heuristic.\nPractical notes\nPractical notes\nA few details matter in practice.\nKeep decoding settings fixed.\nKeep the chunk set fixed.\nUse a fixed random seed for permutation sampling.\nUse the same stopping rules across runs.\nIf you have access to full vocabulary logits, compute entropy and divergence on the full distribution.\nIf you only have top k logging, renormalize on the logged support and treat the metric as a consistent approximation.\nImplementation sketch\n# Inputs: per step top k logprobs: logp[t] = {token: log_prob} # Output: instability strength S def renormalize(logp_dict): probs = {tok: math.exp(lp) for tok, lp in logp_dict.items()} z = sum(probs.values()) return {tok: p / z for tok, p in probs.items()} def entropy(p): return -sum(pi * math.log(pi) for pi in p.values() if pi > 0.0) def jsd(p, q): keys = set(p) | set(q) m = {k: 0.5 * (p.get(k, 0.0) + q.get(k, 0.0)) for k in keys} def kl(a, b): return sum(ai * math.log(ai / b[k]) for k, ai in a.items() if ai > 0.0) return 0.5 * kl(p, m) + 0.5 * kl(q, m) p_prev = None I = [] for t in range(T): p_t = renormalize(logp[t]) H_t = entropy(p_t) D_t = 0.0 if p_prev is None else jsd(p_t, p_prev) I.append(D_t + 1.0 * H_t) p_prev = p_t S = max(I)\n# Inputs: per step top k logprobs: logp[t] = {token: log_prob} # Output: instability strength S def renormalize(logp_dict): probs = {tok: math.exp(lp) for tok, lp in logp_dict.items()} z = sum(probs.values()) return {tok: p / z for tok, p in probs.items()} def entropy(p): return -sum(pi * math.log(pi) for pi in p.values() if pi > 0.0) def jsd(p, q): keys = set(p) | set(q) m = {k: 0.5 * (p.get(k, 0.0) + q.get(k, 0.0)) for k in keys} def kl(a, b): return sum(ai * math.log(ai / b[k]) for k, ai in a.items() if ai > 0.0) return 0.5 * kl(p, m) + 0.5 * kl(q, m) p_prev = None I = [] for t in range(T): p_t = renormalize(logp[t]) H_t = entropy(p_t) D_t = 0.0 if p_prev is None else jsd(p_t, p_prev) I.append(D_t + 1.0 * H_t) p_prev = p_t S = max(I)"
  },
  {
    "id": "2ff40d70fdf5813aa761e9f8f20238d4",
    "title": "Measuring Reasoning Drift: A Token-Level Instability Signal",
    "kind": "page",
    "routePath": "/blog/list/measuring-reasoning-drift-a-token-level-instability-signal",
    "text": "How to detect process-level breakdowns from token probabilities before the final answer fails.\nPeople often describe LLM mistakes as sudden failures: one moment the answer looks coherent, and the next it collapses.\nBut in many reasoning tasks, a model is not jumping straight from question to conclusion. It is stepping through a sequence of decisions. Small deviations early can quietly reshape everything downstream.\nThe natural question is:\nCan we measure when reasoning starts to drift?\nCan we measure when reasoning starts to drift?\nNot by reading chain-of-thought (which can be incomplete or unfaithful), but by looking at what every decoder produces anyway: a probability distribution over the next token.\nIn this post I‚Äôll walk through a simple inference-time diagnostic for reasoning drift:\nTraining-free (no fine-tuning).\nBlack-box (works with logged token probabilities / log-probabilities).\nProcess-level (measures how the trajectory evolves, not just the final answer).\nWhat we can observe during decoding\nWhat we can observe during decoding\nAt each decoding step t, an autoregressive model induces a next-token distribution p_t(\\cdot).\nIn many APIs, we can only log the top candidates (top-k tokens) with their log-probabilities. So we work with a truncated, renormalized distribution \\tilde{p}_t over the logged support.\nFrom \\tilde{p}_t, we compute two lightweight signals:\nUncertainty (entropy): when the model is ‚Äúnear a tie‚Äù among multiple plausible next tokens.\nDistributional change (Jensen-Shannon divergence, JSD): when the model‚Äôs next-token distribution shifts abruptly from one step to the next.\nIntuition:\nHigh entropy = the model is unsure right now.\nHigh JSD = the model‚Äôs ‚Äúbelief over next moves‚Äù just changed sharply.\nA simple instability signal\nA simple instability signal\nWe combine the two into a per-step instability index:\nwhere H_t is entropy, D_t is JSD between consecutive steps, and \\lambda is a fixed mixing weight (I use \\lambda = 1 as a simple reference).\nTo summarize an entire reasoning trace, we use the peak instability strength:\nS = \\max_t I_t\nAnd to check ‚Äúearly warning‚Äù behavior (to control for length), we also use prefix windows:\nS_w = \\max_{t \\leq w} I_t for windows like w = 10, 20, 50, 100.\nWhat changes when reasoning is unstable?\nWhat changes when reasoning is unstable?\nA useful mental model is that decoding is a closed-loop system: each chosen token becomes part of the next input. If the trajectory enters a ‚Äúfragile‚Äù region, a small step-to-step deviation can amplify.\nEmpirically, instability spikes often coincide with:\nSupport turnover: the set of highly probable next tokens changes sharply.\nNear-ties: the top candidates are close, so small shifts can flip the preferred continuation.\nThese show up directly in \\tilde{p}_t, so we do not need hidden states.\nDoes instability actually predict failure?\nDoes instability actually predict failure?\nAs a diagnostic, it‚Äôs often predictive.\nFor example, in one representative GSM8K run (first 300 test problems; greedy decoding), peak instability strength S predicts wrong answers with ROC-AUC ‚âà 0.66.\nThe most interpretable view is bucket trends:\nSort examples by S.\nSplit into 5 equal-sized buckets.\nAccuracy is much higher in the lowest-instability bucket, and remains low in the higher-instability buckets.\nTakeaway:\nHigher instability strength corresponds to a higher risk of reasoning failure.\nEarly warning: you don‚Äôt need the whole trace\nEarly warning: you don‚Äôt need the whole trace\nA practical question is whether the signal only appears after the model has already failed.\nIn the same GSM8K run above, separability is already above chance using short prefixes (e.g., AUC ‚âà 0.67 by w = 20) and stays roughly stable as we extend the window.\nIf the curve looks ‚Äúflat‚Äù, that‚Äôs the point: most of the separability is already present very early, so additional steps don‚Äôt add much extra predictive power (at least under this summary statistic).\nTiming matters: corrective vs destructive instability\nTiming matters: corrective vs destructive instability\nHere‚Äôs the most important nuance:\nA high instability spike does not automatically mean ‚Äúthe model is failing.‚Äù\nSometimes, a model becomes briefly unstable because it is self-correcting (switching from a wrong intermediate route to a better one). Other times, instability happens too late, and the model cannot recover.\nA simple operational proxy is when peak instability occurs.\nLet t^* = \\arg\\max_t I_t be the peak step, T be trace length, and define the relative peak position \\rho = t^*/T.\nEarly peak (small \\rho): the model has remaining budget to recover.\nLate peak (large \\rho): there may be no time left to stabilize.\nAs a sanity check beyond top-k logging, I also ran a small held-out GSM8K set where entropy/JSD are computed from full-vocabulary logits (no truncation). In that run, early-peak traces are much more accurate than late-peak traces (about 46% vs 14%).\nTo make this concrete, here are two example traces: one correct with an early peak, and one wrong with a late peak.\nA small but important limitation: stable-but-wrong failures\nA small but important limitation: stable-but-wrong failures\nInstability is not a universal explanation of all errors.\nSome failures are stable-but-wrong: the model stays confident and consistent, but commits to the wrong solution anyway (knowledge gaps, spurious heuristics, etc.).\nThis is why I treat instability as a diagnostic dimension, not a catch-all label.\nWhat this diagnostic is (and is not)\nWhat this diagnostic is (and is not)\nWhat it is:\nA lightweight, inference-time ‚Äúhealth monitor‚Äù for reasoning trajectories.\nA way to compare models, datasets, and decoding settings by process dynamics, not just accuracy.\nA tool for studying when and how reasoning collapses.\nWhat it is not:\nNot a stabilization method.\nNot an intervention that claims to improve accuracy.\nImplementation sketch (from logged top-k logprobs)\n# Inputs: per-step top-k logprobs: logp[t] = {token: log_prob} # Output: instability strength S def renormalize(logp_dict): # Convert to probabilities and renormalize on logged support probs = {tok: math.exp(lp) for tok, lp in logp_dict.items()} z = sum(probs.values()) return {tok: p / z for tok, p in probs.items()} def entropy(p): return -sum(pi * math.log(pi) for pi in p.values() if pi > 0.0) def jsd(p, q): # Compute on union support by zero-padding keys = set(p) | set(q) m = {k: 0.5 * (p.get(k, 0.0) + q.get(k, 0.0)) for k in keys} def kl(a, b): return sum(ai * math.log(ai / b[k]) for k, ai in a.items() if ai > 0.0) return 0.5 * kl(p, m) + 0.5 * kl(q, m) p_prev = None I = [] for t in range(T): p_t = renormalize(logp[t]) H_t = entropy(p_t) if p_prev is None: D_t = 0.0 else: D_t = jsd(p_t, p_prev) I_t = D_t + 1.0 * H_t # lambda = 1 I.append(I_t) p_prev = p_t S = max(I)\n# Inputs: per-step top-k logprobs: logp[t] = {token: log_prob} # Output: instability strength S def renormalize(logp_dict): # Convert to probabilities and renormalize on logged support probs = {tok: math.exp(lp) for tok, lp in logp_dict.items()} z = sum(probs.values()) return {tok: p / z for tok, p in probs.items()} def entropy(p): return -sum(pi * math.log(pi) for pi in p.values() if pi > 0.0) def jsd(p, q): # Compute on union support by zero-padding keys = set(p) | set(q) m = {k: 0.5 * (p.get(k, 0.0) + q.get(k, 0.0)) for k in keys} def kl(a, b): return sum(ai * math.log(ai / b[k]) for k, ai in a.items() if ai > 0.0) return 0.5 * kl(p, m) + 0.5 * kl(q, m) p_prev = None I = [] for t in range(T): p_t = renormalize(logp[t]) H_t = entropy(p_t) if p_prev is None: D_t = 0.0 else: D_t = jsd(p_t, p_prev) I_t = D_t + 1.0 * H_t # lambda = 1 I.append(I_t) p_prev = p_t S = max(I)\n‰∏≠ÊñáÊëòË¶Å\nÊàë‰ª¨Â∏∏Êää LLM ÁöÑÈîôËØØÂΩìÊàê‚ÄúÁ™ÅÁÑ∂Â¥©Êéâ‚ÄùÔºå‰ΩÜÊé®ÁêÜÂÖ∂ÂÆûÊòØ‰∏Ä‰∏™ÈÄêÊ≠•ÊºîÂåñÁöÑËøáÁ®ã„ÄÇ\nËøôÁØáÊñáÁ´†‰ªãÁªç‰∏Ä‰∏™Á∫ØËØäÊñ≠‰ø°Âè∑ÔºöÂè™Áî®ÊØèÊ≠•ÁöÑ top-k token Ê¶ÇÁéáÔºàlogprobsÔºâÂ∞±ËÉΩËÆ°ÁÆó‰∏çÁ®≥ÂÆöÊÄß I_tÔºåÂπ∂Áî®Â≥∞ÂÄº S Ë°°ÈáèÊï¥Êù°Êé®ÁêÜËΩ®ËøπÁöÑ‚ÄúÂä®ÊÄÅÂ§±Á®≥Âº∫Â∫¶‚Äù„ÄÇ\nÁªèÈ™å‰∏äÔºåS ‰∏éÂ§±Ë¥•È£éÈô©Áõ∏ÂÖ≥ÔºåÂπ∂‰∏îÂú®Âè™ÁúãÂâçÂá†ÂçÅÊ≠•Êó∂Â∞±Â∑≤ÂÖ∑Â§á‰∏ÄÂÆöÁöÑ early-warning Âå∫ÂàÜËÉΩÂäõ„ÄÇ\nÊõ¥ÂÖ≥ÈîÆÁöÑÊòØÔºö‰∏çÁ®≥ÂÆöÂπ∂‰∏çÊÄªÊòØÂùè‰∫ã„ÄÇÊó©ÊúüÁöÑ‰∏çÁ®≥ÂÆöÂèØËÉΩÂØπÂ∫î‚ÄúËá™Êàë‰øÆÊ≠£‚ÄùÔºàrecoverableÔºâÔºåÊôöÊúüÁöÑ‰∏çÁ®≥ÂÆöÊõ¥ÂÉè‚Äú‰∏çÂèØÊÅ¢Â§çÁöÑÂÅèÁ¶ª‚ÄùÔºàirrecoverableÔºâ„ÄÇ"
  },
  {
    "id": "2fa40d70fdf58178a2c1d3a9c3887af1",
    "title": "When AI Reasoning Starts to Drift",
    "kind": "page",
    "routePath": "/blog/list/when-ai-reasoning-starts-to-drift",
    "text": "People often describe AI mistakes as sudden failures. One moment the answer seems reasonable and confident, and the next it is clearly wrong. This makes errors feel unpredictable, as if the system simply failed without warning.\nBut when you look more closely at how AI generates answers, a different picture begins to emerge.\nRather than jumping directly from a question to a conclusion, many modern AI systems produce answers step by step. At each step, the system makes a choice based on what it has already generated. Over time, these choices accumulate. Small changes early on can quietly influence everything that follows.\nThis means that reasoning does not always fail all at once. Sometimes it gradually drifts.\nReasoning as a process, not a moment\nReasoning as a process, not a moment\nIt is tempting to judge AI systems only by their final answers. Was the response correct or incorrect. Did it match what we expected.\nHowever, this way of thinking hides an important detail. Reasoning unfolds over time. Each step shapes the next one. When early steps become uncertain or unstable, later steps may amplify those effects, even if the final answer still sounds confident.\nFrom the outside, this can look like a sudden collapse. Internally, it may be the result of small shifts that built up across the reasoning process.\nWhat instability can look like\nWhat instability can look like\nInstability in reasoning does not always mean obvious confusion. In fact, it often appears in subtle ways.\nAn AI might hesitate between multiple possible directions before settling on one. It might change how it frames the problem midway through an answer. It might become increasingly confident in a line of reasoning that is quietly drifting away from the original question.\nThese changes are easy to miss if we only look at the final output. But they can matter a great deal, especially for tasks that require multiple steps of reasoning.\nWhy confidence is not always a signal of reliability\nWhy confidence is not always a signal of reliability\nOne of the challenges with AI systems is that fluent language and confident tone are not reliable indicators of stable reasoning. A system can sound certain while its internal decision process is becoming less consistent.\nThis helps explain a common user experience. You read an answer that feels persuasive, only to realize later that something is off. The problem is not always a lack of knowledge. Sometimes it is the way reasoning unfolded along the way.\nUnderstanding this distinction can change how we interpret AI outputs. Instead of asking only whether an answer is correct, we can also ask whether the reasoning that led to it remained steady.\nWhy this perspective matters\nWhy this perspective matters\nThinking about reasoning as a dynamic process has practical implications. It helps explain why asking follow up questions can sometimes improve results. It also explains why rephrasing a prompt or breaking a task into smaller steps can lead to more reliable answers.\nMore broadly, it encourages a more careful relationship with AI systems. Rather than treating them as tools that either work or fail, we can view them as systems whose reasoning can evolve, drift, and occasionally become unstable.\nThis does not mean AI reasoning is fragile in every situation. It means that, like many complex systems, its behavior depends on how processes unfold over time.\nPaying attention to those processes can help us better understand when AI is likely to stay on track, and when it may begin to wander."
  },
  {
    "id": "21740d70fdf580f281fbe08a7014ce1d",
    "title": "The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models",
    "kind": "page",
    "routePath": "/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models",
    "text": "Why the order of retrieved information can quietly change how AI reasons step by step\nYou give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.\nThis behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages (\"chunks\") shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, \"multi-step inference\" refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.\nA key finding is that preserving the original document structure, as seen in Document's Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM's sequential processing.\nFor users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.\nThe analysis further reveals that LLMs exhibit a \"cognitive linearity\", performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.\nBeyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.\n1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\n1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\nThe landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.\n1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\n1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\nLarge Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.^1 These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.^1 LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.^2 Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.^3\nDespite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.^2 This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as \"hallucinations\", where the model generates factually incorrect or nonsensical information not present in its training data.^2 Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.^5 These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.\n1.2 Understanding Retrieval-Augmented Generation (RAG)\n1.2 Understanding Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.^4 This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.^4\nThe core process of RAG typically involves two main stages. First, Retrieval and Pre-processing occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.^4 The second stage is Grounded Generation, where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM's context. This integration significantly enhances the LLM's understanding of the topic, allowing it to produce more precise, informative, and engaging responses.^4\nRAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.^4 This factual grounding is crucial for mitigating \"gen AI hallucinations\" by supplying verifiable facts as part of the input prompt.^4 The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM's outputs.^4 This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.\n1.3 The Essence of Multi-Step Inference in LLMs\n1.3 The Essence of Multi-Step Inference in LLMs\nMulti-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM's capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.^5 This capability extends beyond merely following a single instruction or performing a singular task.^{10}\nThe ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.^{11} It is widely recognized as a key indicator of advanced intelligence in AI systems.^{10} However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.^5 While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.^{10}\nTo address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).^{10} Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.^{11} The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. This shift underscores that future advancements in LLMs and RAG systems must prioritize not just what information is retrieved, but how that information facilitates a structured, step-by-step problem-solving process. This elevates the importance of context organization and coherence within the input.\n1.4 Purpose and Scope of the Report\n1.4 Purpose and Scope of the Report\nThe primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM's capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.\n2. Chunking and Retrieval in RAG Architectures\n2. Chunking and Retrieval in RAG Architectures\nThe efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.\n2.1 Principles of Document Chunking for RAG\n2.1 Principles of Document Chunking for RAG\nChunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.^{12} These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.^{13} The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.\nThe necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.^{13} By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.^{13}\nSeveral chunking strategies are employed, each with distinct advantages and use cases:\nFixed Size Chunking: This straightforward approach divides text into uniform chunks based on a predefined character or token count.^{13} For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.^{13} While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.^{13}\nRecursive-Based Chunking: A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.^{13}\nSentence-based Chunking: This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.^{13}\nDocument Structure-based Chunking: This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.^{13}\nSemantic Chunking: This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.^{14}\nThe choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM's ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.\n2.2 Overview of Retrieval Mechanisms in RAG\n2.2 Overview of Retrieval Mechanisms in RAG\nA RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a Query Encoder transforms the user's input query into a representation suitable for searching the knowledge base.^8 Second, a Retriever takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.^8Finally, a Generator, typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.^8\nRetrievers can be broadly categorized based on their underlying mechanisms:\nSparse Retrievers: These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.^8\nDense Retrievers: Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.^4\nHybrid Search: Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.^4 The retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.^4 After the initial retrieval, an essential step often involves\nre-rankers. These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.^4 This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.^4 The effectiveness of RAG is therefore highly dependent on the retriever's ability to provide relevant information and the re-ranker's capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM's performance can degrade, leading to responses that, while \"grounded\" in the provided context, might be off-topic or factually incorrect.^4 The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the most relevant and least distracting content, a factor that profoundly influences the subsequent chunk ordering.\n3. The Critical Role of Chunk Retrieval Sequence\n3. The Critical Role of Chunk Retrieval Sequence\nThe order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM's ability to reason effectively.\n3.1 Impact of Context Order on LLM Performance\n3.1 Impact of Context Order on LLM Performance\nObservations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.^{17} This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM's internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.^{18} For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model's accuracy.^{18} This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.^{18}\nConversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.^{18} This \"ordering effect\" is further exacerbated when irrelevant premises are introduced into the prompt.^{18} When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model's ability to synthesize information and produce coherent responses.^{17} This indicates that LLMs, despite their advanced capabilities, exhibit a form of \"cognitive linearity\" in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a \"ground truth proof\" ^{18} suggests that the LLM's internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.\n3.2 Document's Original Structure (DOS RAG) and its Benefits\n3.2 Document's Original Structure (DOS RAG) and its Benefits\nDocument's Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.^{19} This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.^{19}\nThe benefits of DOS RAG are significant and empirically validated. It primarily preserves passage continuity, maintaining the document's structural integrity and narrative flow.^{13} This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves improved accuracy, outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including ‚àûBench, QuALITY, and NarrativeQA.^{19} This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.^{19} For instance, on the ‚àûBench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG's 87.8%.^{19} Furthermore, DOS RAG demonstrates notable efficiency, often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.^{19} This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.^{19}\nThe consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, contextual coherence and narrative flow, as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.^{17} However, DOS RAG's consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the relationship between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.^{19} Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.^{11} Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM's processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of \"relevance\" for multi-step tasks should be broadened to include \"contextual relevance\" or \"narrative relevance\" in addition to traditional semantic similarity.\n3.3 Reranking Strategies and Context Reordering\n3.3 Reranking Strategies and Context Reordering\nReranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.^8 This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.^{22} Various types of rerankers are employed, each with distinct characteristics:\nCross-Encoders: These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.^{22} Examples include Sentence Transformers, Flashrank, and BGE-M3.^{15}\nMulti-Vector Rerankers: Models like ColBERT use a \"late interaction\" approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.^{22}\nFine-tuned LLM Rerankers: Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.^{22} These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).^{22}\nLLM as a Judge: This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.^{22} While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.^{22} Examples include GPT, Claude, and Gemini.^{22}\nReranking APIs: Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.^{22} Examples include Cohere, Jina, and Mixedbread.^{22}\nBeyond simple relevance scoring, context reordering within the reranking process also plays a role. Inverted Context Ordering is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.^{16} This method has demonstrated a performance increase in correctness for multi-hop QA tasks.^{24} Other advanced approaches include Fusion-based Reranking, which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks ^8, and Adaptive Reranking, which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).^8\nWhile reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where \"better\" relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the most relevant context.^8 However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.^{15} This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system's usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.^{19} This implies that the \"best\" reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM. Table 1: Comparison of Key Chunking and Reordering Strategies in RAG\nStrategy\nDescription\nPrimary Goal\nImpact on Multi-Step Inference\nAdvantages\nDisadvantages\nRelevant Snippets\nFixed Size Chunking\nDivides text into uniform segments (e.g., 500 tokens), often with overlap.\nEfficiency, Simplicity\nCan fragment context, hindering logical flow.\nEasy to implement, fast, consistent.\nContext fragmentation, information loss, inflexible.\n13\nRecursive-Based Chunking\nUses multiple separators (paragraphs, sentences) to find meaningful boundaries.\nContext Preservation\nBetter at maintaining logical units for sequential understanding.\nAdaptive, preserves logical flow.\nMore complex to implement.\n13\nSentence-based Chunking\nDivides text into complete sentences.\nPreserve Complete Thoughts\nSupports logical flow, good for connecting ideas.\nEnsures complete thoughts, natural boundaries.\nMay create very small chunks, less efficient for long documents.\n13\nDocument's Original Structure (DOS RAG)\nRetrieves chunks and reorders them to match their original document sequence.\nNarrative Continuity, Contextual Coherence\nSignificantly improves performance by maintaining logical progression; crucial for multi-hop QA.\nPreserves narrative, robust QA, often outperforms relevance-based sorting.\nRequires tracking chunk positions; may include less relevant chunks if not filtered.\n17\nInverted Context Ordering\nArranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.\nPrioritize Most Relevant\nCan improve correctness; focuses LLM on key information.\nDirects LLM to top relevant info immediately.\nStill relies on relevance score, may disrupt original narrative flow.\n16\nSemantic Chunking\nDivides documents into semantically coherent and non-overlapping chunks.\nReduce Irrelevance, Improve Accuracy\nEnhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.\nReduces hallucinations, improves factual accuracy, aligned with query needs.\nRequires sophisticated LLM-based relevance scoring.\n14\n4. Factors Influencing Multi-Step Inference Performance in RAG\n4. Factors Influencing Multi-Step Inference Performance in RAG\nBeyond the direct ordering of retrieved chunks, several other factors interact with the LLM's context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.\n4.1 Positional Bias and the \"Lost-in-the-Middle\" Effect\n4.1 Positional Bias and the \"Lost-in-the-Middle\" Effect\nPositional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.^{25} A specific manifestation of this is the \"lost-in-the-middle\" effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.^{25} This bias can affect both the LLM's capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.^{25} Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.^{25}\nWhile earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.^{25} This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.^{25} Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM's presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.^{26} This is attributed to a \"contrastive effect\", where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.^{27} Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a \"parametric bias\", relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.^{27}\nThe \"lost-in-the-middle\" effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to \"trick\" the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the quality of retrieved content and enhancing the LLM's inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.^{25} However, in practical RAG systems, where retrievers often fetch both relevant and highly distracting passages ^{25}, the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the \"best\" chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.^{27} This shifts the focus from where to place chunks to what chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.\n4.2 The Detrimental Impact of Irrelevant and Distracting Information\n4.2 The Detrimental Impact of Irrelevant and Distracting Information\nA well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, \"distracting passages\", contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.^{28}\nThe presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.^{28} Studies have shown that \"hard distracting passages\", those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to \"weak\" ones, and this detrimental effect persists even in larger LLMs.^{28} Paradoxically, \"stronger\" retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver more harmful distractors.^{25} This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.^{28} Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.^{28}\nThe very act of retrieval, especially with \"stronger\" retrievers, presents a \"double-edged sword.\" While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG's core purpose is to provide relevant external knowledge.^4 However, no retriever is perfect, and they often return irrelevant or \"distracting\" passages.^{25} The critical observation here is that stronger retrievers, which are designed to find more relevant information, also tend to retrieve more harmful distracting passages.^{25} This creates a paradox: improving the retriever's primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for \"relevance\" (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the distracting effect to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.\n4.3 Cognitive Load and Context Window Management\n4.3 Cognitive Load and Context Window Management\nLarge Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.^{30} The concept of \"cognitive load\", analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).^{31} For LLMs, the inherent \"content complexity\" of the input is a dominant factor influencing their processing efficiency.^{31}\nPresenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its \"cognitive load\".^{30} This overload can lead to reduced selection accuracy and an increase in hallucinations.^{30} Conversely, supplying only the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of \"prompt bloat\" directly lowers the LLM's cognitive load.^{30} By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM's decision-making capabilities are markedly improved.^{30}\nWhile long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.^{17} The key lies in identifying the \"sweet spot\" for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.^{17}\nThe concept of \"cognitive load\" in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts ^{19}, the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.^{17} This is framed in terms of \"cognitive load\".^{30} If the LLM is forced to \"sift through hundreds of distractors\" ^{30}, it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about what to retrieve, but how much and how clean that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance. Table 2: Factors Affecting LLM Performance in Long Contexts\nFactor\nDescription\nImpact on Multi-Step Inference\nInteraction with Chunk Order\nMitigation Strategies\nRelevant Snippets\nPositional Bias\nLLMs weigh information differently based on its position (e.g., \"lost-in-the-middle\" effect).\nCan cause LLMs to ignore relevant info or be misled by distractors in middle positions.\nReordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.\nImprove retrieval quality, LLM robustness to distraction, avoid simple rearrangement.\n25\nIrrelevant/Distracting Information\nPassages that are semantically similar but do not contain the answer or mislead the LLM.\nSignificantly degrades accuracy, even when relevant info is present; can derail reasoning chain.\nStrong retrievers can inadvertently bring more harmful distractors to top ranks.\nRobust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.\n14\nCognitive Load/Context Window Overload\nLLM struggles to process excessive or noisy information within its limited context.\nReduces selection accuracy, increases hallucinations, hinders efficient reasoning.\nToo many chunks (even if somewhat relevant) can overwhelm the model.\nSupplying only relevant context, precise chunking, adaptive streaming, efficient filtering.\n17\nLack of Narrative Continuity\nDisjointed or shuffled presentation of information.\nImpairs sequential reasoning, makes it harder for LLM to build a coherent understanding.\nDirect result of relevance-only sorting; addressed by DOS RAG.\nPreserving original document structure (DOS RAG) or logical flow.\n17\n5. Empirical Evidence and Performance Analysis\n5. Empirical Evidence and Performance Analysis\nEmpirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.\n5.1 Case Studies on Chunk Order and Multi-Step Question Answering\n5.1 Case Studies on Chunk Order and Multi-Step Question Answering\nComparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including ‚àûBench, QuALITY, and NarrativeQA.^{19} This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.^{19} For example, on the ‚àûBench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.^{19} This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs' multi-step reasoning capabilities are profoundly tied to the narrative and structural coherence of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.\nFurthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.^{19} This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.^{20} However, there is a \"sweet spot\" for context length: DOS RAG's performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).^{17} This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.\nResearch on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.^{18} LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.^{18} In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in correctness.^{24} When rerankers like BGE-M3 are combined with higher retrieval@k values, more \"gold documents\" (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.^{24} However, increasing rerank@k with a fixed retrieval@k can introduce higher variation in correctness scores, ranging from 1% to 25%.^{24}\n5.2 Evaluation Benchmarks and Metrics\n5.2 Evaluation Benchmarks and Metrics\nThe field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).^{10} This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.^{10}\nAnother important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.^{11} Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.^{11} DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.^{15} For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval accuracy.^{35} Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.^{36}\nThese specialized benchmarks employ a variety of evaluation metrics. Accuracy is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.^{14} For more nuanced assessments, metrics like F1, BLEU-1, BLEU-4, ROUGE-L, and METEOR are employed, particularly for tasks like NarrativeQA.^{19} In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.^{16} The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks ^{10}, step-by-step procedure following ^{11}, and multi-hop questions ^{15}, demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like \"correctness\" and \"faithfulness\" further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.\nTable 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation\nBenchmark\nPrimary Focus\nKey Features Relevant to Chunk Order/Multi-Step QA\nKey Findings Related to Chunk Order\nRelevant Snippets\nMTI Bench\nMulti-Task Inference (sequential & non-sequential sub-tasks)\nEvaluates LLMs' ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.\nStronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.\n10\nProcBench\nMulti-Step Reasoning & Procedure Following\nDataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.\nHighlights critical gap in current assessments focusing exclusively on multi-step inference.\n11\n‚àûBench\nLong-Context Question Answering\nEvaluates performance under varying retrieval token budgets (1.5K to 40K tokens).\nDOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.\n19\nQuALITY\nLong-Context Question Answering (narrative understanding)\nRequires understanding underlying narrative rather than shallow pattern matching.\nFull-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.\n19\nNarrativeQA\nLong-Context Question Answering (narrative understanding)\nQuestions require understanding the underlying narrative.\nDOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).\n19\nDataMorgana\nQA-pair Generation (single-hop & multi-hop)\nCreates highly customizable synthetic benchmarks; used in LiveRAG Challenge.\nUsed to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.\n15\nmmRAG\nMulti-modal RAG Evaluation\nModular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.\nProvides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.\n35\nChunkRAG\nLLM-driven Chunk Filtering\nEnhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.\nOutperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.\n14\n6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\n6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\nTranslating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.\n6.1 Best Practices for Chunking and Reordering\n6.1 Best Practices for Chunking and Reordering\nTo optimize chunk retrieval sequence, a primary focus must be placed on prioritizing contextual coherence. For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.^{13} Given its consistent outperformance across various benchmarks, adopting DOS RAG as a baseline is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.^{19}\nWhile initial retrieval provides a set of relevant chunks, a strategic reranking step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.^{22} Furthermore, implementing inverted context ordering, where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.^{16}\nOptimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure ^{20}, effective reranking can still improve the selection of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).^{16} This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.\n6.2 Strategies for Mitigating Positional Bias and Distraction\n6.2 Strategies for Mitigating Positional Bias and Distraction\nTo effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages.^{25} This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, robust LLM fine-tuning with carefully selected \"hard distracting passages\" can significantly increase the LLM's accuracy and resilience against noise.^{28} Third, implementing LLM-driven chunk filtering (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.^{14} Fourth, for complex multi-step queries, query rewriting or decomposition into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.^{14} Finally, active context window management is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.^{30} Identifying the \"sweet spot\" for context length, where recall is maximized without introducing excessive noise, is also paramount.^{17}\nMitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the quality and focus of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.^{25} Simply reordering after retrieval is often insufficient to address these issues.^{26} Therefore, the solution must involve proactive measures. This includes improving the initial retrieval to be less prone to fetching distractors, and then employing strong filtering (like ChunkRAG) to eliminate noise before it ever reaches the LLM's context window.^{14} Furthermore, making the LLM itself more robust through fine-tuning with challenging examples ^{28} creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.\n6.3 Advanced Techniques for Multi-Hop Reasoning\n6.3 Advanced Techniques for Multi-Hop Reasoning\nAddressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, iterative retrieval becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.^{16} Adaptive retrieval mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.^{38} The integration of structured knowledge, such as graph-based RAG (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.^{35,39} This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of prompt-based reasoning chains like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM's reasoning process step-by-step, enhancing its ability to perform complex deductions.^6 These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.\n7. Conclusion and Future Directions\n7. Conclusion and Future Directions\nThe analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model's multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.\nThe implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy.\nFuture research and development should focus on several promising directions:\nAdaptive Retrieval Architectures: Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.^8\nReal-time Retrieval Integration: Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.^9\nStructured Reasoning over Multi-Hop Evidence: Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.^{6,39}\nRobustness to Adversarial Inputs: Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.^{8,28}\nCross-Modal and Multi-Lingual RAG: Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.^{4,35,40}\nEvaluation Methodologies: Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.^9\nThese future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.\nReferences\nReferences\nIBM. (n.d.). What Are Large Language Models (LLMs)?. Retrieved from https://www.ibm.com/think/topics/large-language-models#:~:text=LLMs%20consist%20of%20multiple%20layers,specific%20parts%20of%20data%20sets.\nKasneci, E., et al. (2023). Editorial ‚Äì The Use of Large Language Models in Science: Opportunities and Challenges. PMC. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC10485814/.\nIBM. (2023, November 2). What are Large Language Models (LLMs)?. Retrieved from https://www.ibm.com/think/topics/large-language-models.\nGoogle Cloud. (n.d.). What is Retrieval-Augmented Generation (RAG)?. Retrieved from https://cloud.google.com/use-cases/retrieval-augmented-generation.\nZhang, Y., et al. (2025). Empowering LLMs with Logical Reasoning: A Comprehensive Survey. arXiv. Retrieved from https://arxiv.org/html/2502.15652v2.\nZhang, Y., et al. (2025). Empowering LLMs with Logical Reasoning: A Comprehensive Survey. arXiv. Retrieved from https://arxiv.org/html/2502.15652v3.\nAWS. (n.d.). What is RAG (Retrieval-Augmented Generation)?. Retrieved from https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.\nSharma, C. (2025). Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers. arXiv. Retrieved from https://arxiv.org/html/2506.00054v1.\nSharma, C. (2025). Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers. arXiv. Retrieved from https://arxiv.org/abs/2506.00054.\nXu, K., et al. (2024). Multi-Task Inference: Do LLMs Hold the Capability to Handle Multiple Instructions Simultaneously?. arXiv. Retrieved from https://arxiv.org/html/2402.11597v2.\nAbout AI. (2024, May 22). How Well Do Large Language Models (LLMs) Actually Handle Multi-Step Reasoning?. Medium. Retrieved from https://medium.com/about-ai/how-well-do-large-language-models-llms-actually-handle-multi-step-reasoning-83ce37f7fc32.\nF22 Labs. (n.d.). 7 Chunking Strategies in RAG You Need To Know. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/#:~:text=Retrieval%20Augmented%20Generation%20(RAG)%20enhances,for%20faster%20retrieval%20and%20processing.\nF22 Labs. (n.d.). 7 Chunking Strategies in RAG You Need To Know. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/.\nMallen, E., et al. (2024). ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems. arXiv. Retrieved from https://arxiv.org/html/2410.19572v2.\nFilice, S., et al. (2025). RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition. arXiv. Retrieved from https://arxiv.org/html/2506.14412v1.\nFilice, S., et al. (2025). RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.14412.\nSuperAnnotate. (n.d.). RAG vs. Long Context LLMs. Retrieved from https://www.superannotate.com/blog/rag-vs-long-context-llms.\nLiu, Y., et al. (2024). The Impact of Premise Order on LLM Reasoning. arXiv. Retrieved from https://arxiv.org/html/2402.08939v1.\nCuconasu, F., et al. (2025, June 4). Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.03989.\nCuconasu, F., et al. (2025). Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models. arXiv. Retrieved from https://arxiv.org/html/2506.03989v1.\nDataCamp. (n.d.). Advanced RAG Techniques. Retrieved from https://www.datacamp.com/blog/rag-advanced.\nAnalytics Vidhya. (2025, March 28). Comprehensive Guide on Reranker for RAG. Retrieved from https://www.analyticsvidhya.com/blog/2025/03/reranker-for-rag/.\nGalileo AI. (n.d.). Mastering RAG: How to Select a Reranking Model. Retrieved from https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model.\nFilice, S., et al. (2025). RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition. arXiv. Retrieved from https://arxiv.org/html/2506.14412.\nCuconasu, F., et al. (2025). Do RAG Systems Suffer From Positional Bias?. arXiv. Retrieved from https://arxiv.org/html/2505.15561v1.\nCuconasu, F., et al. (2025). Do RAG Systems Suffer From Positional Bias?. arXiv. Retrieved from https://arxiv.org/abs/2505.15561.\nCuconasu, F., et al. (2025). Investigate positional bias in RAG systems, including the 'lost-in-the-middle' effect and impact of distracting passages. arXiv. Retrieved from https://arxiv.org/pdf/2505.15561.\nAmiraz, C., et al. (2025, May 11). Describe the 'distracting effect' of irrelevant passages in RAG and its impact on LLM accuracy. arXiv. Retrieved from https://arxiv.org/abs/2505.06914.\nAmiraz, C., et al. (2025). The Distracting Effect: Understanding Irrelevant Passages in RAG. arXiv. Retrieved from https://arxiv.org/html/2505.06914v1.\nLi, Y., et al. (2025). RAG-MCP: Retrieval-Augmented Generation for Model Context Protocol. arXiv. Retrieved from https://arxiv.org/html/2505.03275v1.\nZhang, M., et al. (2025). Cognitive-Aware LLM Streaming. arXiv. Retrieved from https://arxiv.org/html/2504.17999v1.\nLi, Y., et al. (2025). How does supplying only relevant context in RAG reduce cognitive load and improve LLM decision making?. arXiv. Retrieved from https://arxiv.org/pdf/2505.03275.\nWang, Y., et al. (2024). InfLLM: Enabling LLMs to Understand Extremely Long Sequences Without Any Fine-tuning. NeurIPS. Retrieved from https://neurips.cc/virtual/2024/poster/94480.\nXu, J., et al. (2025). Long Context vs. RAG for LLMs: An Evaluation and Revisits. arXiv. Retrieved from https://arxiv.org/html/2501.01880v1.\nChen, C., et al. (2025). mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs. arXiv. Retrieved from https://arxiv.org/html/2505.11180v1.\nZhang, Z., et al. (2024). RAGChecker: A Fine-grained Evaluation Framework for Retrieval-Augmented Generation. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html.\nDiamant, N. (n.d.). RAG_Techniques. GitHub. Retrieved from https://github.com/NirDiamant/RAG_Techniques.\nMallen, E., et al. (2024). Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models. ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-emnlp.831/.\nLi, H., et al. (2024). RAGRAPH: A General Retrieval-Augmented Graph Learning Framework. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/file/34d6c7090bc5af0b96aeaf92fa074899-Paper-Conference.pdf.\nReddit. (2024, March 17). Advanced Chunking/Retrieving Strategies for Legal Documents. Retrieved from https://www.reddit.com/r/Rag/comments/1jdi4sg/advanced_chunkingretrieving_strategies_for_legal/."
  },
  {
    "id": "21640d70fdf580fb9977ea94cfa745a5",
    "title": "Key Challenges in Current LLM Memory Systems",
    "kind": "page",
    "routePath": "/blog/list/key-challenges-in-current-llm-memory-systems",
    "text": "Why how AI remembers information can quietly shape its reasoning\nPeople often assume that when an AI gives inconsistent answers, it is simply forgetting information. In reality, many modern AI systems do remember past content, but they do so in ways that can subtly distort how reasoning unfolds.\nThis report provides a comprehensive review of the key challenges observed in memory systems for Large Language Models (LLMs) and LLM-based interactive agents. Covering both text-based systems and interactive multi-turn dialog agents, the report synthesizes insights from multiple research studies, benchmarks, and emerging approaches. The focus spans architectural challenges and evaluation metrics while also discussing innovative concepts such as dynamic context-aware embeddings and hierarchical memory structures. The following sections detail the conceptual challenges, prior research learnings, and future directions in the domain\nIntroduction\nIntroduction\nThe rapid evolution of LLMs over recent years has led to the integration of memory systems designed to augment models with retrieval-based and context-aware mechanisms. However, many of these systems remain limited by static representations, unordered retrieval processes, and lack a robust framework for managing evolving contexts. This review synthesizes findings from key research initiatives, including benchmarks like Minerva and HoH, advanced frameworks such as A-MEM and MemTree, and innovation in dynamic retrieval and hierarchical memory construction. The goal is to detail the foundational challenges and propose potential avenues for improvement, particularly in text-based LLM memory systems and interactive agent scenarios.\nCore Challenges in LLM Memory Systems\nCore Challenges in LLM Memory Systems\nThe following sections discuss each critical challenge along with representative methods and research learnings.\nUncontrollable Retrieval Order\nUncontrollable Retrieval Order\nProblem Statement\nProblem Statement\nMany retrieval-augmented systems (e.g., traditional RAG, MT-RAG, RICHES, SORT) retrieve information in an uncontrolled and static order. This lack of explicit control creates misaligned reasoning paths, particularly harmful in multi-step inference tasks. You can find sample experiments from my pervious blog.\nKey Challenges\nKey Challenges\nStatic Embeddings: The use of static embeddings does not permit dynamic adaptation or prioritization.\nUnordered Chunk Concatenation: Merging retrieved fragments without enforcing any sequence disrupts a coherent reasoning chain.\nResearch Insights\nResearch Insights\nThe \"My agent understands me better\" paper illustrates how human-like memory architectures can leverage exponential decay models (r(t)=Œºe^(‚Äìat)) to guide recall triggers, suggesting that a time-aware, relevance-based mechanism might control retrieval order.\nThe DH-RAG model‚Äôs incorporation of a History-Learning Based Query Reconstruction Module demonstrates that integrating dynamic historical context can adjust retrieval processes on the fly.\nPossible Solutions\nPossible Solutions\nImplementing dynamic pruning mechanisms (e.g., using techniques from ETH Z√ºrich‚Äôs work on context pruning) to remove uninformative tokens.\nEmploying multi-stage retrievers (as in A-MEM) that orchestrate retrieval in a more ordered and context-aware manner.\nFor everyday users, this means that two answers based on the same underlying facts can still differ in important ways‚Äîbecause the system may ‚Äúthink‚Äù through the facts in a different order.\nLack of Structured and Hierarchical Memory\nLack of Structured and Hierarchical Memory\nProblem Statement\nProblem Statement\nCurrent systems often treat retrieved memories as isolated, flat fragments. There is limited support for representing structural relationships such as hierarchical groupings or contextual dependencies.\nKey Challenges\nKey Challenges\nAbsence of Hierarchical Schemas: Memory remains an unstructured blob, making it challenging to derive composite reasoning from subcomponents.\nScalability Issues: Flat memory representations struggle to extend meaningfully across long sequences or multi-turn dialogues.\nFor users, this can show up as the AI failing to combine related details into a coherent explanation, or losing track of what matters most across a longer conversation.\nResearch Insights\nResearch Insights\nMemTree Framework: Both the Cornell University and Accenture works introduce tree-based memory representations that organize information hierarchically. These structures mimic human cognitive schemas and improve long-term integration.\nHAT Memory Structure: Employs a hierarchical aggregate tree that recursively aggregates dialogue context, thereby balancing information breadth with depth.\nOS-inspired memory management: Neeraj Kumar‚Äôs approach utilizes operating system concepts (e.g., FIFO queues and virtual memory) to manage hierarchical context.\nProposed Mechanisms\nProposed Mechanisms\nTree-based Dynamic Hierarchies: Enable organizations of conversational or document fragments into nodes, with insertion complexities managed in O(log N) time.\nGraph-based Structures: Using Directed Acyclic Graphs (DAGs), as demonstrated in Ye Ye‚Äôs Task Memory Engine, can further capture task relationships and semantic groupings.\nAbsence of Polymorphic and Context-Aware Representation\nAbsence of Polymorphic and Context-Aware Representation\nProblem Statement\nProblem Statement\nMemory representations in LLM systems often fail to adapt their output based on varying query types, user intents, or specific task contexts.\nKey Challenges\nKey Challenges\nRigid Representations: Lack of contextual flexibility restricts the reusability and dynamic tailoring of memory outputs.\nStatic Context Dependency: All queries are treated uniformly without personalization or context-dependent tuning.\nResearch Insights\nResearch Insights\nA-MEM Framework: Generates structured memory notes with metadata (time, context, keywords). This metadata-driven approach enables efficient re-adaptation based on the query.\nDynamic Context Pruning: Techniques from NeurIPS 2023 have shown that pruning redundant information can help tailor representations to current task requirements.\nLEAP Approach: By inducing error-based introspection and explicit task principle extraction, LLMs can improve context adaptability and embed dynamic learning perspectives.\nPotential Improvements\nPotential Improvements\nPolymorphic Embeddings: Development of embeddings that can change form based on context, as suggested by experimental frameworks in dynamic multimodal RAG systems.\nMetadata-rich Memory Notes: Systematic categorization and tagging (e.g., through the InSeNT approach) produce more flexible memory retrieval capabilities.\nInability to Handle Redundancy, Conflicts, or Salience\nInability to Handle Redundancy, Conflicts, or Salience\nProblem Statement\nProblem Statement\nRetrieved memory often contains redundant, irrelevant, or conflicting fragments. Current memory architectures struggle to resolve these issues, leading to degraded reasoning.\nKey Challenges\nKey Challenges\nContent Filtering: Difficulty in filtering irrelevant memory chunks or resolving conflicts.\nSalience Modeling: Lack of mechanisms to prioritize salient information for downstream tasks.\nResearch Insights\nResearch Insights\nExt2Gen and CoV-RAG: These representative methods emphasize content selection as a critical step in managing retrieved information.\nDynamic Multimodal RAG (Dyn-VQA, OmniSearch): Introduces a self-adaptive planning agent that partitions complex queries into sub-questions, thereby reducing overload and redundant retrieval.\nIn HoH Benchmark studies, dynamic evaluation revealed that outdated information can reduce performance by at least 20%, underscoring the need for effective conflict resolution.\nStrategies for Resolution\nStrategies for Resolution\nTwo-stage Diff Algorithms: Techniques such as those used in HoH Benchmark (using token-level diff) can identify and effectively remove conflicting or outdated data.\nSimilarity Thresholding and Clustering: Methods from MemTree and HAT demonstrate that cosine similarity thresholds scaled with depth can fingerprint and remove unnecessary details.\nNo Lifecycle Management or Update Mechanism\nNo Lifecycle Management or Update Mechanism\nProblem Statement\nProblem Statement\nCurrent memory pools do not distinguish between temporary and persistent information. Without proper lifecycle management, outdated or irrelevant memories persist, contaminating future reasoning.\nKey Challenges\nKey Challenges\nAbsence of Version Control: There is no systematic approach for updating, retiring, or overwriting memory content.\nMemory Hygiene Issues: Continuous accumulation without lifecycle awareness leads to inefficiencies and potential reasoning errors.\nResearch Insights\nResearch Insights\nDynamic Consolidation in \"My agent understands me better\": By setting recall triggers and leveraging memory decay, the system dynamically updates memory relevance.\nRAM, Memory¬≥, and SEAKR Models: These approaches introduce mechanisms for segregating and updating memories through periodic review and consolidation.\nLLMOps Frameworks: Platforms such as LangSmith and Weights & Biases provide version control, logging, and metric tracking to manage the entire LLM lifecycle‚Äîfrom data curation to deployment.\nRecommended Solutions\nRecommended Solutions\nMemory Versioning Systems: Similar to traditional OS systems that flush obsolete data, LLM memory can incorporate triggers for rewriting or summarizing outdated content.\nRecursive Summarization: Techniques applied in OS-inspired memory management can recursively summarize old memory segments to keep the active context optimized.\nPoor Interpretability and Traceability\nPoor Interpretability and Traceability\nProblem Statement\nProblem Statement\nUsers frequently encounter opaque memory usage paths. The underlying reasons for retrievals remain hidden, leading to difficulties in debugging and ensuring accountability.\nKey Challenges\nKey Challenges\nOpaque Retrieval Process: Dense vectors and black-box retrievers obscure the influence of retrieved memory on generated responses.\nLack of Explainability: There is minimal information regarding why certain chunks were prioritized or how they were integrated into responses.\nResearch Insights\nResearch Insights\nWISE and R1-Searcher: These representative methods highlight the need for tools to visualize and interpret retrieval actions.\nDynamic Context Pruning: By highlighting which tokens are pruned (e.g., via learnable sparsification mechanisms), researchers have begun to improve model interpretability.\nData-centric Debugging: Techniques such as OLMoTrace facilitate tracing errors back to training examples, a method that can be extended to memory systems for improved transparency.\nMitigation Strategies\nMitigation Strategies\nVisualization Dashboards: Implement dashboards that track and present the memory retrieval pathway in real-time.\nToken-level Attribution: Adapt token-level diff methods (as used in HoH Benchmark) to create transparent logs of memory integration events.\nIterative Debugging: Combine data-focused and model-focused debugging to trace output discrepancies back to memory retrieval processes.\nModality- and Task-Specific Limitations\nModality- and Task-Specific Limitations\nProblem Statement\nProblem Statement\nMany memory systems are specifically designed for text-based queries, leading to challenges in generalizing to multi-modal or interactive agent scenarios.\nKey Challenges\nKey Challenges\nSpecialization of Retrievers: Systems like Video-RAG and VisDoMBench reveal that current retrievers are highly specialized for specific modalities.\nUnified Abstraction Deficit: A lack of centralized mechanisms to integrate memory across text, visuals, and interactions restricts broader applicability.\nResearch Insights\nResearch Insights\nDynamic Multimodal RAG and OmniSearch: Introduce self-adaptive methods that can handle questions with rapidly changing, multi-modal contexts.\nCAMU Framework: Combines vision‚Äìlanguage models with multimodal grounding to capture cultural nuances and complex interactions in hateful meme detection.\nAugmented Object Intelligence (AOI): XR-Objects exemplify how real-world objects can be transformed into interactive entities within XR environments, hinting at the potential for unified memory abstractions.\nImprovement Pathways\nImprovement Pathways\nCentralized Multi-modal Memory Frameworks: Architect systems that seamlessly integrate structured text-based memories with visual and interactive data.\nTask-specific Adaptation Layers: Use data-centric approaches so that memory representations adjust based on the modality‚Äîwhether it is pure text or an interactive scenario.\nEvaluation Metrics and Benchmarks\nEvaluation Metrics and Benchmarks\nBelow is a summary table of identified benchmarks and evaluation metrics from various research studies:\nBenchmark/Method\nFocus Area\nKey Metrics and Techniques\nNotable Models Tested\nMinerva\nComprehensive memory evaluation (atomic & composite tasks)\nExact match accuracy, ROUGE-L, Jaccard similarity\nGPT-4 variants, Cohere, LLaMA, Mistral\nHoH Benchmark\nDynamic QA and outdated information impact\nToken-level diff (Myers), accuracy (96.8%), F1 (95.1%)\nQwen2.5-0.5B, mainstream LLMs\nConTEB & InSeNT\nContextual document embedding evaluation\nDocument-wide context sensitivity, recall/edit scores\nVarious embedding models across datasets\nDynamic Context Pruning (NeurIPS)\nEfficient autoregressive transformers\nInference throughput (up to 2√ó), latency reduction\nPre-trained transformer models\nDyKnow\nDetection of outdated factual knowledge\nValidity start-years, factual accuracy comparisons\nGPT-4, GPT-J, ChatGPT, Llama-2\nThese evaluation strategies provide actionable insights into both basic retrieval capabilities and composite memory utilization challenges. They emphasize the need to balance model performance with interpretability and dynamic memory adaptability.\nEmerging Approaches and Novel Strategies\nEmerging Approaches and Novel Strategies\nThe literature indicates several innovative trends and experimental methods aimed at overcoming the limitations of current LLM memory systems:\nDynamic Context-Aware Embeddings\nDynamic Context-Aware Embeddings\nError-based Introspection (LEAP): Inducing models to reflect on mistakes improves context-driven reasoning.\nAdaptive Cosine Similarity Thresholds: Employed in tree-structured frameworks like MemTree to decide on memory insertion paths.\nLearnable Pruning Modules: Dynamically remove uninformative tokens to save computational resources and enhance interpretability.\nHierarchical Memory and Tree Structures\nHierarchical Memory and Tree Structures\nMemTree and HAT Architectures: These models organize memories in tree or hierarchical formats that allow for effective aggregation and multi-turn dialogue management.\nGraph-based Memory Representations: DAG-based systems (e.g., Ye Ye‚Äôs Task Memory Engine) offer improvements in multi-step tasks by modeling tasks as spatial graphs.\nLifecycle Management and Versioning\nLifecycle Management and Versioning\nOS-inspired Memory Management: Concepts from operating systems are applied to manage LLM memory, introducing FIFO queues, context flushing, and recursive summarization.\nLLMOps and Continuous Monitoring: Tools and frameworks streamline the lifecycle from data curation to deployment, offering continuous feedback loops and version control.\nMulti-modal and Interactive Agent Integration\nMulti-modal and Interactive Agent Integration\nDynamic Multimodal RAG: Systems such as Dyn-VQA and OmniSearch expand retrieval strategies beyond text, providing self-adaptive query decomposition.\nAugmented Reality Integration (AOI/XR-Objects): Approaches that bridge digital and analog experiences by representing real-world objects as interactive digital entities.\nDiscussion and Future Directions\nDiscussion and Future Directions\nSummary of Key Findings\nSummary of Key Findings\nCurrent LLM memory systems suffer from several intertwined challenges: uncontrolled retrieval order, unstructured memory pools, rigid representations, and insufficient interpretability.\nDynamic and hierarchical models (e.g., MemTree, HAT) show promising potential in addressing structural deficiencies.\nEvaluation benchmarks like Minerva and HoH highlight significant performance gaps, especially for composite tasks and outdated information scenarios.\nEmerging methods, including learnable pruning and adaptive context-aware embeddings, offer promising strategies to overcome present limitations.\nFuture Research Priorities\nFuture Research Priorities\nUnified Memory Abstractions: Develop memory systems that can transition seamlessly between text-based, multi-modal, and interactive scenarios.\nEnhanced Transparency: Focus on explainability by integrating data-centric debugging tools and real-time visualization dashboards.\nLifecycle and Version Control: Implement robust update mechanisms drawing from OS-inspired and LLMOps frameworks to keep memory pools clean and relevant.\nEvaluation Expansion: Broaden the evaluation metrics and benchmarks to include interactive agent scenarios and multimodal tasks.\nRecommendations for Practitioners\nRecommendations for Practitioners\nLeverage frameworks like A-MEM and dynamic hierarchical structures to enable more detailed, context-aware retrieval strategies.\nApply continuous monitoring and data-centric debugging techniques to ensure sustained performance and memory accuracy over time.\nIncorporate emerging dynamic pruning and consolidation mechanisms to strike a balance between inference speed and memory quality.\nWhy this matters beyond system design\nWhy this matters beyond system design\nThese challenges are not only engineering details. They shape how much you can trust an AI system in everyday use: a model can sound confident while its reasoning quietly shifts depending on what it retrieves, in what order, and how it organizes what it ‚Äúremembers‚Äù.\nConclusions\nConclusions\nThis review has detailed the significant challenges in existing LLM memory systems, drawing on extensive research literature and emerging techniques. From uncontrollable retrieval orders to the need for dynamic, hierarchical memory management, current approaches highlight critical limitations that impede coherent reasoning and multimodal generalization. The integration of dynamic context-aware mechanisms, structured memory hierarchies, and lifecycle management frameworks promises to drive future advances.\nLooking ahead, combining research insights with practical implementations, backed by robust evaluation benchmarks, will be crucial for developing LLM memory systems that are powerful, interpretable, adaptable, and scalable across diverse applications.\nBy synthesizing findings from a range of studies and emerging benchmarks, this report provides a detailed roadmap for both researchers and practitioners seeking to overcome current limitations and pioneer the next generation of LLM memory management.\nReferences\nReferences\nhttps://arxiv.org/html/2404.00573v1\nhttps://bdtechtalks.substack.com/p/how-to-create-an-optimal-memory-structure\nhttps://arxiv.org/html/2502.03358v2\nhttps://arxiv.org/html/2503.04800v1\nhttps://openreview.net/forum?id=VvDEuyVXkG\nhttps://arxiv.org/html/2502.13847v1\nhttps://arxiv.org/abs/2410.14052\nhttps://arxiv.org/html/2305.15805v3\nhttps://arxiv.org/html/2505.24782v1\nhttps://neerajku.medium.com/memgpt-extending-llm-context-through-os-inspired-virtual-memory-and-hierarchical-storage-c5cc96f9818a\nhttps://arxiv.org/html/2410.14052v1\nhttps://arxiv.org/html/2505.19436v1\nhttps://arxiv.org/html/2404.08700v1\nhttps://arxiv.org/html/2502.03358v1\nhttps://cameronrwolfe.substack.com/p/llm-debugging\nhttps://www.useready.com/blog/mastering-llm-lifecycle-management-with-llmops\nhttps://smartrdm.com/blog/managing-the-lifecycle-of-large-language-model/\nhttps://arxiv.org/html/2406.06124v1\nhttps://arxiv.org/html/2411.16003v1\nhttps://arxiv.org/html/2504.17902v1\nhttps://arxiv.org/html/2404.13274v4\nhttps://arxiv.org/html/2402.05403v2\nhttps://openreview.net/forum?id=uvdJgFFzby&noteId=q7Ub8yAmoc\nhttps://arxiv.org/abs/2305.15805\nhttps://neurips.cc/virtual/2023/poster/70129"
  },
  {
    "id": "21040d70fdf580878bd6ca6b918034e0",
    "title": "The Surprising Impact of Memory Order on LLM Responses",
    "kind": "page",
    "routePath": "/blog/list/the-surprising-impact-of-memory-order-on-llm-responses",
    "text": "Why the sequence of information can quietly change how AI reasons\nYou ask an AI the same question twice, using the same information, but you get two different answers. Nothing obvious has changed, yet the response feels subtly off. This kind of inconsistency often surprises users, but it reveals something important about how AI systems reason.\nYou can think of this like a human trying to solve a problem. If you recall background facts first and conclusions later, your reasoning may unfold differently than if you start from conclusions and work backward. AI systems can behave in a similar way when the same \"memory\" is presented in a different order.\nWhat We Discovered: LLMs Have \"Path Dependence\"\nWhat We Discovered: LLMs Have \"Path Dependence\"\nWe've been experimenting with memory-augmented LLMs, and we stumbled upon something fascinating: the order in which we feed memory snippets to the model dramatically changes its output - even when the actual information is identical.\nI call this \"path dependence\" - the LLM's journey through information matters just as much as the information itself.\nLook at this heatmap above - it shows how responses vary when we change the order of memory slots. The darker areas show greater differences in outputs. Pretty striking, right?\nOur Experimental Setup\nOur Experimental Setup\nHere's how we tested this:\nWe created a set of memory slots containing factual information (e.g., about FDR's New Deal and Reagan's economic policies)\nWe arranged these slots in different sequences (paths)\nWe asked GPT-4 identical questions with these different memory paths\nWe measured the differences in outputs using embedding similarity\nFor example, one of our memory slots looked like this:\n[Memory Slot] FDR launched the New Deal to combat the Great Depression. It involved government intervention and public programs.\nAnd we'd change the order of 4-5 such slots to see how it affected the response to questions like \"Compare FDR's New Deal with Reagan's economic policy.\"\nHow We Measured This Effect\nHow We Measured This Effect\nWe tracked several metrics:\nSemantic differences using sentence embeddings (all-MiniLM-L6-v2)\nStructural changes including word count, sentence count, and average sentence length\nQuality scores using GPT-4 to evaluate structure, reasoning, and conclusion clarity (0-10 scale)\nCheck out this visualization of our results:\nEach dot represents an LLM response, clustered by writing style. Notice how they form distinct groups? Different memory paths actually pushed the LLM toward different writing styles!\nThe Most Interesting Findings\nThe Most Interesting Findings\nHere's what jumped out at us:\nFirst impressions matter: When we put FDR-related slots first, responses tended to frame comparisons from a government intervention perspective. When Reagan-related slots came first, responses emphasized market-based approaches.\nStyle shifting: Some memory paths consistently produced verbose responses (avg. 150+ words), while others led to more concise writing (avg. 100-120 words).\nQuality remained solid: Despite variations, the responses maintained good quality scores (8-10 range) across different paths.\nNoise tolerance: Adding irrelevant memory slots (like facts about the moon landing) increased variability but didn't completely derail responses.\nThis scatter plot shows the relationship between similarity scores and structural features:\nWhy This Matters\nWhy This Matters\nIf you're building or using LLM systems, this has real implications:\nConsistency challenges: If you need reliable, predictable outputs, you need to think about memory ordering\nDesign opportunities: This could be used creatively to generate diverse perspectives on the same information\nUser awareness: Users should understand that the way they present information to LLMs affects what they get back\nWhat's Next?\nWhat's Next?\nWe're exploring several directions:\nBuilding memory routing systems that can provide more consistent outputs\nTesting whether some models are more \"path dependent\" than others\nFinding ways to use this property as a feature rather than a bug\nThe Takeaway\nThe Takeaway\nThe next time you interact with an LLM assistant, remember that the order of information matters - a lot! This isn't just an academic curiosity; it's a fundamental aspect of how these systems process information.\nAs we continue to integrate LLMs into more aspects of our lives, understanding these quirks becomes increasingly important. Path dependence isn't just a technical issue - it's a reminder that LLMs, like humans, don't process information in a purely logical, order-independent way.\nWhat this means for everyday AI users\nWhat this means for everyday AI users\nAI answers are shaped by internal processes, not just by what information is available. Small changes in the sequence of context can quietly shift the reasoning path, even when the facts are the same. A confident-sounding answer doesn't necessarily mean the model would stay consistent under a different ordering. If something feels inconsistent, try re-asking with the same key facts in a different order or asking a short follow-up question to probe what the model is relying on.\nWhat do you think? Have you noticed this effect in your interactions with LLMs?"
  },
  {
    "id": "21040d70fdf5807b8ae1c046f39753ed",
    "title": "Do Language Model Embeddings Form an Approximate Abelian Group?",
    "kind": "page",
    "routePath": "/blog/list/do-language-model-embeddings-form-an-approximate-abelian-group",
    "text": "For non-expert readers\nFor non-expert readers\nThis post explores a theoretical question about how language models represent meaning internally. The main text uses mathematical concepts to examine whether word and sentence representations behave like a well-structured system, but the broader idea is simple: small assumptions about internal structure can strongly influence how models combine and transform meaning.\nIf you don't have a technical background, the key takeaway is that not all intuitive or elegant mathematical frameworks fully capture how language models actually work. Understanding where these abstractions break down helps explain why models can behave inconsistently when combining ideas, even if they perform well on surface tasks.\nIf you're only here for a high-level takeaway, you can stop after this section.\nAbstract\nAbstract\nDo language model embeddings follow a deeper mathematical structure, perhaps resembling an abelian group from abstract algebra? In this post, we explore whether embeddings from models like GPT-2 exhibit such properties. Our empirical study evaluates five group axioms using vector addition as the operation. The results show near-perfect adherence, with over 99 percent satisfaction across all tests.\nIntroduction\nIntroduction\nLanguage model embeddings have long attracted attention for their geometric properties, such as analogy solving and linear interpolation. But can we go further? Can these spaces be described using formal algebraic structure?\nThis investigation tests whether the embedding space behaves like an abelian group, which is a well-defined mathematical object characterized by five axioms. This perspective provides a new lens for understanding how language models encode and manipulate meaning.\nMethodology\nMethodology\nExperimental Framework\nExperimental Framework\nWe define a group candidate as(G, \\oplus) where G is the set of all embeddings and \\oplus is approximated by vector addition: \\text{embed}(x \\oplus y) \\approx \\text{embed}(x) + \\text{embed}(y)\nThe five axioms tested are:\nClosure: \\forall a, b \\in G: a \\oplus b \\in G\nIdentity: \\exists e \\in G: \\forall a \\in G, a \\oplus e = a\nInverse: \\forall a \\in G: \\exists a^{-1} \\in G, a \\oplus a^{-1} = e\nCommutativity: \\forall a, b \\in G: a \\oplus b = b \\oplus a\nAssociativity: \\forall a, b, c \\in G: (a \\oplus b) \\oplus c = a \\oplus (b \\oplus c)\nEach axiom is evaluated by generating text combinations and comparing the embeddings using cosine similarity.\nData Generation\nData Generation\nWe constructed over 500 sentence pairs from the following categories:\nArithmetic expressions such as ‚Äú2 plus 3‚Äù\nSentence conjunctions such as ‚ÄúCats and dogs‚Äù\nNegations such as ‚ÄúNot true‚Äù\nComparisons such as ‚ÄúTaller than‚Äù\nModels Tested\nModels Tested\nGPT-2 (117M parameters): Base transformer model\nDistilGPT-2 (82M parameters): Distilled version for efficiency comparison\nEvaluation Metrics\nEvaluation Metrics\nWe employ cosine similarity as the primary metric for evaluating axiom satisfaction:\nA threshold of \\tau = 0.95 is used for determining axiom satisfaction. For each axiom test, we compute:\nwhere \\mathbf{1}[\\cdot] is the indicator function, and (u_i, v_i) are the expected and observed embedding pairs. Additionally, we compute semantic consistency scores to ensure meaningful preservation of linguistic relationships.\nResults\nResults\nLarge-Scale Experiment (500 samples)\nLarge-Scale Experiment (500 samples)\nOur comprehensive evaluation on GPT-2 with 500 samples per axiom category yielded exceptional results:\n| Axiom | Samples | Satisfaction Rate | Mean Similarity | | ------------- | ------- | ----------------- | --------------- | | Closure | 500 | 100.0% | 0.995 | | Identity | 100 | 100.0% | 0.993 | | Inverse | 500 | 100.0% | 0.997 | | Commutativity | 500 | 100.0% | 0.994 | | Associativity | 100 | 100.0% | 0.998 |\n| Axiom | Samples | Satisfaction Rate | Mean Similarity | | ------------- | ------- | ----------------- | --------------- | | Closure | 500 | 100.0% | 0.995 | | Identity | 100 | 100.0% | 0.993 | | Inverse | 500 | 100.0% | 0.997 | | Commutativity | 500 | 100.0% | 0.994 | | Associativity | 100 | 100.0% | 0.998 |\nOverall Satisfaction Rate: 100.0%\nMulti-Model Comparison\nMulti-Model Comparison\nBoth GPT-2 and DistilGPT-2 demonstrated identical performance:\n| Model | Overall Rate | Processing Time | | ----------- | ------------ | --------------- | | GPT-2 | 100.0% | 63.1 seconds | | DistilGPT-2 | 100.0% | 34.4 seconds |\n| Model | Overall Rate | Processing Time | | ----------- | ------------ | --------------- | | GPT-2 | 100.0% | 63.1 seconds | | DistilGPT-2 | 100.0% | 34.4 seconds |\nPerformance Analysis\nPerformance Analysis\nThe experiments revealed several key insights:\nComputational Efficiency: DistilGPT-2 achieved identical mathematical structure preservation while requiring ~45% less computation time\nSemantic Consistency: High semantic similarity scores (>0.99) indicate that group operations preserve meaningful linguistic relationships\nScalability: Performance remained consistent across different sample sizes (100, 200, 500 samples)\nTechnical Implementation\nTechnical Implementation\nProgress Tracking\nProgress Tracking\nThe framework includes automatic fallback mechanisms:\nPrimary execution on Apple Silicon (MPS) when available\nGraceful fallback to CPU for unsupported operations\nOptimized memory management for large-scale experiments\nVisualization\nVisualization\nGenerated visualizations include:\nInteractive embedding space plots (t-SNE)\nOperation trajectory visualizations\nPerformance radar charts\nError distribution analysis\nAdditionally, we provide interactive visualizations including:\nDiscussion\nDiscussion\nImplications for Language Understanding\nImplications for Language Understanding\nThe near-perfect satisfaction of abelian group axioms suggests that transformer embeddings naturally encode mathematical structure that extends beyond simple vector arithmetic. This finding has several implications:\nCompositional Semantics: The group structure provides a formal foundation for understanding how meaning composes in embedding spaces. If \\text{embed}(A) and \\text{embed}(B) represent semantic concepts, then: \\text{embed}(A \\text{ and } B) \\approx \\text{embed}(A) \\oplus \\text{embed}(B)\nAlgebraic Reasoning: Language models may inherently perform algebraic operations when processing linguistic relationships. The commutativity property ensures: \\text{embed}(\\text{``A and B\"}) \\approx \\text{embed}(\\text{``B and A\"})\nTransfer Learning: The mathematical structure could explain the effectiveness of pre-trained embeddings across diverse tasks. The group properties ensure that semantic relationships are preserved under the embedding transformation \\phi: \\text{Language} \\rightarrow \\mathbb{R}^d.\nLimitations\nLimitations\nWhile our results are promising, several limitations warrant consideration:\nThreshold Sensitivity: The \\tau = 0.95 cosine similarity threshold, while standard, may influence results. Future work should explore the relationship: \\text{satisfaction\\_rate}(\\tau) = f(\\tau)\nSample Diversity: Future work should explore more diverse linguistic constructions and test the robustness of the group structure across different domains.\nCross-Lingual Analysis: Testing group structure across different languages to determine if the mathematical properties are universal or language-specific.\nLarger Models: Investigating whether group structure scales to larger transformer models and whether the relationship holds: \\lim_{|\\theta| \\rightarrow \\infty} \\text{group\\_satisfaction}(\\mathcal{M}_\\theta) = 1 where \\mathcal{M}_\\theta represents a model with parameters \\theta.\nMathematical Summary\nMathematical Summary\nOur empirical investigation demonstrates that transformer embeddings satisfy the abelian group axioms with remarkable consistency. Formally, we have shown that the embedding space (E, \\oplus)where E = \\{\\text{embed}(x) : x \\in \\text{Language}\\} and \\oplus represents vector addition, satisfies:\nThis establishes (E, \\oplus) as an empirical abelian group with high confidence.\nConclusion\nConclusion\nSo, do language model embeddings form an approximate abelian group?\nEmpirically, yes.\nAcross five axioms, two models, and hundreds of examples, transformer embeddings showed remarkably consistent group-like behavior. While the term ‚Äúgroup‚Äù is used informally here, the results suggest an underlying structure that goes beyond intuitive vector arithmetic.\nThis might just be the beginning of understanding the mathematics of meaning in large language models."
  },
  {
    "id": "bd838cdf0d974ea59268a1c6b353fc20",
    "title": "BIO",
    "kind": "page",
    "routePath": "/bio",
    "text": "Jinkun Chen (he/him/his) is a Ph.D. student in Computer Science at Dalhousie University, specializing in Explainable AI, Natural Language Processing (NLP), and Visualization. He earned a bachelor's degree in Computer Science with First-Class Honours from Dalhousie University. Jinkun is actively involved in research, working on advancing fairness, responsibility, trustworthiness, and explainability within Large Language Models (LLMs) and AI. In addition to his academic pursuits, Jinkun also serves as an AIS Data Analyst at MERIDIAN and is a valuable member of the HyperMatrix Lab and MALNIS Lab, all of which contribute to his research-related activities.\nüè¢ Academic Affiliations\nüè¢ Academic Affiliations\nModeling and Analytics on Predictive Systems (MAPS) Lab\nMaritime Risk and Safety Research Group (MARS)\nMERIDIAN, Institute for Big Data Analytics\nüë®‚Äçüéì Education\nüë®‚Äçüéì Education\nOn-Campus\nOn-Campus\nPhD of Computer Science (Post Bachelor‚Äôs) Dalhousie University Jan 2022 - Now\nBachelor of Computer Science (First-Class Honour) Dalhousie University Sept 2016 - Sept 2020 Minor in Math Certified in Data Science and Data Analytics Honour Advisor: Dr. Fernando Paulovich Honour Thesis Reader: Dr. Amilcar Soares\nOff-Campus\nOff-Campus\nProject Management Professional (PMP) May 2023 - Now\nPrivate Pilot License (PPL) Feb 2024 - Now\nüöÄ Certification\nüöÄ Certification\nTCPS2\nAdvanced Open Water Driver"
  },
  {
    "id": "428aa769b98042adb1aa84daa3d62d7b",
    "title": "Connect",
    "kind": "page",
    "routePath": "/connect",
    "text": "This page helps verify my online identity and provides ways to connect with me.\nüåê Online Identities\nüåê Online Identities\nThese domains and email addresses are my official digital identities. If you receive communication from these sources, you can verify it's genuinely from me.\nüìß Email Addresses\nüìß Email Addresses\njinkun.chen@dal.ca ‚Äî Academic & Dalhousie\ni@jinkunchen.com ‚Äî Personal\njinkun.chen@exorcat.com ‚Äî Exorcat\njinkun@donutbrowser.ai ‚Äî Donut Browser\njinkunc@acm.org ‚Äî ACM\nüîó Domains\nüîó Domains\njinkunchen.com ‚Äî Primary website\nchenjinkun.com\njinnkunn.com\njinnkunn.eth ‚Äî Web3\njinkun.blue ‚Äî Web3\nüí¨ Social Media\nüí¨ Social Media\nI'm active on these platforms for professional networking and sharing research updates.\nLinkedIn ‚Äî Professional profile\nX/Twitter ‚Äî Research updates & thoughts\nGitHub ‚Äî Code & projects"
  },
  {
    "id": "17740d70fdf5809e9d90e23f24baeb63",
    "title": "Yimen Chen (‰πâÈó®Èôà) Lineage",
    "kind": "page",
    "routePath": "/chen",
    "text": "\"ÁôæÁä¨ÂêåÊßΩËÄåÈ£üÔºå‰∏ÄÁä¨‰∏çËá≥ÔºåÁôæÁä¨‰∏çÈ£ü\" \"A hundred dogs share the same trough; if one is absent, the rest will not eat.\" This ancient saying embodies the extraordinary unity of the Yimen Chen family.\nüìñ Introduction\nüìñ Introduction\nThe Yimen Chen (‰πâÈó®Èôà) lineage represents one of the most extraordinary examples of collective living and Confucian family values in Chinese history. For over three centuries (731-1062 CE), this remarkable family maintained unity across fifteen generations, with thousands of members sharing property, meals, and daily life under a single household system.\nAt its peak, the Yimen Chen family comprised over 3,900 individuals living together without private property or separate kitchens. Their achievement attracted imperial recognition from successive dynasties and praise from renowned scholars including Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi.\nThe family upheld core Confucian values of loyalty, filial piety, integrity, and righteousness. Their governance system, codified in the famous \"33 Family Rules,\" became a model for family management throughout China. The saying \"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" (\"All Chen families under heaven originate from Jiangzhou\") reflects their lasting influence, as descendants from the 1062 CE division now inhabit communities across China and beyond.\nüèõÔ∏è Historical Origins\nüèõÔ∏è Historical Origins\nFounded: 731 CE (Tang Dynasty, Kaiyuan 19th year) Location: Putang Village, Jiangzhou (present-day Yimen Village, De'an County, Jiangxi Province) Founder: Chen Wang (ÈôàÊó∫)\nThe lineage traces its ancestry to Chen Baxian (ÈôàÈú∏ÂÖà), the founding emperor of the Chen Dynasty (557-589 CE). Chen Wang, a descendant of Chen Baxian's brother Chen Tanxian, established the family settlement in what would become known as Yimen (Áæ©ÈñÄ, \"Righteous Gate\").\nüìú Imperial Recognition In 884 CE (Tang Dynasty, Zhonghe 4th year), Emperor Xizong personally bestowed the title \"Yimen Chen Clan\" (‰πâÈó®ÈôàÊ∞è) upon the family, marking the beginning of imperial recognition that would continue through successive dynasties.\nüìú Imperial Recognition In 884 CE (Tang Dynasty, Zhonghe 4th year), Emperor Xizong personally bestowed the title \"Yimen Chen Clan\" (‰πâÈó®ÈôàÊ∞è) upon the family, marking the beginning of imperial recognition that would continue through successive dynasties.\nüåü Remarkable Achievements\nüåü Remarkable Achievements\nThe Yimen Chen family created an unprecedented model of collective living that astonished the entire nation and was celebrated by emperors and scholars alike.\nKey Facts\nKey Facts\nDuration: 15 generations living together (332 years, approximately 731-1062 CE)\nPopulation: Grew from 700+ members (947-957 CE) to 3,900+ members by 1062 CE)\nShared Living: All members shared meals from a common kitchen and held no private property)\nCore Principles: \"ÂÆ§Êó†ÁßÅË¥¢ÔºåÂé®Êó†Âà´Áà®\" (No private wealth in rooms; no separate cooking in kitchens)\n‚öñÔ∏è Governance System\n‚öñÔ∏è Governance System\nThe Yimen Chen family established a sophisticated governance system characterized by fairness, education, and moral cultivation.\n33 Family Rules (ÈôàÊ∞èÂÆ∂Ê≥ï)\n33 Family Rules (ÈôàÊ∞èÂÆ∂Ê≥ï)\nThe family's 33 rules (ÈôàÊ∞èÂÆ∂Ê≥ï) were so well-regarded that in 1026 CE, Emperor Renzong of Song ordered them to be preserved in the Imperial Archives and distributed to all high officials as a model for family governance.\nThese rules emphasized:\nFilial piety and respect for elders\nCollective property and equitable distribution\nDiligence and frugality\nEducation and moral cultivation\n‰∏ú‰Ω≥‰π¶Èô¢ (Dongjia Academy)\n‰∏ú‰Ω≥‰π¶Èô¢ (Dongjia Academy)\nThe family founded the Dongjia Academy (‰∏ú‰Ω≥‰π¶Èô¢), one of China's earliest private higher education institutions. The academy was renowned for its extensive collection of books and calligraphy, described as unparalleled in the region.\n\"ÂÖ´ÁôæÂ§¥ÁâõËÄïÊó•ÊúàÔºå‰∏âÂçÉÁÅØÁÅ´ËØªÊñáÁ´†\" \"Eight hundred oxen plow day and night; three thousand lanterns illuminate the study of literature.\" ‚Äî Song Dynasty official Lv Duan, describing the family's prosperity and dedication to education\nüåç The Great Division (1062 CE)\nüåç The Great Division (1062 CE)\nIn 1062 CE (Song Dynasty, Jiayou 7th year), after the family population exceeded 3,900 members, Emperor Renzong‚Äîurged by prominent ministers including Wen Yanbo (ÊñáÂΩ¶Âçö) and Bao Zheng (ÂåÖÊãØ)‚Äîissued an imperial decree for the family to divide.\nThe Division Process\nThe Division Process\nThe division took nine months to plan (July 1062 to March 1063). The family property was divided into:\n291 estates spread across 16 provinces and 125 counties\nEach branch drew lots to determine their new locations\nBranches used a 12-character generational naming system bestowed by Emperor Taizong: \"Áü•ÂÆàÂÆó„ÄÅÂ∏åÂÖ¨Ê±ù„ÄÅÊâçÊÄùÂΩ¶„ÄÅÊâøÂª∂Áªß\"\n\"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" \"All Chen families under heaven originate from Jiangzhou\" This saying reflects the far-reaching influence of the Yimen Chen division. Descendants settled across China, with each family displaying a \"Yimen Shijia\" (‰πâÈó®‰∏ñÂÆ∂, \"Yimen Noble Family\") plaque above their doors.\n\"Â§©‰∏ãÈôàÊ∞èÂá∫Ê±üÂ∑û\" \"All Chen families under heaven originate from Jiangzhou\" This saying reflects the far-reaching influence of the Yimen Chen division. Descendants settled across China, with each family displaying a \"Yimen Shijia\" (‰πâÈó®‰∏ñÂÆ∂, \"Yimen Noble Family\") plaque above their doors.\nüåü Modern Legacy\nüåü Modern Legacy\nToday, the Yimen Chen lineage continues to be a source of pride for millions of descendants worldwide.\nNotable Descendants\nNotable Descendants\nMany prominent figures in modern Chinese history trace their ancestry to the Yimen Chen division, including:\nChen Duxiu (ÈôàÁã¨ÁßÄ) - Co-founder of the Chinese Communist Party\nChen Yi (ÈôàÊØÖ) - Marshal of the People's Republic of China\nChen Yun (Èôà‰∫ë) - Former Vice Premier of China\nChen Geng (ÈôàËµì) - General in the People's Liberation Army\nCultural Preservation\nCultural Preservation\nEfforts to preserve and celebrate the Yimen Chen heritage include:\nAnnual family reunions bringing together descendants from around the world\nRestoration of ancestral halls in Yimen Village, Jiangxi Province\nPublication of genealogical records documenting the 291 branches and their migrations\nAcademic research into the family's unique governance system and Confucian values\nHistorical Significance The Yimen Chen family stands as a unique chapter in Chinese history. A testament to the power of Confucian values, collective living, and family harmony. Their achievement of maintaining unity across 15 generations and 3,900+ members remains unparalleled in recorded history. Renowned scholars such as Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi all praised the Yimen Chen family in their writings, spreading their fame throughout the nation and establishing them as a model for family governance and moral cultivation.\nHistorical Significance The Yimen Chen family stands as a unique chapter in Chinese history. A testament to the power of Confucian values, collective living, and family harmony. Their achievement of maintaining unity across 15 generations and 3,900+ members remains unparalleled in recorded history. Renowned scholars such as Ouyang Xiu, Su Shi, Huang Tingjian, and Zhu Xi all praised the Yimen Chen family in their writings, spreading their fame throughout the nation and establishing them as a model for family governance and moral cultivation."
  },
  {
    "id": "27640d70fdf580beb336f18fefe82284",
    "title": "News",
    "kind": "page",
    "routePath": "/news",
    "text": "2025/12/18\n2025/12/18\nI will be serving as a reviewer for ICML 2026.\n2025/11/11\n2025/11/11\nI‚Äôm pleased to share that I‚Äôve recently joined Donut Labs (Donut Browser AI), a startup supported by Sequoia Capital and several other leading investors with a total of 22 million US dollars in Pre Seed and Seed funding. I am contributing to the development of its agentic crypto trading browser, a system designed to perceive markets and reason like a quant. My work focuses on applying my research in LLM reasoning and agentic systems to real world trading environments.\n2025/11/07\n2025/11/07\nI‚Äôm pleased to share that our recent work, Unified Minimax Optimization Framework for Propensity Score Estimation in Debiased Recommendation, has been accepted for an oral presentation at AAAI 2026 Main Technical Track.\n2025/09/22\n2025/09/22\nI will be serving as a reviewer for ICLR 2026.\n2025/08/01\n2025/08/01\nI will be serving as a reviewer for AAAI 2026.\n2025/06/20\n2025/06/20\nI will be serving as a reviewer for the XLLM-Reason-Plan Workshop, and Workshop on AI Agents: Capabilities and Safety at COLM 2025.\n2025/05/21\n2025/05/21\nOur paper, \"Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems\", has been accepted for an oral presentation at KDD 2025.\n2024/12/30\n2024/12/30\nAs the new year approaches, I‚Äôm excited to share a personal milestone‚Äîmy research papers have been cited over 100 times üéâ! It‚Äôs a great way to wrap up the year and look ahead to more learning and contributions in 2025.\n2024/12/24\n2024/12/24\nüéâ Excited to share a significant milestone in my journey as an educator! Teaching CSCI3141 (Foundations/Data Science) as my first course as an Instructor at Dalhousie University was an incredibly rewarding experience. The course received a perfect 5/5 student rating on Rate My Professors and achieved a 0% drop rate throughout the term."
  }
]
