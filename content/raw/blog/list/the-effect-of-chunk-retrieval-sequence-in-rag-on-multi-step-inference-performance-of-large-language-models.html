<!DOCTYPE html><html lang="en" dir="ltr" class="theme-light"><head><meta charSet="utf-8"/><link rel="preconnect" href="/" crossorigin=""/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2" as="font" crossorigin="anonymous" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2" as="font" crossorigin="anonymous" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2" as="font" crossorigin="anonymous" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2" as="font" crossorigin="anonymous" type="font/woff2"/><link rel="stylesheet" href="/_next/static/chunks/4057cc9dbcc744c0.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/ef7e1780489ef47b.js"/><script src="/_next/static/chunks/329c0e9c2897197b.js" async=""></script><script src="/_next/static/chunks/48b9427da8073be4.js" async=""></script><script src="/_next/static/chunks/c08b8935044855cc.js" async=""></script><script src="/_next/static/chunks/turbopack-a797a9dcefb3af65.js" async=""></script><script src="/_next/static/chunks/d96012bcfc98706a.js" async=""></script><script src="/_next/static/chunks/963c71eec1d89c3f.js" async=""></script><script src="/_next/static/chunks/f051bbd12aec0cc1.js" async=""></script><script src="/_next/static/chunks/81e796a7b8c3a175.js" async=""></script><script src="/_next/static/chunks/547a8eca1774889f.js" async=""></script><script src="/_next/static/chunks/d0383f817159b1cf.js" async=""></script><script src="/_next/static/chunks/c020afdb26b53a60.js" async=""></script><script src="/_next/static/chunks/7f22801e85c972ca.js" async=""></script><script src="/_next/static/chunks/1b70408e1ee0ede3.js" async=""></script><script src="/_next/static/chunks/ee5c4fc589f91413.js" async=""></script><script src="/_next/static/chunks/8d3945c9ea1274d1.js" async=""></script><script src="/_next/static/chunks/398f78cbac628ac1.js" async=""></script><link rel="preload" href="/styles/static.css" as="style"/><link rel="preload" href="/styles/notion.css" as="style"/><link rel="preload" href="/styles/super.css" as="style"/><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-QJVQ934892" as="script"/><link rel="preload" href="/_next/static/chunks/d2b3d01ef7b174de.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/91790076a86e03fe.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/cf891f9976f43330.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/c4c36ed291fe305e.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/7f8882c47d63ccc5.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/fcfac4eaff1d08f4.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/d3be2928c0beec56.js" as="script" fetchPriority="low"/><link rel="preload" href="/_next/static/chunks/2011050db4c5e47f.js" as="script" fetchPriority="low"/><link rel="icon" href="https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2" crossorigin="anonymous" as="font" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2" crossorigin="anonymous" as="font" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2" crossorigin="anonymous" as="font" type="font/woff2"/><link rel="preload" href="https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2" crossorigin="anonymous" as="font" type="font/woff2"/><meta name="baidu-site-verification" content="codeva-IrK0WEobxJ"/><title>The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models</title><meta name="description" content="Why the order of retrieved information can quietly change how AI reasons step by step"/><meta name="generator" content="Super"/><meta name="robots" content="index, follow"/><meta property="og:title" content="The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models"/><meta property="og:description" content="Why the order of retrieved information can quietly change how AI reasons step by step"/><meta property="og:url" content="https://jinkunchen.com/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models"/><meta property="og:site_name" content="The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models"/><meta property="og:locale" content="en-US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models"/><meta name="twitter:description" content="Why the order of retrieved information can quietly change how AI reasons step by step"/><script>
    const html = document.getElementsByTagName("html")[0];
    try {
      const colorPreference = localStorage
        ? localStorage.getItem("color-preference")
        : null;

      if (true) {
        html.classList.remove("theme-light");
      }
      
      if (false && colorPreference && html) {
        html.classList.add("theme-" + colorPreference);
      } else {
        html.classList.add("theme-light");
      }
    } catch (e) {
      console.log('ERROR themeEffect', e)
      html.classList.add("theme-light");
    }
</script><style>
      @font-face {
        font-family: "Noto Sans";
        font-style: normal;
        font-weight: 400;
        src: url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.eot"); /* IE9 Compat Modes */
        
          src: local(""),
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.eot?#iefix")
            format("embedded-opentype"),
            /* IE6-IE8 */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2")
            format("woff2"),
            /* Super Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff")
            format("woff"),
            /* Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.ttf")
            format("truetype"),
            /* Safari, Android, iOS */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.svg#NotoSans")
            format("svg"); /* Legacy iOS */
        
      }
    
      @font-face {
        font-family: "Noto Sans";
        font-style: normal;
        font-weight: 500;
        src: url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.eot"); /* IE9 Compat Modes */
        
          src: local(""),
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.eot?#iefix")
            format("embedded-opentype"),
            /* IE6-IE8 */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2")
            format("woff2"),
            /* Super Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff")
            format("woff"),
            /* Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.ttf")
            format("truetype"),
            /* Safari, Android, iOS */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.svg#NotoSans")
            format("svg"); /* Legacy iOS */
        
      }
    
      @font-face {
        font-family: "Noto Sans";
        font-style: normal;
        font-weight: 600;
        src: url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.eot"); /* IE9 Compat Modes */
        
          src: local(""),
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.eot?#iefix")
            format("embedded-opentype"),
            /* IE6-IE8 */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2")
            format("woff2"),
            /* Super Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff")
            format("woff"),
            /* Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.ttf")
            format("truetype"),
            /* Safari, Android, iOS */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.svg#NotoSans")
            format("svg"); /* Legacy iOS */
        
      }
    
      @font-face {
        font-family: "Noto Sans";
        font-style: normal;
        font-weight: 700;
        src: url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.eot"); /* IE9 Compat Modes */
        
          src: local(""),
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.eot?#iefix")
            format("embedded-opentype"),
            /* IE6-IE8 */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2")
            format("woff2"),
            /* Super Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff")
            format("woff"),
            /* Modern Browsers */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.ttf")
            format("truetype"),
            /* Safari, Android, iOS */
            url("https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.svg#NotoSans")
            format("svg"); /* Legacy iOS */
        
      }
    </style><link rel="stylesheet" href="/styles/static.css"/><link rel="stylesheet" href="/styles/notion.css"/><link rel="stylesheet" href="/styles/super.css"/><script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "pswoswkqtf");
</script><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><div class="super-root"><nav aria-label="Main" data-orientation="horizontal" dir="ltr" class="super-navbar simple" style="position:relative;box-shadow:var(--navbar-shadow);-webkit-box-shadow:var(--navbar-shadow)"><div class="super-navbar__content"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-navbar__logo"><span class="super-navbar__logo-text" style="font-size:16px">Jinkun Chen.</span></span><!--/$--><div style="position:relative"><ul data-orientation="horizontal" class="super-navbar__item-list" dir="ltr"><li><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-navbar__item">Home</span><!--/$--></li><li><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-navbar__item">News</span><!--/$--></li><li><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-navbar__item">Publications</span><!--/$--></li><li><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-navbar__item">Works</span><!--/$--></li><li><span id="radix-_R_b5j9fivb_-trigger-aac48447-4e23-460e-a73d-2f63407c84a5" data-state="closed" aria-expanded="false" aria-controls="radix-_R_b5j9fivb_-content-aac48447-4e23-460e-a73d-2f63407c84a5" class="super-navbar__list" data-radix-collection-item=""><!--$--><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-down " type="ChevronDown"><path d="m6 9 6 6 6-6"></path></svg><!--/$-->More</span></li></ul></div><div class="super-navbar__actions"><div class="super-navbar__button super-navbar__search"><!--$--><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-search " type="Search"><circle cx="11" cy="11" r="8"></circle><path d="m21 21-4.3-4.3"></path></svg><!--/$--></div><div class="super-navbar__button super-navbar__menu-open"><!--$--><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu " type="Menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg><!--/$--></div></div></div><div class="super-navbar__viewport-wrapper"></div></nav><div class="super-content-wrapper"><main id="page-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" class="super-content page__blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models parent-page__blog-list"><div class="super-navbar__breadcrumbs" style="position:absolute"><div class="notion-breadcrumb"><a id="block-8d6dfeef4c7f4d678b4899d2198877cb" href="/" class="notion-link notion-breadcrumb__item single" data-server-link="true" data-link-uri="/"><div class="notion-navbar__title notion-breadcrumb__title">Hi there!</div></a></div></div><div class="notion-header page"><div class="notion-header__cover no-cover no-icon"></div><div class="notion-header__content max-width no-cover no-icon"><div class="notion-header__title-wrapper"><h1 class="notion-header__title">The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models</h1></div></div></div><article id="block-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" class="notion-root max-width has-footer"><div class="notion-page__properties"><div class="notion-page__property"><div class="notion-page__property-name-wrapper"><div class="notion-page__property-icon-wrapper"><svg viewBox="0 0 16 16" style="width:16px;height:16px"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></div><div class="notion-page__property-name"><span>Date</span></div></div><div class="notion-property notion-property__date property-5d71516e notion-semantic-string"><span class="date">June 19, 2025</span></div></div><div class="notion-page__property"><div class="notion-page__property-name-wrapper"><div class="notion-page__property-icon-wrapper"><svg viewBox="0 0 16 16" style="width:16px;height:16px"><path d="M10.9536 7.90088C12.217 7.90088 13.2559 6.79468 13.2559 5.38525C13.2559 4.01514 12.2114 2.92017 10.9536 2.92017C9.70142 2.92017 8.65137 4.02637 8.65698 5.39087C8.6626 6.79468 9.69019 7.90088 10.9536 7.90088ZM4.4231 8.03003C5.52368 8.03003 6.42212 7.05859 6.42212 5.83447C6.42212 4.63843 5.51245 3.68945 4.4231 3.68945C3.33374 3.68945 2.41846 4.64966 2.41846 5.84009C2.42407 7.05859 3.32251 8.03003 4.4231 8.03003ZM1.37964 13.168H5.49561C4.87231 12.292 5.43384 10.6074 6.78711 9.51807C6.18628 9.14746 5.37769 8.87231 4.4231 8.87231C1.95239 8.87231 0.262207 10.6917 0.262207 12.1628C0.262207 12.7974 0.548584 13.168 1.37964 13.168ZM7.50024 13.168H14.407C15.4009 13.168 15.7322 12.8423 15.7322 12.2864C15.7322 10.8489 13.8679 8.88354 10.9536 8.88354C8.04492 8.88354 6.17505 10.8489 6.17505 12.2864C6.17505 12.8423 6.50635 13.168 7.50024 13.168Z"></path></svg></div><div class="notion-page__property-name"><span>Author</span></div></div><div class="notion-property notion-property__person property-5d51766e notion-semantic-string no-wrap"><span class="individual-with-image"><div class="individual-letter-avatar">J</div><span>Jinkun Chen</span></span></div></div><div id="block-root-divider" class="notion-divider"></div></div><p id="block-37f7210c96014e2a9f56aecbc694e2c5" class="notion-text notion-text__content notion-semantic-string"><em>Why the order of retrieved information can quietly change how AI reasons step by step</em></p><p id="block-6c8403d936fb490e8e24f1bf3563e2eb" class="notion-text notion-text__content notion-semantic-string">You give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.</p><p id="block-2cbe5c0189b147baaaf2eebd568f3e11" class="notion-text notion-text__content notion-semantic-string">This behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages (&quot;chunks&quot;) shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, &quot;multi-step inference&quot; refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.</p><p id="block-4a8efe9c8c844c6099a647fae633e65a" class="notion-text notion-text__content notion-semantic-string">A key finding is that preserving the original document structure, as seen in Document&#x27;s Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM&#x27;s sequential processing.</p><p id="block-d95145e3a15c47179bb2c31dae95d2ef" class="notion-text notion-text__content notion-semantic-string">For users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.</p><p id="block-975a6c020a53497d81afe6ed91c2ac38" class="notion-text notion-text__content notion-semantic-string">The analysis further reveals that LLMs exhibit a &quot;cognitive linearity&quot;, performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.</p><p id="block-15da42c0920c4fe5a384039880acbf25" class="notion-text notion-text__content notion-semantic-string">Beyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.</p><div id="block-21740d70fdf5809e9b08e50fd38ec2c9" class="notion-text"></div><div id="block-21740d70fdf5807cb076cbe47bde3bc5" class="notion-divider"></div><ul id="block-21740d70fdf5803ba492f8f7a0f1124b" class="notion-table-of-contents color-gray"><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">1.2 Understanding Retrieval-Augmented Generation (RAG)</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">1.3 The Essence of Multi-Step Inference in LLMs</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">1.4 Purpose and Scope of the Report</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">2. Chunking and Retrieval in RAG Architectures</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">2.1 Principles of Document Chunking for RAG</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">2.2 Overview of Retrieval Mechanisms in RAG</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:48px" class="notion-semantic-string">3. The Critical Role of Chunk Retrieval Sequence</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">3.1 Impact of Context Order on LLM Performance</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">3.2 Document&#x27;s Original Structure (DOS RAG) and its Benefits</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">3.3 Reranking Strategies and Context Reordering</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">4. Factors Influencing Multi-Step Inference Performance in RAG</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">4.1 Positional Bias and the &quot;Lost-in-the-Middle&quot; Effect</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">4.2 The Detrimental Impact of Irrelevant and Distracting Information</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">4.3 Cognitive Load and Context Window Management</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">5. Empirical Evidence and Performance Analysis</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">5.1 Case Studies on Chunk Order and Multi-Step Question Answering</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">5.2 Evaluation Benchmarks and Metrics</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">6.1 Best Practices for Chunking and Reordering</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">6.2 Strategies for Mitigating Positional Bias and Distraction</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:24px" class="notion-semantic-string">6.3 Advanced Techniques for Multi-Hop Reasoning</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">7. Conclusion and Future Directions</div></span><!--/$--></li><li class="notion-table-of-contents__item"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><div style="margin-inline-start:0px" class="notion-semantic-string">References</div></span><!--/$--></li></ul><div id="block-21740d70fdf5804496dffff09e93c171" class="notion-divider"></div><div id="block-21740d70fdf58019b4e5f82038300e4e" class="notion-text"></div><span class="notion-heading__anchor" id="21740d70fdf580789318c4b68c9dc180"></span><h1 id="block-21740d70fdf580789318c4b68c9dc180" class="notion-heading notion-semantic-string"><strong>1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning</strong></h1><p id="block-21740d70fdf580b59114ebb95e594cb4" class="notion-text notion-text__content notion-semantic-string">The landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.</p><span class="notion-heading__anchor" id="21740d70fdf580998476c769158dc800"></span><h2 id="block-21740d70fdf580998476c769158dc800" class="notion-heading notion-semantic-string"><strong>1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities</strong></h2><p id="block-21740d70fdf580ae8f73f007b0a8ec92" class="notion-text notion-text__content notion-semantic-string">Large Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5807ba813e037c8c3158e" class="notion-text notion-text__content notion-semantic-string">Despite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as &quot;hallucinations&quot;, where the model generates factually incorrect or nonsensical information not present in its training data.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.</p><div id="block-21740d70fdf580e8a51ad82068f715f8" class="notion-text"></div><span class="notion-heading__anchor" id="21740d70fdf5804ea1b3c814f4776242"></span><h2 id="block-21740d70fdf5804ea1b3c814f4776242" class="notion-heading notion-semantic-string"><strong>1.2 Understanding Retrieval-Augmented Generation (RAG)</strong></h2><p id="block-21740d70fdf5804ea49fd96ef424e931" class="notion-text notion-text__content notion-semantic-string">
Retrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58009ba9df8dfcba9a012" class="notion-text notion-text__content notion-semantic-string">The core process of RAG typically involves two main stages. First, <strong>Retrieval and Pre-processing</strong> occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The second stage is <strong>Grounded Generation</strong>, where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM&#x27;s context. This integration significantly enhances the LLM&#x27;s understanding of the topic, allowing it to produce more precise, informative, and engaging responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580849758fee1679a61da" class="notion-text notion-text__content notion-semantic-string">RAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This factual grounding is crucial for mitigating &quot;gen AI hallucinations&quot; by supplying verifiable facts as part of the input prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM&#x27;s outputs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.</p><span class="notion-heading__anchor" id="21740d70fdf580b78f06c6a97022bc5c"></span><h2 id="block-21740d70fdf580b78f06c6a97022bc5c" class="notion-heading notion-semantic-string"><strong>1.3 The Essence of Multi-Step Inference in LLMs</strong></h2><p id="block-21740d70fdf580e887d9cfc1af9f673c" class="notion-text notion-text__content notion-semantic-string">Multi-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM&#x27;s capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> This capability extends beyond merely following a single instruction or performing a singular task.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580b3a0d7c0300ffd118a" class="notion-text notion-text__content notion-semantic-string">The ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> It is widely recognized as a key indicator of advanced intelligence in AI systems.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d1b729df508408cc3d" class="notion-text notion-text__content notion-semantic-string">To address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. <!-- -->This shift underscores that future advancements in LLMs and RAG systems must prioritize not just <em><strong><u>what</u></strong></em> information is retrieved, but <em><strong><u>how</u></strong></em> that information facilitates a structured<!-- -->, step-by-step problem-solving process.<!-- --> This elevates the importance of context organization and coherence within the input.</p><span class="notion-heading__anchor" id="21740d70fdf58034b829fad21730876f"></span><h2 id="block-21740d70fdf58034b829fad21730876f" class="notion-heading notion-semantic-string"><strong>1.4 Purpose and Scope of the Report</strong></h2><p id="block-21740d70fdf580a7a5e1c5589a27f4b4" class="notion-text notion-text__content notion-semantic-string">The primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM&#x27;s capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.</p><span class="notion-heading__anchor" id="21740d70fdf5803d9807e94a97e877c9"></span><h1 id="block-21740d70fdf5803d9807e94a97e877c9" class="notion-heading notion-semantic-string"><strong>2. Chunking and Retrieval in RAG Architectures</strong></h1><p id="block-21740d70fdf580428548dda1c2b7ff6a" class="notion-text notion-text__content notion-semantic-string">The efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.</p><span class="notion-heading__anchor" id="21740d70fdf580e68ebfe97e77da4cf0"></span><h2 id="block-21740d70fdf580e68ebfe97e77da4cf0" class="notion-heading notion-semantic-string"><strong>2.1 Principles of Document Chunking for RAG</strong></h2><p id="block-21740d70fdf580ceb7d1e138749fece9" class="notion-text notion-text__content notion-semantic-string">Chunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>12</mn></msup></mrow><annotation encoding="application/x-tex">^{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span></span></span></span></span> These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.</p><p id="block-21740d70fdf5802594fac1a34fc2b5be" class="notion-text notion-text__content notion-semantic-string">The necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580aca4b9f403a1abb511" class="notion-text notion-text__content notion-semantic-string">Several chunking strategies are employed, each with distinct advantages and use cases:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf5807685ece0dd6484ea2e" class="notion-list-item notion-semantic-string"><strong>Fixed Size Chunking:</strong> This straightforward approach divides text into uniform chunks based on a predefined character or token count.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58026839cfc6fc508c42b" class="notion-list-item notion-semantic-string"><strong>Recursive-Based Chunking:</strong> A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580efb9a6db405332f049" class="notion-list-item notion-semantic-string"><strong>Sentence-based Chunking:</strong> This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5800fae7ed100c5ecd009" class="notion-list-item notion-semantic-string"><strong>Document Structure-based Chunking:</strong> This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580959badf2d68cc1f896" class="notion-list-item notion-semantic-string"><strong>Semantic Chunking:</strong> This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf580c0b810f7633ce50e32" class="notion-text notion-text__content notion-semantic-string">The choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM&#x27;s ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.</p><span class="notion-heading__anchor" id="21740d70fdf5807b8ea1e3f0ef8cce21"></span><h2 id="block-21740d70fdf5807b8ea1e3f0ef8cce21" class="notion-heading notion-semantic-string"><strong>2.2 Overview of Retrieval Mechanisms in RAG</strong></h2><p id="block-21740d70fdf580408d4fc76f4eef75dc" class="notion-text notion-text__content notion-semantic-string">A RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a <strong>Query Encoder</strong> transforms the user&#x27;s input query into a representation suitable for searching the knowledge base.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> Second, a <strong>Retriever</strong> takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>Finally, a <strong>Generator</strong>, typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>
</p><p id="block-21740d70fdf580d78798c7861511ed2e" class="notion-text notion-text__content notion-semantic-string">Retrievers can be broadly categorized based on their underlying mechanisms:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf5805299ddcb51cebbc031" class="notion-list-item notion-semantic-string"><strong>Sparse Retrievers:</strong> These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580f880d2ce188348e4e8" class="notion-list-item notion-semantic-string"><strong>Dense Retrievers:</strong> Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580778da5dc116592082c" class="notion-list-item notion-semantic-string"><strong>Hybrid Search:</strong> Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>
The retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> After the initial retrieval, an essential step often involves </li><li id="block-21740d70fdf580f39177ff83be84b5c3" class="notion-list-item notion-semantic-string"><strong>re-rankers</strong>. These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The effectiveness of RAG is therefore highly dependent on the retriever&#x27;s ability to provide relevant information and the re-ranker&#x27;s capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM&#x27;s performance can degrade, leading to responses that, while &quot;grounded&quot; in the provided context, might be off-topic or factually incorrect.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the  <!-- --><em>most relevant</em> and <!-- --><em>least distracting</em> content, a factor that profoundly influences the subsequent chunk ordering.<!-- --></li></ul><span class="notion-heading__anchor" id="21740d70fdf58015b472e155c66d727f"></span><h3 id="block-21740d70fdf58015b472e155c66d727f" class="notion-heading notion-semantic-string"><strong>3. The Critical Role of Chunk Retrieval Sequence</strong></h3><p id="block-21740d70fdf580a68709d4be6ca361e1" class="notion-text notion-text__content notion-semantic-string">The order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM&#x27;s ability to reason effectively.</p><span class="notion-heading__anchor" id="21740d70fdf580c4a559c3eff45d8257"></span><h2 id="block-21740d70fdf580c4a559c3eff45d8257" class="notion-heading notion-semantic-string"><strong>3.1 Impact of Context Order on LLM Performance</strong></h2><p id="block-21740d70fdf5800585f8e60d24de38aa" class="notion-text notion-text__content notion-semantic-string">Observations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM&#x27;s internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model&#x27;s accuracy.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580aebbeaf862f0f1bb84" class="notion-text notion-text__content notion-semantic-string">Conversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> This &quot;ordering effect&quot; is further exacerbated when irrelevant premises are introduced into the prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model&#x27;s ability to synthesize information and produce coherent responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This indicates that LLMs, despite their advanced capabilities, exhibit a form of &quot;cognitive linearity&quot; in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a &quot;ground truth proof&quot; <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> suggests that the LLM&#x27;s internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.</p><span class="notion-heading__anchor" id="21740d70fdf580d8a90af2ad47b882d5"></span><h2 id="block-21740d70fdf580d8a90af2ad47b882d5" class="notion-heading notion-semantic-string"><strong>3.2 Document&#x27;s Original Structure (DOS RAG) and its Benefits</strong></h2><p id="block-21740d70fdf580ba874bff70dae33e6c" class="notion-text notion-text__content notion-semantic-string">Document&#x27;s Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580a281e2d404fdd2bcc7" class="notion-text notion-text__content notion-semantic-string">The benefits of DOS RAG are significant and empirically validated. It primarily <strong>preserves passage continuity</strong>, maintaining the document&#x27;s structural integrity and narrative flow.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves 
<strong>improved accuracy</strong>, outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including Bench, QuALITY, and NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> For instance, on the Bench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG&#x27;s 87.8%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, DOS RAG demonstrates notable 
<strong>efficiency</strong>, often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580848881d08f51d59cdf" class="notion-text notion-text__content notion-semantic-string">The consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, <em>contextual coherence</em> and <em>narrative flow</em>, as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> However, DOS RAG&#x27;s consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the <em>relationship</em> between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM&#x27;s processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of &quot;relevance&quot; for multi-step tasks should be broadened to include &quot;contextual relevance&quot; or &quot;narrative relevance&quot; in addition to traditional semantic similarity.</p><span class="notion-heading__anchor" id="21740d70fdf5808f9238d67cfb9d9d82"></span><h2 id="block-21740d70fdf5808f9238d67cfb9d9d82" class="notion-heading notion-semantic-string"><strong>3.3 Reranking Strategies and Context Reordering</strong></h2><p id="block-21740d70fdf58081a3ecd927972a059f" class="notion-text notion-text__content notion-semantic-string">Reranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span>
Various types of rerankers are employed, each with distinct characteristics:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf58058a46cfc4b81aaecc4" class="notion-list-item notion-semantic-string"><strong>Cross-Encoders:</strong> These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include Sentence Transformers, Flashrank, and BGE-M3.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5807490e3ef6ba727c883" class="notion-list-item notion-semantic-string"><strong>Multi-Vector Rerankers:</strong> Models like ColBERT use a &quot;late interaction&quot; approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58003b9c1f7a840f17ca1" class="notion-list-item notion-semantic-string"><strong>Fine-tuned LLM Rerankers:</strong> Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5807382e1cfca2e9abdb2" class="notion-list-item notion-semantic-string"><strong>LLM as a Judge:</strong> This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include GPT, Claude, and Gemini.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5802cb8e3e90f285484c4" class="notion-list-item notion-semantic-string"><strong>Reranking APIs:</strong> Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include Cohere, Jina, and Mixedbread.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf5808da1edd71553e8d49c" class="notion-text notion-text__content notion-semantic-string">Beyond simple relevance scoring, context reordering within the reranking process also plays a role. <strong>Inverted Context Ordering</strong> is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> This method has demonstrated a performance increase in correctness for multi-hop QA tasks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> Other advanced approaches include <strong>Fusion-based Reranking</strong>, which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>, and <strong>Adaptive Reranking</strong>, which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5807d866be836369721de" class="notion-text notion-text__content notion-semantic-string">While reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where &quot;better&quot; relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the <em>most</em> relevant context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span> This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system&#x27;s usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This implies that the &quot;best&quot; reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM.
<strong>
Table 1: Comparison of Key Chunking and Reordering Strategies in RAG</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Strategy</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Description</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Primary Goal</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Impact on Multi-Step Inference</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Advantages</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Disadvantages</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Fixed Size Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides text into uniform segments (e.g., 500 tokens), often with overlap.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Efficiency, Simplicity</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can fragment context, hindering logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Easy to implement, fast, consistent.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Context fragmentation, information loss, inflexible.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Recursive-Based Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Uses multiple separators (paragraphs, sentences) to find meaningful boundaries.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Context Preservation</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Better at maintaining logical units for sequential understanding.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Adaptive, preserves logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">More complex to implement.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Sentence-based Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides text into complete sentences.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserve Complete Thoughts</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Supports logical flow, good for connecting ideas.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Ensures complete thoughts, natural boundaries.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">May create very small chunks, less efficient for long documents.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Document&#x27;s Original Structure (DOS RAG)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Retrieves chunks and reorders them to match their original document sequence.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Narrative Continuity, Contextual Coherence</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Significantly improves performance by maintaining logical progression; crucial for multi-hop QA.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserves narrative, robust QA, often outperforms relevance-based sorting.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires tracking chunk positions; may include less relevant chunks if not filtered.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Inverted Context Ordering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Arranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Prioritize Most Relevant</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can improve correctness; focuses LLM on key information.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Directs LLM to top relevant info immediately.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Still relies on relevance score, may disrupt original narrative flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">16</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Semantic Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides documents into semantically coherent and non-overlapping chunks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduce Irrelevance, Improve Accuracy</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Enhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduces hallucinations, improves factual accuracy, aligned with query needs.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires sophisticated LLM-based relevance scoring.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf5807c929cdc484058c538"></span><h1 id="block-21740d70fdf5807c929cdc484058c538" class="notion-heading notion-semantic-string"><strong>4. Factors Influencing Multi-Step Inference Performance in RAG</strong></h1><p id="block-21740d70fdf580f9821be4ec0e5fc9d4" class="notion-text notion-text__content notion-semantic-string">Beyond the direct ordering of retrieved chunks, several other factors interact with the LLM&#x27;s context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.</p><span class="notion-heading__anchor" id="21740d70fdf5805f9dc2d7b972b2041d"></span><h2 id="block-21740d70fdf5805f9dc2d7b972b2041d" class="notion-heading notion-semantic-string"><strong>4.1 Positional Bias and the &quot;Lost-in-the-Middle&quot; Effect</strong></h2><p id="block-21740d70fdf5806b9a45ea2b164dfdd2" class="notion-text notion-text__content notion-semantic-string">Positional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> A specific manifestation of this is the &quot;lost-in-the-middle&quot; effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This bias can affect both the LLM&#x27;s capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d4ba66c5c4765273a0" class="notion-text notion-text__content notion-semantic-string">While earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM&#x27;s presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>26</mn></msup></mrow><annotation encoding="application/x-tex">^{26}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">26</span></span></span></span></span></span></span></span></span></span></span></span></span> This is attributed to a &quot;contrastive effect&quot;, where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a &quot;parametric bias&quot;, relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580459646fcd5a2a23040" class="notion-text notion-text__content notion-semantic-string">The &quot;lost-in-the-middle&quot; effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to &quot;trick&quot; the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the <em>quality</em> of retrieved content and enhancing the LLM&#x27;s inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> However, in practical RAG systems, where retrievers often fetch <em>both</em> relevant and highly distracting passages <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span>, the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the &quot;best&quot; chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span> This shifts the focus from <em>where</em> to place chunks to <em>what</em> chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.</p><span class="notion-heading__anchor" id="21740d70fdf5804cb009d3d8e8e002a5"></span><h2 id="block-21740d70fdf5804cb009d3d8e8e002a5" class="notion-heading notion-semantic-string"><strong>4.2 The Detrimental Impact of Irrelevant and Distracting Information</strong></h2><p id="block-21740d70fdf58004ad5fd2eeaf65e692" class="notion-text notion-text__content notion-semantic-string">A well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, &quot;distracting passages&quot;, contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580ba9bdfd3bff3ab2e3e" class="notion-text notion-text__content notion-semantic-string">The presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> Studies have shown that &quot;hard distracting passages&quot;, those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to &quot;weak&quot; ones, and this detrimental effect persists even in larger LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span>
Paradoxically, &quot;stronger&quot; retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver <em>more harmful distractors</em>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580f7923dca2a57ce5607" class="notion-text notion-text__content notion-semantic-string">The very act of retrieval, especially with &quot;stronger&quot; retrievers, presents a &quot;double-edged sword.&quot; While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG&#x27;s core purpose is to provide relevant external knowledge.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> However, no retriever is perfect, and they often return irrelevant or &quot;distracting&quot; passages.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> The critical observation here is that <em>stronger</em> retrievers, which are designed to find more relevant information, also tend to retrieve <em>more harmful</em> distracting passages.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This creates a paradox: improving the retriever&#x27;s primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for &quot;relevance&quot; (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the <em>distracting effect</em> to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.</p><span class="notion-heading__anchor" id="21740d70fdf580e2a4f9c97d9434d4cb"></span><h2 id="block-21740d70fdf580e2a4f9c97d9434d4cb" class="notion-heading notion-semantic-string"><strong>4.3 Cognitive Load and Context Window Management</strong></h2><p id="block-21740d70fdf580d28441d48edb42e729" class="notion-text notion-text__content notion-semantic-string">Large Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> The concept of &quot;cognitive load&quot;, analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>31</mn></msup></mrow><annotation encoding="application/x-tex">^{31}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">31</span></span></span></span></span></span></span></span></span></span></span></span></span> For LLMs, the inherent &quot;content complexity&quot; of the input is a dominant factor influencing their processing efficiency.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>31</mn></msup></mrow><annotation encoding="application/x-tex">^{31}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">31</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580139869c3006b44178b" class="notion-text notion-text__content notion-semantic-string">Presenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its &quot;cognitive load&quot;.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> This overload can lead to reduced selection accuracy and an increase in hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> Conversely, supplying <em>only</em> the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of &quot;prompt bloat&quot; directly lowers the LLM&#x27;s cognitive load.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM&#x27;s decision-making capabilities are markedly improved.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58011839fd05b5333a270" class="notion-text notion-text__content notion-semantic-string">While long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> The key lies in identifying the &quot;sweet spot&quot; for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5809dbe10fa8f1c49222f" class="notion-text notion-text__content notion-semantic-string">The concept of &quot;cognitive load&quot; in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the <em>quality and conciseness</em> of the retrieved context directly impact the LLM&#x27;s processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span>, the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This is framed in terms of &quot;cognitive load&quot;.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> If the LLM is forced to &quot;sift through hundreds of distractors&quot; <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span>, it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about <em>what</em> to retrieve, but <em><strong>how much</strong></em> and <em><strong>how clean</strong></em> that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance.
<strong>
Table 2: Factors Affecting LLM Performance in Long Contexts</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Factor</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Description</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Impact on Multi-Step Inference</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Interaction with Chunk Order</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Mitigation Strategies</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Positional Bias</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLMs weigh information differently based on its position (e.g., &quot;lost-in-the-middle&quot; effect).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can cause LLMs to ignore relevant info or be misled by distractors in middle positions.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Improve retrieval quality, LLM robustness to distraction, avoid simple rearrangement.<!-- --></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">25</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Irrelevant/Distracting Information</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Passages that are semantically similar but do not contain the answer or mislead the LLM.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Significantly degrades accuracy, even when relevant info is present; can derail reasoning chain.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Strong retrievers can inadvertently bring more harmful distractors to top ranks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Robust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Cognitive Load/Context Window Overload</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLM struggles to process excessive or noisy information within its limited context.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduces selection accuracy, increases hallucinations, hinders efficient reasoning.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Too many chunks (even if somewhat relevant) can overwhelm the model.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Supplying only relevant context, precise chunking, adaptive streaming, efficient filtering.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Lack of Narrative Continuity</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Disjointed or shuffled presentation of information.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Impairs sequential reasoning, makes it harder for LLM to build a coherent understanding.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Direct result of relevance-only sorting; addressed by DOS RAG.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserving original document structure (DOS RAG) or logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf580e4a606e6f4424b31e5"></span><h1 id="block-21740d70fdf580e4a606e6f4424b31e5" class="notion-heading notion-semantic-string"><strong>5. Empirical Evidence and Performance Analysis</strong></h1><p id="block-21740d70fdf58093a210cb9d25926543" class="notion-text notion-text__content notion-semantic-string">Empirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.</p><span class="notion-heading__anchor" id="21740d70fdf580ad845cdef42ba8a4e7"></span><h2 id="block-21740d70fdf580ad845cdef42ba8a4e7" class="notion-heading notion-semantic-string"><strong>5.1 Case Studies on Chunk Order and Multi-Step Question Answering</strong></h2><p id="block-21740d70fdf580948a38d453a10fe70d" class="notion-text notion-text__content notion-semantic-string">Comparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including Bench, QuALITY, and NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> For example, on the Bench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs&#x27; multi-step reasoning capabilities are profoundly tied to the 
<em>narrative and structural coherence</em> of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.</p><p id="block-21740d70fdf58044bc06dbd3566e29b3" class="notion-text notion-text__content notion-semantic-string">Furthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>20</mn></msup></mrow><annotation encoding="application/x-tex">^{20}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">20</span></span></span></span></span></span></span></span></span></span></span></span></span> However, there is a &quot;sweet spot&quot; for context length: DOS RAG&#x27;s performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.</p><p id="block-21740d70fdf580c58320e68a6859b88f" class="notion-text notion-text__content notion-semantic-string">Research on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in <code class="code">correctness</code>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> When rerankers like BGE-M3 are combined with higher <code class="code">retrieval@k</code> values, more &quot;gold documents&quot; (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> However, increasing <code class="code">rerank@k</code> with a fixed <code class="code">retrieval@k</code> can introduce higher variation in <code class="code">correctness</code> scores, ranging from 1% to 25%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span></p><span class="notion-heading__anchor" id="21740d70fdf580889f7bc0535e95d69a"></span><h2 id="block-21740d70fdf580889f7bc0535e95d69a" class="notion-heading notion-semantic-string"><strong>5.2 Evaluation Benchmarks and Metrics</strong></h2><p id="block-21740d70fdf5806baa0cd10113df6c6c" class="notion-text notion-text__content notion-semantic-string">The field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580cf802debf13bd62d5a" class="notion-text notion-text__content notion-semantic-string">Another important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span> For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval <code class="code">accuracy</code>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>35</mn></msup></mrow><annotation encoding="application/x-tex">^{35}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">35</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>36</mn></msup></mrow><annotation encoding="application/x-tex">^{36}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">36</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d5978af3c7e30f6305" class="notion-text notion-text__content notion-semantic-string">These specialized benchmarks employ a variety of evaluation metrics. <code class="code">Accuracy</code> is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> For more nuanced assessments, metrics like <code class="code">F1</code>, <code class="code">BLEU-1</code>, <code class="code">BLEU-4</code>, <code class="code">ROUGE-L</code>, and <code class="code">METEOR</code> are employed, particularly for tasks like NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span>, step-by-step procedure following <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span>, and multi-hop questions <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span>, demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like &quot;<code class="code">correctness</code>&quot; and &quot;<!-- --><code class="code">faithfulness</code>&quot; further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.
<!-- --></p><p id="block-21740d70fdf580c2b190c8654f824624" class="notion-text notion-text__content notion-semantic-string"><strong>
Table 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Benchmark</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Primary Focus</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Key Features Relevant to Chunk Order/Multi-Step QA</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Key Findings Related to Chunk Order</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>MTI Bench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-Task Inference (sequential &amp; non-sequential sub-tasks)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Evaluates LLMs&#x27; ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Stronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">10</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>ProcBench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-Step Reasoning &amp; Procedure Following</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Dataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Highlights critical gap in current assessments focusing exclusively on multi-step inference.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">11</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Bench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Evaluates performance under varying retrieval token budgets (1.5K to 40K tokens).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">DOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>QuALITY</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering (narrative understanding)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires understanding underlying narrative rather than shallow pattern matching.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Full-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>NarrativeQA</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering (narrative understanding)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Questions require understanding the underlying narrative.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">DOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>DataMorgana</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">QA-pair Generation (single-hop &amp; multi-hop)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Creates highly customizable synthetic benchmarks; used in LiveRAG Challenge.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Used to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">15</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>mmRAG</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-modal RAG Evaluation</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Modular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Provides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">35</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>ChunkRAG</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLM-driven Chunk Filtering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Enhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Outperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf580e0a67ddf16d23ac95b"></span><h1 id="block-21740d70fdf580e0a67ddf16d23ac95b" class="notion-heading notion-semantic-string"><strong>6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning</strong></h1><p id="block-21740d70fdf580bdab4cf02a41144b18" class="notion-text notion-text__content notion-semantic-string">Translating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.</p><span class="notion-heading__anchor" id="21740d70fdf58085858fd4791e4f101b"></span><h2 id="block-21740d70fdf58085858fd4791e4f101b" class="notion-heading notion-semantic-string"><strong>6.1 Best Practices for Chunking and Reordering</strong></h2><p id="block-21740d70fdf58011a43df811886b1920" class="notion-text notion-text__content notion-semantic-string">To optimize chunk retrieval sequence, a primary focus must be placed on <strong>prioritizing contextual coherence</strong>. For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> Given its consistent outperformance across various benchmarks, <strong>adopting DOS RAG as a baseline</strong> is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5800bb60debab93527a57" class="notion-text notion-text__content notion-semantic-string">While initial retrieval provides a set of relevant chunks, a <strong>strategic reranking</strong> step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, implementing <strong>inverted context ordering</strong>, where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58049bce1f1a47e2d8051" class="notion-text notion-text__content notion-semantic-string">Optimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>20</mn></msup></mrow><annotation encoding="application/x-tex">^{20}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">20</span></span></span></span></span></span></span></span></span></span></span></span></span>, effective reranking can still improve the 
<em>selection</em> of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.</p><span class="notion-heading__anchor" id="21740d70fdf580e49c36f4912bd6ac93"></span><h2 id="block-21740d70fdf580e49c36f4912bd6ac93" class="notion-heading notion-semantic-string"><strong>6.2 Strategies for Mitigating Positional Bias and Distraction</strong></h2><p id="block-21740d70fdf580e895f8f04cd3bdbc49" class="notion-text notion-text__content notion-semantic-string">To effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing <strong>retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages</strong>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, <strong>robust LLM fine-tuning</strong> with carefully selected &quot;hard distracting passages&quot; can significantly increase the LLM&#x27;s accuracy and resilience against noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span><strong> </strong>Third, implementing <strong>LLM-driven chunk filtering</strong> (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Fourth, for complex multi-step queries, <strong>query rewriting or decomposition</strong> into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Finally, active <strong>context window management</strong> is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> Identifying the &quot;sweet spot&quot; for context length, where recall is maximized without introducing excessive noise, is also paramount.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58025a2fec1c13e30b145" class="notion-text notion-text__content notion-semantic-string">Mitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the <em>quality and focus</em> of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Simply reordering <em>after</em> retrieval is often insufficient to address these issues.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>26</mn></msup></mrow><annotation encoding="application/x-tex">^{26}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">26</span></span></span></span></span></span></span></span></span></span></span></span></span> Therefore, the solution must involve proactive measures. This includes improving the <em>initial retrieval</em> to be less prone to fetching distractors, and then employing <em>strong filtering</em> (like ChunkRAG) to eliminate noise before it ever reaches the LLM&#x27;s context window.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, making the LLM itself more robust through fine-tuning with challenging examples <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.</p><span class="notion-heading__anchor" id="21740d70fdf580b3bb55f18aef19ec66"></span><h2 id="block-21740d70fdf580b3bb55f18aef19ec66" class="notion-heading notion-semantic-string"><strong>6.3 Advanced Techniques for Multi-Hop Reasoning</strong></h2><p id="block-21740d70fdf580f4b7a0d4c4313da49c" class="notion-text notion-text__content notion-semantic-string">Addressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, <strong>iterative retrieval</strong> becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span><strong> Adaptive retrieval</strong> mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>38</mn></msup></mrow><annotation encoding="application/x-tex">^{38}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">38</span></span></span></span></span></span></span></span></span></span></span></span></span> The integration of structured knowledge, such as <strong>graph-based RAG</strong> (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>35</mn><mo separator="true">,</mo><mn>39</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{35,39}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">35</span><span class="mpunct mtight">,</span><span class="mord mtight">39</span></span></span></span></span></span></span></span></span></span></span></span></span> This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of <strong>prompt-based reasoning chains</strong> like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM&#x27;s reasoning process step-by-step, enhancing its ability to perform complex deductions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">^6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span></span> These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.<!-- --></p><span class="notion-heading__anchor" id="21740d70fdf58057bc2bc6c92d18067c"></span><h1 id="block-21740d70fdf58057bc2bc6c92d18067c" class="notion-heading notion-semantic-string"><strong>7. Conclusion and Future Directions</strong></h1><p id="block-21740d70fdf580c9a5c9cbbe9a3723c1" class="notion-text notion-text__content notion-semantic-string">The analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model&#x27;s multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.</p><p id="block-21740d70fdf580059d5be7ecaceed316" class="notion-text notion-text__content notion-semantic-string">The implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM&#x27;s processing efficiency and reasoning accuracy.</p><p id="block-21740d70fdf580ca8329d34ae124a4ac" class="notion-text notion-text__content notion-semantic-string">Future research and development should focus on several promising directions:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf580bb9f60cd579f054e8f" class="notion-list-item notion-semantic-string"><strong>Adaptive Retrieval Architectures:</strong> Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58072ab0fc215757eb48b" class="notion-list-item notion-semantic-string"><strong>Real-time Retrieval Integration:</strong> Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">^9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5802cb5a5e4c8136d32b4" class="notion-list-item notion-semantic-string"><strong>Structured Reasoning over Multi-Hop Evidence:</strong> Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>6</mn><mo separator="true">,</mo><mn>39</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{6,39}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span><span class="mpunct mtight">,</span><span class="mord mtight">39</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580438856f1a03608dfd0" class="notion-list-item notion-semantic-string"><strong>Robustness to Adversarial Inputs:</strong> Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>8</mn><mo separator="true">,</mo><mn>28</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{8,28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span><span class="mpunct mtight">,</span><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58012ab78f973a8a870ee" class="notion-list-item notion-semantic-string"><strong>Cross-Modal and Multi-Lingual RAG:</strong> Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>4</mn><mo separator="true">,</mo><mn>35</mn><mo separator="true">,</mo><mn>40</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{4,35,40}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mpunct mtight">,</span><span class="mord mtight">35</span><span class="mpunct mtight">,</span><span class="mord mtight">40</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580ba9d25c88f6cfa1e49" class="notion-list-item notion-semantic-string"><strong>Evaluation Methodologies:</strong> Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">^9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf580aa9d37d06095234106" class="notion-text notion-text__content notion-semantic-string">These future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.</p><div id="block-21740d70fdf580388756d009905ff072" class="notion-toggle closed notion-toggle-heading-1"><div class="notion-toggle__summary"><div class="notion-toggle__trigger"><div class="notion-toggle__trigger_icon"><span></span></div></div><span class="notion-heading__anchor" id="21740d70fdf580388756d009905ff072"></span><h1 id="block-21740d70fdf580388756d009905ff072" class="notion-heading toggle notion-semantic-string"><strong>References</strong></h1></div></div><div id="block-21740d70fdf580fe8ae4f3d4e38c4ec2" class="notion-text"></div></article></main><style>
    html {
      font-size: 16px;
    }

    
  </style><!--$--><!--/$--></div><footer class="super-footer stack no-links no-footnote"><div class="super-footer__content"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link super-footer__logo"><div class="super-footer__logo-image"><span data-full-size="https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/logo/712f74e3-00ca-453b-9511-39896485699f.png" data-lightbox-src="https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/logo/712f74e3-00ca-453b-9511-39896485699f.png" style="display:contents"><img alt="Logo" loading="lazy" width="180" height="48" decoding="async" data-nimg="1" style="color:transparent;object-fit:contain;object-position:left" srcSet="/_next/image?url=https%3A%2F%2Fassets.super.so%2Fe331c927-5859-4092-b1ca-16eddc17b1bb%2Fuploads%2Flogo%2F712f74e3-00ca-453b-9511-39896485699f.png&amp;w=256&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets.super.so%2Fe331c927-5859-4092-b1ca-16eddc17b1bb%2Fuploads%2Flogo%2F712f74e3-00ca-453b-9511-39896485699f.png&amp;w=384&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fassets.super.so%2Fe331c927-5859-4092-b1ca-16eddc17b1bb%2Fuploads%2Flogo%2F712f74e3-00ca-453b-9511-39896485699f.png&amp;w=384&amp;q=75"/></span></div></span><!--/$--><div class="super-footer__icons"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></span><!--/$--><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></span><!--/$--><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 16 16" width="24" height="24"><title>X</title><path fill-rule="evenodd" clip-rule="evenodd" d="M0.5 0.5H5.75L9.48421 5.71053L14 0.5H16L10.3895 6.97368L16.5 15.5H11.25L7.51579 10.2895L3 15.5H1L6.61053 9.02632L0.5 0.5ZM12.0204 14L3.42043 2H4.97957L13.5796 14H12.0204Z"></path></svg></span><!--/$--><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><span class="notion-link"><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><title>Discord</title><path d="M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z"></path></svg></span><!--/$--></div></div></footer></div><style>
  
    html.theme-light {
      /* Gray */
      --gray-h: 36;
      --gray-s: 2%;
      --gray-l: 46%;
      --color-gray: hsl(var(--gray-h), var(--gray-s), var(--gray-l));
      --color-text-gray: var(--color-gray);
      --color-bg-gray: hsl(
        var(--gray-h),
        var(--gray-s),
        90%
      );
      --color-bg-gray-light: var(--color-bg-gray);
      --color-pill-gray: hsl(
        calc(var(--gray-h) + 14),
        var(--gray-s),
        88%
      );
      --color-pill-text-gray: hsl(
        var(--gray-h),
        var(--gray-s),
        calc(var(--gray-l) - 28%)
      );
      --color-bg-form-btn-gray: #A7A39A;

      /* Brown */
      --brown-h: 19;
      --brown-s: 31%;
      --brown-l: 47%;
      --color-brown: hsl(var(--brown-h), var(--brown-s), var(--brown-l));
      --color-text-brown: var(--color-brown);
      --color-bg-brown: hsl(
        var(--brown-h),
        var(--brown-s),
        90%
      );
      --color-bg-brown-light: var(--color-bg-brown);
      --color-pill-brown: hsl(
        var(--brown-h),
        calc(var(--brown-s) + 10%),
        89%
      );
      --color-pill-text-brown: hsl(
        var(--brown-h),
        calc(var(--brown-s) + 10%),
        calc(var(--brown-l) - 28%)
      );
      --color-bg-form-btn-brown: #9A6851;

      /* Orange */
      --orange-h: 30;
      --orange-s: 87%;
      --orange-l: 45%;
      --color-orange: hsl(
        var(--orange-h),
        var(--orange-s),
        var(--orange-l)
      );
      --color-text-orange: var(--color-orange);
      --color-bg-orange: hsl(
        var(--orange-h),
        var(--orange-s),
        90%
      );
      --color-bg-orange-light: var(--color-bg-orange);
      --color-pill-orange: hsl(
        var(--orange-h),
        calc(var(--orange-s) - 6%),
        87%
      );
      --color-pill-text-orange: hsl(
        calc(var(--orange-h) - 5),
        calc(var(--orange-s) - 18%),
        calc(var(--orange-l) - 28%)
      );
      --color-bg-form-btn-orange: #D9730D;

      /* Yellow */
      --yellow-h: 38;
      --yellow-s: 62%;
      --yellow-l: 49%;
      --color-yellow: hsl(
        var(--yellow-h),
        var(--yellow-s),
        var(--yellow-l)
      );
      --color-text-yellow: var(--color-yellow);
      --color-bg-yellow: hsl(
        var(--yellow-h),
        calc(var(--yellow-s) + 90%),
        90%
      );
      --color-bg-yellow-light: var(--color-bg-yellow);
      --color-pill-yellow: hsl(
        calc(var(--yellow-h) + 3),
        calc(var(--yellow-s) + 24%),
        90%
      );
      --color-pill-text-yellow: hsl(
        calc(var(--yellow-h) - 11),
        calc(var(--yellow-s) - 22%),
        calc(var(--yellow-l) - 28%)
      );
      --color-bg-form-btn-yellow: #CA922F;

      /* Green */
      --green-h: 149;
      --green-s: 31%;
      --green-l: 39%;
      --color-green: hsl(var(--green-h), var(--green-s), var(--green-l));
      --color-text-green: var(--color-green);
      --color-bg-green: hsl(
        var(--green-h),
        var(--green-s),
        90%
      );
      --color-bg-green-light: var(--color-bg-green);
      --color-pill-green: hsl(
        calc(var(--green-h) - 28),
        calc(var(--green-s) + 3%),
        89%
      );
      --color-pill-text-green: hsl(
        calc(var(--green-h) - 2),
        var(--green-s),
        calc(var(--green-l) - 22%)
      );
      --color-bg-form-btn-green: #448361;

      /* Blue */
      --blue-h: 202;
      --blue-s: 53%;
      --blue-l: 43%;
      --color-blue: hsl(var(--blue-h), var(--blue-s), var(--blue-l));
      --color-text-blue: var(--color-blue);
      --color-bg-blue: hsl(
        var(--blue-h),
        var(--blue-s),
        90%
      );
      --color-bg-blue-light: var(--color-bg-blue);
      --color-pill-blue: hsl(
        var(--blue-h),
        calc(var(--blue-s) - 5%),
        85%
      );
      --color-pill-text-blue: hsl(
        calc(var(--blue-h) + 7),
        calc(var(--blue-s) - 6%),
        calc(var(--blue-l) - 22%)
      );
      --color-bg-form-btn-blue: #327DA9;

      /* Purple */
      --purple-h: 274;
      --purple-s: 32%;
      --purple-l: 54%;
      --color-purple: hsl(
        var(--purple-h),
        var(--purple-s),
        var(--purple-l)
      );
      --color-text-purple: var(--color-purple);
      --color-bg-purple: hsl(
        var(--purple-h),
        var(--purple-s),
        90%
      );
      --color-bg-purple-light: var(--color-bg-purple);
      --color-pill-purple: hsl(
        var(--purple-h),
        calc(var(--purple-s) + 5%),
        90%
      );
      --color-pill-text-purple: hsl(
        calc(var(--purple-h) + 1),
        calc(var(--purple-s) + 10%),
        calc(var(--purple-l) - 31%)
      );
      --color-bg-form-btn-purple: #8F64AF;

      /* Pink */
      --pink-h: 328;
      --pink-s: 48%;
      --pink-l: 53%;
      --color-pink: hsl(var(--pink-h), var(--pink-s), var(--pink-l));
      --color-text-pink: var(--color-pink);
      --color-bg-pink: hsl(
        var(--pink-h),
        var(--pink-s),
        90%
      );
      --color-bg-pink-light: var(--color-bg-pink);
      --color-pill-pink: hsl(
        var(--pink-h),
        var(--pink-s),
        90%
      );
      --color-pill-text-pink: hsl(
        calc(var(--pink-h) + 3),
        calc(var(--pink-s) - 14%),
        calc(var(--pink-l) - 31%)
      );
      --color-bg-form-btn-pink: #C24C8B;

      /* Red */
      --red-h: 2;
      --red-s: 62%;
      --red-l: 55%;
      --color-red: hsl(var(--red-h), var(--red-s), var(--red-l));
      --color-text-red: var(--color-red);
      --color-bg-red: hsl(
        var(--red-h),
        var(--red-s),
        90%
      );
      --color-bg-red-light: var(--color-bg-red);
      --color-pill-red: hsl(
        calc(var(--red-h) + 6),
        calc(var(--red-s) + 42%),
        90%
      );
      --color-pill-text-red: hsl(
        calc(var(--red-h) + 0),
        var(--red-s),
        calc(var(--red-l) - 32%)
      );
      --color-bg-form-btn-red: #D44E49;

      /*Default*/
      --default-h: 45;
      --default-s: 8%;
      --default-l: 20%;
      --color-default: hsl(
        var(--default-h),
        var(--default-s),
        var(--default-l)
      );
      --color-pill-default: hsl(
        var(--gray-h),
        var(--gray-s),
        90%
      );
      --color-pill-text-default: hsl(
        var(--gray-h),
        var(--gray-s),
        calc(var(--gray-l) - 28%)
      );
      --color-bg-form-btn-default: #55534E;

      /*Other*/
      --color-text-default: #37352F;
      --color-text-default-light: #7d7c78;
      --color-bg-default: #ffffff;
      --color-border-default: #E9E9E7;
      --color-border-dark: var(--color-border-default);
      --color-ui-hover-bg: #efefef;
      --color-card-bg: #ffffff;
      --color-card-bg-hover: #f9f9f8;
      --color-calendar-weekend-bg: #f7f6f3;
      --color-checkbox-bg: #2EAADC;
      --color-code-bg: rgba(135,131,120,.15);

      /*Scrollbar*/
      --scrollbar-background-color: #FAFAFA;
      --scrollbar-thumb-color: #C1C1C1;
      --scrollbar-border-color: #E8E8E8;

      /*Navbar*/
      --navbar-text-color: #37352F;
      --navbar-list-item-hover: rgba(130, 130, 130, 0.09);
      --navbar-background-color-hover: #f0f0f0;
      --navbar-background-color: #ffffff;
      --navbar-button-text-color: #ffffff;
      --navbar-button-background-color: #37352F;
      --navbar-menu-background-color: var(--navbar-background-color);
      --navbar-text-color-dark: #282620;

      /*Footer*/
      --footer-text-color: #37352F;
      --footer-background-color: #ffffff;

      /*Sidebar*/
      --sidebar-text-color: #37352F;
      --sidebar-cta-text-color: #37352F;
      --sidebar-background-color: #ffffff;
      --sidebar-cta-background-color: #ffffff;
      --sidebar-border-color: #E9E9E7;
      --sidebar-background-color-hover: #efefef;
      --sidebar-text-color-dark: #282620;
      --sidebar-cta-background-color-hover: #efefef;
    }

    
  /**
   * One Light theme for prism.js
   * Based on Atom's One Light theme: https://github.com/atom/atom/tree/master/packages/one-light-syntax
   */
  
  /**
   * One Light colours (accurate as of commit eb064bf on 19 Feb 2021)
   * From colors.less
   * --mono-1: hsl(230, 8%, 24%);
   * --mono-2: hsl(230, 6%, 44%);
   * --mono-3: hsl(230, 4%, 64%)
   * --hue-1: hsl(198, 99%, 37%);
   * --hue-2: hsl(221, 87%, 60%);
   * --hue-3: hsl(301, 63%, 40%);
   * --hue-4: hsl(119, 34%, 47%);
   * --hue-5: hsl(5, 74%, 59%);
   * --hue-5-2: hsl(344, 84%, 43%);
   * --hue-6: hsl(35, 99%, 36%);
   * --hue-6-2: hsl(35, 99%, 40%);
   * --syntax-fg: hsl(230, 8%, 24%);
   * --syntax-bg: hsl(230, 1%, 98%);
   * --syntax-gutter: hsl(230, 1%, 62%);
   * --syntax-guide: hsla(230, 8%, 24%, 0.2);
   * --syntax-accent: hsl(230, 100%, 66%);
   * From syntax-variables.less
   * --syntax-selection-color: hsl(230, 1%, 90%);
   * --syntax-gutter-background-color-selected: hsl(230, 1%, 90%);
   * --syntax-cursor-line: hsla(230, 8%, 24%, 0.05);
   */
  
  html.theme-light code[class*="language-"],
  html.theme-light pre[class*="language-"] {
    color: hsl(230, 8%, 24%);
    font-family: "Fira Code", "Fira Mono", Menlo, Consolas, "DejaVu Sans Mono", monospace;
    direction: ltr;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    line-height: 1.5;
    -moz-tab-size: 2;
    -o-tab-size: 2;
    tab-size: 2;
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  /* Selection */
  html.theme-light code[class*="language-"]::-moz-selection,
  html.theme-light code[class*="language-"] *::-moz-selection,
  html.theme-light pre[class*="language-"] *::-moz-selection {
    background: hsl(230, 1%, 90%);
    color: inherit;
  }
  
  html.theme-light code[class*="language-"]::selection,
  html.theme-light code[class*="language-"] *::selection,
  html.theme-light pre[class*="language-"] *::selection {
    background: hsl(230, 1%, 90%);
    color: inherit;
  }
  
  /* Code blocks */
  html.theme-light pre[class*="language-"] {
    padding: 1em;
    margin: 0.5em 0;
    overflow: auto;
    border-radius: 0.3em;
  }
  
  /* Inline code */
  html.theme-light :not(pre) > code[class*="language-"] {
    padding: 0.2em 0.3em;
    border-radius: 0.3em;
    white-space: normal;
  }
  
  html.theme-light .token.comment,
  html.theme-light .token.prolog,
  html.theme-light .token.cdata {
    color: hsl(230, 4%, 64%);
  }
  
  html.theme-light .token.doctype,
  html.theme-light .token.punctuation,
  html.theme-light .token.entity {
    color: hsl(230, 8%, 24%);
  }
  
  html.theme-light .token.attr-name,
  html.theme-light .token.class-name,
  html.theme-light .token.boolean,
  html.theme-light .token.constant,
  html.theme-light .token.number,
  html.theme-light .token.atrule {
    color: hsl(35, 99%, 36%);
  }
  
  html.theme-light .token.keyword {
    color: hsl(301, 63%, 40%);
  }
  
  html.theme-light .token.property,
  html.theme-light .token.tag,
  html.theme-light .token.symbol,
  html.theme-light .token.deleted,
  html.theme-light .token.important {
    color: hsl(5, 74%, 59%);
  }
  
  html.theme-light .token.selector,
  html.theme-light .token.string,
  html.theme-light .token.char,
  html.theme-light .token.builtin,
  html.theme-light .token.inserted,
  html.theme-light .token.regex,
  html.theme-light .token.attr-value,
  html.theme-light .token.attr-value > .token.punctuation {
    color: hsl(119, 34%, 47%);
  }
  
  html.theme-light .token.variable,
  html.theme-light .token.operator,
  html.theme-light .token.function {
    color: hsl(221, 87%, 60%);
  }
  
  html.theme-light .token.url {
    color: hsl(198, 99%, 37%);
  }
  
  /* HTML overrides */
  html.theme-light .token.attr-value > .token.punctuation.attr-equals,
  html.theme-light .token.special-attr > .token.attr-value > .token.value.css {
    color: hsl(230, 8%, 24%);
  }
  
  /* CSS overrides */
  html.theme-light .language-css .token.selector {
    color: hsl(5, 74%, 59%);
  }
  
  html.theme-light .language-css .token.property {
    color: hsl(230, 8%, 24%);
  }
  
  html.theme-light .language-css .token.function,
  html.theme-light .language-css .token.url > .token.function {
    color: hsl(198, 99%, 37%);
  }
  
  html.theme-light .language-css .token.url > .token.string.url {
    color: hsl(119, 34%, 47%);
  }
  
  html.theme-light .language-css .token.important,
  html.theme-light .language-css .token.atrule .token.rule {
    color: hsl(301, 63%, 40%);
  }
  
  /* JS overrides */
  html.theme-light .language-javascript .token.operator {
    color: hsl(301, 63%, 40%);
  }
  
  html.theme-light .language-javascript .token.template-string > .token.interpolation > .token.interpolation-punctuation.punctuation {
    color: hsl(344, 84%, 43%);
  }
  
  /* JSON overrides */
  html.theme-light .language-json .token.operator {
    color: hsl(230, 8%, 24%);
  }
  
  html.theme-light .language-json .token.null.keyword {
    color: hsl(35, 99%, 36%);
  }
  
  /* MD overrides */
  html.theme-light .language-markdown .token.url,
  html.theme-light .language-markdown .token.url > .token.operator,
  html.theme-light .language-markdown .token.url-reference.url > .token.string {
    color: hsl(230, 8%, 24%);
  }
  
  html.theme-light .language-markdown .token.url > .token.content {
    color: hsl(221, 87%, 60%);
  }
  
  html.theme-light .language-markdown .token.url > .token.url,
  html.theme-light .language-markdown .token.url-reference.url {
    color: hsl(198, 99%, 37%);
  }
  
  html.theme-light .language-markdown .token.blockquote.punctuation,
  html.theme-light .language-markdown .token.hr.punctuation {
    color: hsl(230, 4%, 64%);
    font-style: italic;
  }
  
  html.theme-light .language-markdown .token.code-snippet {
    color: hsl(119, 34%, 47%);
  }
  
  html.theme-light .language-markdown .token.bold .token.content {
    color: hsl(35, 99%, 36%);
  }
  
  html.theme-light .language-markdown .token.italic .token.content {
    color: hsl(301, 63%, 40%);
  }
  
  html.theme-light .language-markdown .token.strike .token.content,
  html.theme-light .language-markdown .token.strike .token.punctuation,
  html.theme-light .language-markdown .token.list.punctuation,
  html.theme-light .language-markdown .token.title.important > .token.punctuation {
    color: hsl(5, 74%, 59%);
  }
  
  /* General */
  html.theme-light .token.bold {
    font-weight: bold;
  }
  
  html.theme-light .token.comment,
  html.theme-light .token.italic {
    font-style: italic;
  }
  
  html.theme-light .token.entity {
    cursor: help;
  }
  
  html.theme-light .token.namespace {
    opacity: 0.8;
  }
  
  /* Plugin overrides */
  /* Selectors should have higher specificity than those in the plugins' default stylesheets */
  
  /* Show Invisibles plugin overrides */
  html.theme-light .token.token.tab:not(:empty):before,
  html.theme-light .token.token.cr:before,
  html.theme-light .token.token.lf:before,
  html.theme-light .token.token.space:before {
    color: hsla(230, 8%, 24%, 0.2);
  }
  
  /* Toolbar plugin overrides */
  /* Space out all buttons and move them away from the right edge of the code block */
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item {
    margin-right: 0.4em;
  }
  
  /* Styling the buttons */
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > button,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > a,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > span {
    background: hsl(230, 1%, 90%);
    color: hsl(230, 6%, 44%);
    padding: 0.1em 0.4em;
    border-radius: 0.3em;
  }
  
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > button:hover,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > button:focus,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > a:hover,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > a:focus,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > span:hover,
  html.theme-light div.code-toolbar > .toolbar.toolbar > .toolbar-item > span:focus {
    background: hsl(230, 1%, 78%); /* custom: darken(--syntax-bg, 20%) */
    color: hsl(230, 8%, 24%);
  }
  
  /* Line Highlight plugin overrides */
  /* The highlighted line itself */
  html.theme-light .line-highlight.line-highlight {
    background: hsla(230, 8%, 24%, 0.05);
  }
  
  /* Default line numbers in Line Highlight plugin */
  html.theme-light .line-highlight.line-highlight:before,
  html.theme-light .line-highlight.line-highlight[data-end]:after {
    background: hsl(230, 1%, 90%);
    color: hsl(230, 8%, 24%);
    padding: 0.1em 0.6em;
    border-radius: 0.3em;
    box-shadow: 0 2px 0 0 rgba(0, 0, 0, 0.2); /* same as Toolbar plugin default */
  }
  
  /* Hovering over a linkable line number (in the gutter area) */
  /* Requires Line Numbers plugin as well */
  html.theme-light pre[id].linkable-line-numbers.linkable-line-numbers span.line-numbers-rows > span:hover:before {
    background-color: hsla(230, 8%, 24%, 0.05);
  }
  
  /* Line Numbers and Command Line plugins overrides */
  /* Line separating gutter from coding area */
  html.theme-light .line-numbers.line-numbers .line-numbers-rows,
  html.theme-light .command-line .command-line-prompt {
    border-right-color: hsla(230, 8%, 24%, 0.2);
  }
  
  /* Stuff in the gutter */
  html.theme-light .line-numbers .line-numbers-rows > span:before,
  html.theme-light .command-line .command-line-prompt > span:before {
    color: hsl(230, 1%, 62%);
  }
  
  /* Match Braces plugin overrides */
  /* Note: Outline colour is inherited from the braces */
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-1,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-5,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-9 {
    color: hsl(5, 74%, 59%);
  }
  
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-2,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-6,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-10 {
    color: hsl(119, 34%, 47%);
  }
  
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-3,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-7,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-11 {
    color: hsl(221, 87%, 60%);
  }
  
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-4,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-8,
  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-12 {
    color: hsl(301, 63%, 40%);
  }
  
  /* Diff Highlight plugin overrides */
  /* Taken from https://github.com/atom/github/blob/master/styles/variables.less */
  html.theme-light pre.diff-highlight > code .token.token.deleted:not(.prefix),
  html.theme-light pre > code.diff-highlight .token.token.deleted:not(.prefix) {
    background-color: hsla(353, 100%, 66%, 0.15);
  }
  
  html.theme-light pre.diff-highlight > code .token.token.deleted:not(.prefix)::-moz-selection,
  html.theme-light pre.diff-highlight > code .token.token.deleted:not(.prefix) *::-moz-selection,
  html.theme-light pre > code.diff-highlight .token.token.deleted:not(.prefix)::-moz-selection,
  html.theme-light pre > code.diff-highlight .token.token.deleted:not(.prefix) *::-moz-selection {
    background-color: hsla(353, 95%, 66%, 0.25);
  }
  
  html.theme-light pre.diff-highlight > code .token.token.deleted:not(.prefix)::selection,
  html.theme-light pre.diff-highlight > code .token.token.deleted:not(.prefix) *::selection,
  html.theme-light pre > code.diff-highlight .token.token.deleted:not(.prefix)::selection,
  html.theme-light pre > code.diff-highlight .token.token.deleted:not(.prefix) *::selection {
    background-color: hsla(353, 95%, 66%, 0.25);
  }
  
  html.theme-light pre.diff-highlight > code .token.token.inserted:not(.prefix),
  html.theme-light pre > code.diff-highlight .token.token.inserted:not(.prefix) {
    background-color: hsla(137, 100%, 55%, 0.15);
  }
  
  html.theme-light pre.diff-highlight > code .token.token.inserted:not(.prefix)::-moz-selection,
  html.theme-light pre.diff-highlight > code .token.token.inserted:not(.prefix) *::-moz-selection,
  html.theme-light pre > code.diff-highlight .token.token.inserted:not(.prefix)::-moz-selection,
  html.theme-light pre > code.diff-highlight .token.token.inserted:not(.prefix) *::-moz-selection {
    background-color: hsla(135, 73%, 55%, 0.25);
  }
  
  html.theme-light pre.diff-highlight > code .token.token.inserted:not(.prefix)::selection,
  html.theme-light pre.diff-highlight > code .token.token.inserted:not(.prefix) *::selection,
  html.theme-light pre > code.diff-highlight .token.token.inserted:not(.prefix)::selection,
  html.theme-light pre > code.diff-highlight .token.token.inserted:not(.prefix) *::selection {
    background-color: hsla(135, 73%, 55%, 0.25);
  }
  
  /* Previewers plugin overrides */
  /* Based on https://github.com/atom-community/atom-ide-datatip/blob/master/styles/atom-ide-datatips.less and https://github.com/atom/atom/blob/master/packages/one-light-ui */
  /* Border around popup */
  html.theme-light .prism-previewer.prism-previewer:before,
  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div {
    border-color: hsl(0, 0, 95%);
  }
  
  /* Angle and time should remain as circles and are hence not included */
  html.theme-light .prism-previewer-color.prism-previewer-color:before,
  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div,
  html.theme-light .prism-previewer-easing.prism-previewer-easing:before {
    border-radius: 0.3em;
  }
  
  /* Triangles pointing to the code */
  html.theme-light .prism-previewer.prism-previewer:after {
    border-top-color: hsl(0, 0, 95%);
  }
  
  html.theme-light .prism-previewer-flipped.prism-previewer-flipped.after {
    border-bottom-color: hsl(0, 0, 95%);
  }
  
  /* Background colour within the popup */
  html.theme-light .prism-previewer-angle.prism-previewer-angle:before,
  html.theme-light .prism-previewer-time.prism-previewer-time:before,
  html.theme-light .prism-previewer-easing.prism-previewer-easing {
    background: hsl(0, 0%, 100%);
  }
  
  /* For angle, this is the positive area (eg. 90deg will display one quadrant in this colour) */
  /* For time, this is the alternate colour */
  html.theme-light .prism-previewer-angle.prism-previewer-angle circle,
  html.theme-light .prism-previewer-time.prism-previewer-time circle {
    stroke: hsl(230, 8%, 24%);
    stroke-opacity: 1;
  }
  
  /* Stroke colours of the handle, direction point, and vector itself */
  html.theme-light .prism-previewer-easing.prism-previewer-easing circle,
  html.theme-light .prism-previewer-easing.prism-previewer-easing path,
  html.theme-light .prism-previewer-easing.prism-previewer-easing line {
    stroke: hsl(230, 8%, 24%);
  }
  
  /* Fill colour of the handle */
  html.theme-light .prism-previewer-easing.prism-previewer-easing circle {
    fill: transparent;
  }
  
  
  
    :root {
      /* Layout */
      --padding-layout: 0.6rem;
      --border-radii-layout: 5px;
      --border-thickness-layout: 1px;
      --border-type-layout: solid;
      --border-layout: var(--border-thickness-layout)
        var(--border-type-layout) var(--color-border-default);
      --layout-max-width: 900px;
      --column-spacing: 46px;
      --page-display: none;
      --padding-right: calc(env(safe-area-inset-right) + 96px);
      --padding-left: calc(env(safe-area-inset-left) + 96px);
      --padding-right-mobile: calc(env(safe-area-inset-right) + 24px);
      --padding-left-mobile: calc(env(safe-area-inset-left) + 24px);
      /* Header */
      --header-cover-height: 30vh;
      --header-title-align: start;
      --header-icon-align: -112px auto auto auto;
      --header-display: block;
      /* Collection header */
      --collection-header-border: var(--border-layout);
      /* Collection table */
      --collection-table-cell-padding: calc(var(--padding-layout) - 0.3rem)
        calc(var(--padding-layout) - 0.1rem);
      /* Collection list */
      --collection-list-item-padding: calc(var(--padding-layout) - 0.5rem);
      --collection-list-item-border-radii: calc(
        var(--border-radii-layout) - 1px
      );
      /* Collection card */
      --collection-card-padding: 0px;
      --collection-card-title-padding: 0px;
      --collection-card-content-padding: var(--padding-layout);
      --collection-card-border-radii: var(--border-radii-layout);
      --collection-card-gap: 10px;
      --collection-card-shadow: rgba(15, 15, 15, 0.1) 0px 0px 0px 1px, rgba(15, 15, 15, 0.1) 0px 2px 4px;
      --collection-card-title-size: 0.875rem;
      --collection-card-cover-height-small: 128px;
      --collection-card-cover-size-small: 172px;
      --collection-card-cover-height-medium: 200px;
      --collection-card-cover-size-medium: 260px;
      --collection-card-cover-height-large: 200px;
      --collection-card-cover-size-large: 320px;
      --collection-card-icon-display: inline-flex;
      /* Callout */
      --callout-padding: calc(var(--padding-layout) + 0.4rem)
        calc(var(--padding-layout) + 0.4rem)
        calc(var(--padding-layout) + 0.4rem)
        calc(var(--padding-layout) + 0.1em);
      --callout-border-radii: calc(var(--border-radii-layout) - 2px);
      --callout-border: var(--border-layout);
      --callout-icon-display: block;
      --callout-shadow: none;
      /* File */
      --file-border-radii: calc(var(--border-radii-layout) - 2px);
      /* Equation */
      --equation-border-radii: calc(var(--border-radii-layout) - 2px);
      /* Divider */
      --divider-border: var(--border-layout);
      /* Quote */
      --quote-border: calc(var(--border-thickness-layout) + 2px) solid
        currentcolor;
      /* Code */
      --code-padding: calc(var(--padding-layout) + 1.4rem);
      --code-border-radii: var(--border-radii-layout);
      /* Tweet */
      --tweet-padding: calc(var(--padding-layout) + 0.65rem)
        calc(var(--padding-layout) + 0.65rem)
        calc(var(--padding-layout) + 0.05rem)
        calc(var(--padding-layout) + 0.65rem);
      --tweet-border-radii: var(--border-radii-layout);
      --tweet-border: var(--border-layout);
      /* Bookmark */
      --bookmark-padding: calc(var(--padding-layout) + 0.15rem) 0px
        calc(var(--padding-layout) + 0.025rem)
        calc(var(--padding-layout) + 0.275rem);
      --bookmark-border-radii: var(--border-radii-layout);
      --bookmark-border: var(--border-layout);
      --bookmark-image-border-radii: 0px
        calc(var(--border-radii-layout) - 1px)
        calc(var(--border-radii-layout) - 1px) 0px;
      /* Embed */
      --embed-border-radii: calc(var(--border-radii-layout) - 5px);
      /* Image */
      --image-border-radii: calc(var(--border-radii-layout) - 5px);
      /* Typography */
      --title-size: 2.5rem;
      --quote-size: 1.2rem;
      --quote-size-large: 1.4rem;
      --heading-size: 1rem;
      --primary-font: Noto Sans, Noto Sans-fallback, Helvetica, Apple Color Emoji,
        Segoe UI Emoji, NotoColorEmoji, Noto Color Emoji,
        Segoe UI Symbol, Android Emoji, EmojiSymbols, -apple-system,
        BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Noto Sans,
        sans-serif;
        --secondary-font: Noto Sans, Noto Sans-fallback, Helvetica, Apple Color Emoji,
        Segoe UI Emoji, NotoColorEmoji, Noto Color Emoji,
        Segoe UI Symbol, Android Emoji, EmojiSymbols, -apple-system,
        BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Noto Sans,
        sans-serif;
      --text-weight: 400;
      --heading-weight: 600;
      --heading1-size: calc(var(--heading-size) * 1.875);
      --heading2-size: calc(var(--heading-size) * 1.5);
      --heading3-size: calc(var(--heading-size) * 1.25);
      --heading4-size: calc(var(--heading-size) * 1);
      --heading5-size: calc(var(--heading-size) * 0.8125);
      /* Scrollbars */
      --scrollbar-width: 15px;
      /* Navbar */
      --navbar-height: 56px;
      --navbar-shadow: none;
      --navbar-button-border-radii: 50px;
      --navbar-list-width-single-column: 320px;
      --navbar-list-width: 620px;
      /*Sidebar*/
      --sidebar-width: 280px;
      --sidebar-shadow: none;
    }
    html {
      scroll-padding-top: 62px;
    }
    body {
      font-family: var(--secondary-font);
    }

    span[class="highlighted-background bg-yellow"] {
  /*display: inline-block;*/
  background-color: transparent;
}

a[class="notion-link link"] {
  display: inline-block;
  position: relative;
}

a[class="notion-link link"]:after {
  position: absolute;
  content: " ";
  top: 66%;
  bottom: 0;
  left: 0em;
  right: -0.1em;
  transition: top 800ms cubic-bezier(0, 0.8, 0.13, 1);
  background-color: hsl(38, 152%, 80%);
  z-index: -1;
}

a[class="notion-link link"]:hover:after {
  top: 2%;
}
  
</style><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/_next/static/chunks/ef7e1780489ef47b.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[339756,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"default\"]\n3:I[837457,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"default\"]\n7:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"ViewportBoundary\"]\nc:I[897367,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"MetadataBoundary\"]\ne:I[168027,[\"/_next/static/chunks/d96012bcfc98706a.js\",\"/_next/static/chunks/963c71eec1d89c3f.js\"],\"default\"]\n:HC[\"/\",\"\"]\n:HL[\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"iHQB6_iZU3_2m4AOPndlH\",\"c\":[\"\",\"main\",\"jinkunchen.com\",\"blog\",\"list\",\"the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"main\",{\"children\":[[\"site\",\"jinkunchen.com\",\"d\"],{\"children\":[[\"page\",\"blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true]}]}],[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/81e796a7b8c3a175.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/547a8eca1774889f.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/d0383f817159b1cf.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-4\",{\"src\":\"/_next/static/chunks/c020afdb26b53a60.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-5\",{\"src\":\"/_next/static/chunks/7f22801e85c972ca.js\",\"async\":true,\"nonce\":\"$undefined\"}]],\"$L4\"]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L5\",[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/4057cc9dbcc744c0.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/ee5c4fc589f91413.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/8d3945c9ea1274d1.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[208077,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"ErrorPage\"]\n5:[\"$\",\"$Lf\",null,{\"type\":\"notfound\"}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"10:T1454,"])</script><script>self.__next_f.push([1,"\n      @font-face {\n        font-family: \"Noto Sans\";\n        font-style: normal;\n        font-weight: 400;\n        src: url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.eot\"); /* IE9 Compat Modes */\n        \n          src: local(\"\"),\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.eot?#iefix\")\n            format(\"embedded-opentype\"),\n            /* IE6-IE8 */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2\")\n            format(\"woff2\"),\n            /* Super Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff\")\n            format(\"woff\"),\n            /* Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.ttf\")\n            format(\"truetype\"),\n            /* Safari, Android, iOS */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.svg#NotoSans\")\n            format(\"svg\"); /* Legacy iOS */\n        \n      }\n    \n      @font-face {\n        font-family: \"Noto Sans\";\n        font-style: normal;\n        font-weight: 500;\n        src: url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.eot\"); /* IE9 Compat Modes */\n        \n          src: local(\"\"),\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.eot?#iefix\")\n            format(\"embedded-opentype\"),\n            /* IE6-IE8 */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2\")\n            format(\"woff2\"),\n            /* Super Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff\")\n            format(\"woff\"),\n            /* Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.ttf\")\n            format(\"truetype\"),\n            /* Safari, Android, iOS */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.svg#NotoSans\")\n            format(\"svg\"); /* Legacy iOS */\n        \n      }\n    \n      @font-face {\n        font-family: \"Noto Sans\";\n        font-style: normal;\n        font-weight: 600;\n        src: url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.eot\"); /* IE9 Compat Modes */\n        \n          src: local(\"\"),\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.eot?#iefix\")\n            format(\"embedded-opentype\"),\n            /* IE6-IE8 */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2\")\n            format(\"woff2\"),\n            /* Super Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff\")\n            format(\"woff\"),\n            /* Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.ttf\")\n            format(\"truetype\"),\n            /* Safari, Android, iOS */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.svg#NotoSans\")\n            format(\"svg\"); /* Legacy iOS */\n        \n      }\n    \n      @font-face {\n        font-family: \"Noto Sans\";\n        font-style: normal;\n        font-weight: 700;\n        src: url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.eot\"); /* IE9 Compat Modes */\n        \n          src: local(\"\"),\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.eot?#iefix\")\n            format(\"embedded-opentype\"),\n            /* IE6-IE8 */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2\")\n            format(\"woff2\"),\n            /* Super Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff\")\n            format(\"woff\"),\n            /* Modern Browsers */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.ttf\")\n            format(\"truetype\"),\n            /* Safari, Android, iOS */\n            url(\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.svg#NotoSans\")\n            format(\"svg\"); /* Legacy iOS */\n        \n      }\n    "])</script><script>self.__next_f.push([1,"4:[\"$\",\"html\",null,{\"lang\":\"en\",\"dir\":\"ltr\",\"className\":\"theme-light\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n    const html = document.getElementsByTagName(\\\"html\\\")[0];\\n    try {\\n      const colorPreference = localStorage\\n        ? localStorage.getItem(\\\"color-preference\\\")\\n        : null;\\n\\n      if (true) {\\n        html.classList.remove(\\\"theme-light\\\");\\n      }\\n      \\n      if (false \u0026\u0026 colorPreference \u0026\u0026 html) {\\n        html.classList.add(\\\"theme-\\\" + colorPreference);\\n      } else {\\n        html.classList.add(\\\"theme-light\\\");\\n      }\\n    } catch (e) {\\n      console.log('ERROR themeEffect', e)\\n      html.classList.add(\\\"theme-light\\\");\\n    }\\n\"}}],[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}],\"$L11\",\"$L12\",\"$L13\",[\"$L14\",\"$L15\",\"$L16\",\"$L17\"],[\"$L18\",\"$L19\"]]}],\"$L1a\"]}]\n"])</script><script>self.__next_f.push([1,"1b:I[868422,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"SiteProvider\"]\n20:I[23131,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"PageProvider\"]\n:HL[\"/styles/static.css\",\"style\"]\n:HL[\"/styles/notion.css\",\"style\"]\n:HL[\"/styles/super.css\",\"style\"]\n:HL[\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2\",\"font\",{\"crossOrigin\":\"anonymous\",\"type\":\"font/woff2\"}]\n:HL[\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2\",\"font\",{\"crossOrigin\":\"anonymous\",\"type\":\"font/woff2\"}]\n:HL[\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2\",\"font\",{\"crossOrigin\":\"anonymous\",\"type\":\"font/woff2\"}]\n:HL[\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2\",\"font\",{\"crossOrigin\":\"anonymous\",\"type\":\"font/woff2\"}]\n11:[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"/styles/static.css\"}]\n12:[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"/styles/notion.css\"}]\n13:[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"/styles/super.css\"}]\n14:[\"$\",\"link\",\"font-preload-Noto Sans-regular\",{\"rel\":\"preload\",\"href\":\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-regular.woff2\",\"crossOrigin\":\"anonymous\",\"as\":\"font\",\"type\":\"font/woff2\"}]\n15:[\"$\",\"link\",\"font-preload-Noto Sans-500\",{\"rel\":\"preload\",\"href\":\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-500.woff2\",\"crossOrigin\":\"anonymous\",\"as\":\"font\",\"type\":\"font/woff2\"}]\n16:[\"$\",\"link\",\"font-preload-Noto Sans-600\",{\"rel\":\"preload\",\"href\":\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-600.woff2\",\"crossOrigin\":\"anonymous\",\"as\":\"font\",\"type\":\"font/woff2\"}]\n17:[\"$\",\"link\",\"font-preload-Noto Sans-700\",{\"rel\":\"preload\",\"href\":\"https://assets-v2.super.so/global/fonts/Noto_Sans/noto-sans-v27-latin-ext_latin_cyrillic-ext_cyrillic-700.woff2\",\"crossOrigin\":\"anonymous\",\"as\":\"font\",\"type\":\"font/woff2\"}]\n18:[\"$\",\"script\",\"0\",{\"type\":\"text/javascript\",\"dangerouslySetInnerHTML\":{\"__html\":\"\\n    (function(c,l,a,r,i,t,y){\\n        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};\\n        t=l.createElement(r);t.async=1;t.src=\\\"https://www.clarity.ms/tag/\\\"+i;\\n        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);\\n    })(window, document, \\\"clarity\\\", \\\"script\\\", \\\"pswoswkqtf\\\");\\n\"},\"children\":null}]\n19:[\"$\",\"meta\",\"2\",{\"name\":\"baidu-site-verification\",\"content\":\"codeva-IrK0WEobxJ\",\"children\":null}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L1b\",null,{\"settings\":{\"siteId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb\",\"userId\":\"ef6c16b7-79e4-44d2-99bf-fe8a479479b5\",\"domainName\":\"jinkunchen.com\",\"name\":\"Jinkun Chen\",\"active\":true,\"free\":false,\"tier\":\"personal\",\"favicon\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\",\"fontFamily\":\"Inter\",\"legacyTheme\":null,\"language\":\"en\",\"languages\":[],\"notionPage\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"indexPageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"pageProperties\":true,\"pageSyncing\":true,\"viewSwitcher\":true,\"layoutSidePanel\":true,\"siteSearch\":true,\"advancedSearch\":false,\"themeToggle\":false,\"mondayFirst\":false,\"loadLimits\":true,\"hidePersonInfo\":false,\"showPropertyIcons\":true,\"filterRedirectsFromSearch\":false,\"googleAnalyticsId\":\"G-QJVQ934892\",\"redirectSubdomain\":null,\"noIndex\":false,\"viewsExceeded\":false,\"cacheTTL\":0,\"manualSync\":false,\"syncStatus\":\"unsynced\",\"publishedAt\":null,\"custom404Enabled\":false,\"navbar\":{\"type\":\"simple\",\"style\":{\"height\":56,\"textColor\":\"var(--color-text-default)\",\"backgroundColor\":\"var(--color-bg-default)\",\"ctaTextColor\":\"var(--color-text-default)\",\"ctaBackgroundColor\":\"var(--color-bg-default)\",\"isSticky\":true,\"shadow\":{\"type\":\"none\",\"color\":\"rgba(0,0,0,1)\",\"opacity\":12}},\"links\":[{\"id\":\"13abc3b9-9aea-4041-bc24-1911d867cc9b\",\"type\":\"page\",\"label\":\"Home\",\"link\":\"/\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"list\":[]},{\"id\":\"767070b4-5ee7-425f-b4cb-af9097fd0d1c\",\"type\":\"page\",\"label\":\"News\",\"link\":\"/news\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::a258b05f-4680-4449-94d7-c6cef822f3a8\",\"icon\":\"\",\"list\":[]},{\"id\":\"b9351034-5c9f-4d8b-8612-0dd720c4e549\",\"type\":\"page\",\"label\":\"Publications\",\"link\":\"/publications\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::5d6544b2-7666-4fec-b572-7efb772d6c72\",\"list\":[]},{\"id\":\"528edeca-f90b-43b5-bd30-20fb90d1d3ff\",\"type\":\"page\",\"label\":\"Works\",\"link\":\"/works\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::493262ef-7a89-4b2a-af4a-afbefbd76c62\",\"list\":[]},{\"id\":\"aac48447-4e23-460e-a73d-2f63407c84a5\",\"type\":\"list\",\"label\":\"More\",\"icon\":\"ChevronDown\",\"list\":[{\"id\":\"bc44ad1a-6eb5-4955-b3b7-07ca6b4961a5\",\"type\":\"page\",\"label\":\"Blog\",\"link\":\"/blog\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::48ab9899-7e57-4426-8a4a-456ef42960e3\",\"icon\":\"\",\"list\":[]},{\"id\":\"f04ec711-4e1a-4761-965c-67fa982f7e67\",\"type\":\"page\",\"label\":\"Teaching\",\"link\":\"/teaching\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::6dc1e535-0a32-43a0-baf9-5c35aeb7f3f9\",\"list\":[]},{\"id\":\"af007f7f-e443-4c6e-becf-b33b303b1847\",\"type\":\"page\",\"label\":\"BIO\",\"link\":\"/bio\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::b6059c83-f5b7-4cb0-93c8-fd2169925d49\",\"icon\":\"\",\"description\":\"\",\"list\":[]},{\"id\":\"535c171b-da61-44b4-9729-e5e13908b47e\",\"type\":\"page\",\"label\":\"Notice\",\"link\":\"/notice\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::2b1b7936-a021-4100-a611-0a657d643aa4\",\"icon\":\"\",\"description\":\"\",\"list\":[]}]}],\"logo\":{\"type\":\"text\",\"width\":90,\"fontSize\":16,\"textContent\":\"Jinkun Chen.\",\"imageContent\":null,\"imageContentDark\":null,\"disabled\":false},\"cta\":{\"type\":\"page\",\"label\":\"\",\"link\":\"/\",\"pageId\":\"\"},\"isSticky\":false,\"breadcrumbs\":true},\"footer\":{\"type\":\"stack\",\"style\":{\"textColor\":\"var(--color-text-default)\",\"backgroundColor\":\"var(--color-bg-default)\"},\"links\":[],\"logo\":{\"type\":\"image\",\"width\":180,\"fontSize\":16,\"textContent\":\"Jinkun Chen\",\"imageContent\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/logo/712f74e3-00ca-453b-9511-39896485699f.png\",\"imageContentDark\":null,\"disabled\":false},\"socials\":[{\"type\":\"github\",\"link\":\"https://github.com/Jinnkunn\"},{\"type\":\"linkedin\",\"link\":\"https://www.linkedin.com/in/jinkun-chen/\"},{\"type\":\"twitter\",\"link\":\"https://twitter.com/_jinnkunn\"},{\"type\":\"discord\",\"link\":\"https://discord.com/users/_jinnkunn\"}],\"footnote\":\"\",\"divider\":false},\"sidebar\":{\"enabled\":false,\"links\":[],\"cta\":[],\"logo\":null,\"searchEnabled\":true},\"theme\":{\"colorMode\":\"light\",\"layout\":{\"paddingLayout\":0.6,\"layoutMaxWidth\":900,\"columnSpacing\":46,\"borderThicknessLayout\":1,\"borderTypeLayout\":\"solid\",\"borderRadiiLayout\":5,\"pageDisplay\":\"none\"},\"header\":{\"coverHeight\":30,\"titleAlign\":\"left\",\"iconAlign\":\"-112px auto auto auto\",\"display\":\"block\"},\"collectionCard\":{\"gap\":10,\"shadow\":\"rgba(15, 15, 15, 0.1) 0px 0px 0px 1px, rgba(15, 15, 15, 0.1) 0px 2px 4px\",\"coverHeightLarge\":200,\"coverHeightMedium\":200,\"coverHeightSmall\":128,\"titleSize\":0.875,\"coverSizeSmall\":172,\"coverSizeMedium\":260,\"coverSizeLarge\":320,\"iconDisplay\":\"inline-flex\"},\"callout\":{\"iconDisplay\":\"block\",\"shadow\":\"none\"},\"typography\":{\"primaryFont\":\"Noto Sans\",\"secondaryFont\":\"Noto Sans\",\"fonts\":[],\"baseSize\":16,\"titleSize\":2.5,\"headingSize\":1,\"quoteSize\":1.2,\"quoteSizeLarge\":1.4,\"textWeight\":400,\"headingWeight\":600},\"scrollbar\":{\"width\":15},\"navbar\":{\"height\":56,\"shadow\":\"none\"},\"footer\":{\"height\":0},\"sidebar\":{\"width\":280,\"shadow\":\"none\"},\"colors\":{\"light\":{\"text\":\"#37352F\",\"textLight\":\"#7d7c78\",\"background\":\"#ffffff\",\"borderColor\":\"#E9E9E7\",\"checkboxBackground\":\"#2EAADC\",\"hoverBackground\":\"#efefef\",\"scrollbar\":{\"background\":\"#FAFAFA\",\"handle\":\"#C1C1C1\",\"border\":\"#E8E8E8\"},\"navbar\":{\"text\":\"#37352F\",\"background\":\"#ffffff\",\"buttonText\":\"#ffffff\",\"buttonBackground\":\"#37352F\"},\"footer\":{\"text\":\"#37352F\",\"background\":\"#ffffff\",\"buttonText\":\"#ffffff\",\"buttonBackground\":\"#37352F\"},\"sidebar\":{\"text\":\"#37352F\",\"ctaText\":\"#37352F\",\"background\":\"#ffffff\",\"hoverBackground\":\"#efefef\",\"ctaBackground\":\"#ffffff\",\"border\":\"#E9E9E7\"},\"database\":{\"cardBackground\":\"#ffffff\",\"cardHoverBackground\":\"#f9f9f8\",\"calendarWeekendBackground\":\"#f7f6f3\"},\"default\":[45,8,20],\"gray\":[36,2,46],\"brown\":[19,31,47],\"orange\":[30,87,45],\"yellow\":[38,62,49],\"green\":[149,31,39],\"blue\":[202,53,43],\"purple\":[274,32,54],\"pink\":[328,48,53],\"red\":[2,62,55],\"form\":{\"submitButton\":{\"default\":\"#55534E\",\"gray\":\"#A7A39A\",\"brown\":\"#9A6851\",\"orange\":\"#D9730D\",\"yellow\":\"#CA922F\",\"green\":\"#448361\",\"blue\":\"#327DA9\",\"purple\":\"#8F64AF\",\"pink\":\"#C24C8B\",\"red\":\"#D44E49\"}}},\"dark\":{\"text\":\"#e1e1e1\",\"textLight\":\"#9b9b9b\",\"background\":\"#191919\",\"borderColor\":\"#373737\",\"checkboxBackground\":\"#2EAADC\",\"hoverBackground\":\"#262626\",\"scrollbar\":{\"background\":\"#FAFAFA\",\"handle\":\"#C1C1C1\",\"border\":\"#E8E8E8\"},\"navbar\":{\"text\":\"#e1e1e1\",\"background\":\"#191919\",\"buttonText\":\"#191919\",\"buttonBackground\":\"#e1e1e1\"},\"footer\":{\"text\":\"#e1e1e1\",\"background\":\"#191919\",\"buttonText\":\"#191919\",\"buttonBackground\":\"#e1e1e1\"},\"sidebar\":{\"text\":\"#e1e1e1\",\"ctaText\":\"#e1e1e1\",\"background\":\"#191919\",\"hoverBackground\":\"#292929\",\"ctaBackground\":\"#191919\",\"border\":\"#373737\"},\"database\":{\"cardBackground\":\"#262626\",\"cardHoverBackground\":\"#2f2f2f\",\"calendarWeekendBackground\":\"#202020\"},\"form\":{\"submitButton\":{\"default\":\"#55534E\",\"gray\":\"#A7A39A\",\"brown\":\"#9A6851\",\"orange\":\"#D9730D\",\"yellow\":\"#CA922F\",\"green\":\"#448361\",\"blue\":\"#327DA9\",\"purple\":\"#8F64AF\",\"pink\":\"#C24C8B\",\"red\":\"#D44E49\"}},\"default\":[45,8,20],\"gray\":[0,0,61],\"brown\":[18,35,58],\"orange\":[25,54,53],\"yellow\":[38,54,54],\"green\":[146,32,47],\"blue\":[217,50,58],\"purple\":[270,55,62],\"pink\":[329,57,58],\"red\":[1,69,60]}}},\"code\":{\"global\":[{\"id\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::e8b7f00d-71b6-4a4f-b678-64ca618370c7\",\"content\":\"\u003cscript type=\\\"text/javascript\\\"\u003e\\n    (function(c,l,a,r,i,t,y){\\n        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};\\n        t=l.createElement(r);t.async=1;t.src=\\\"https://www.clarity.ms/tag/\\\"+i;\\n        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);\\n    })(window, document, \\\"clarity\\\", \\\"script\\\", \\\"pswoswkqtf\\\");\\n\u003c/script\u003e\\n\u003cmeta name=\\\"baidu-site-verification\\\" content=\\\"codeva-IrK0WEobxJ\\\" /\u003e\\n\"}],\"head\":[],\"body\":[],\"style\":[],\"css\":\"span[class=\\\"highlighted-background bg-yellow\\\"] {\\n  /*display: inline-block;*/\\n  background-color: transparent;\\n}\\n\\na[class=\\\"notion-link link\\\"] {\\n  display: inline-block;\\n  position: relative;\\n}\\n\\na[class=\\\"notion-link link\\\"]:after {\\n  position: absolute;\\n  content: \\\" \\\";\\n  top: 66%;\\n  bottom: 0;\\n  left: 0em;\\n  right: -0.1em;\\n  transition: top 800ms cubic-bezier(0, 0.8, 0.13, 1);\\n  background-color: hsl(38, 152%, 80%);\\n  z-index: -1;\\n}\\n\\na[class=\\\"notion-link link\\\"]:hover:after {\\n  top: 2%;\\n}\"},\"files\":[]},\"config\":{\"assetEndpoint\":\"https://assets.super.so\",\"isPreview\":false},\"children\":[\"$L1c\",\"$L1d\",\"$L1e\"]}],\"$L1f\"]}]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"$L20\",null,{\"pageReplacement\":\"$undefined\",\"getWebVital\":false,\"pageId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"records\":{\"block\":{\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\":{\"id\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"children\":[\"37f7210c96014e2a9f56aecbc694e2c5\",\"6c8403d936fb490e8e24f1bf3563e2eb\",\"2cbe5c0189b147baaaf2eebd568f3e11\",\"4a8efe9c8c844c6099a647fae633e65a\",\"d95145e3a15c47179bb2c31dae95d2ef\",\"975a6c020a53497d81afe6ed91c2ac38\",\"15da42c0920c4fe5a384039880acbf25\",\"21740d70fdf5809e9b08e50fd38ec2c9\",\"21740d70fdf5807cb076cbe47bde3bc5\",\"21740d70fdf5803ba492f8f7a0f1124b\",\"21740d70fdf5804496dffff09e93c171\",\"21740d70fdf58019b4e5f82038300e4e\",\"21740d70fdf580789318c4b68c9dc180\",\"21740d70fdf580b59114ebb95e594cb4\",\"21740d70fdf580998476c769158dc800\",\"21740d70fdf580ae8f73f007b0a8ec92\",\"21740d70fdf5807ba813e037c8c3158e\",\"21740d70fdf580e8a51ad82068f715f8\",\"21740d70fdf5804ea1b3c814f4776242\",\"21740d70fdf5804ea49fd96ef424e931\",\"21740d70fdf58009ba9df8dfcba9a012\",\"21740d70fdf580849758fee1679a61da\",\"21740d70fdf580b78f06c6a97022bc5c\",\"21740d70fdf580e887d9cfc1af9f673c\",\"21740d70fdf580b3a0d7c0300ffd118a\",\"21740d70fdf580d1b729df508408cc3d\",\"21740d70fdf58034b829fad21730876f\",\"21740d70fdf580a7a5e1c5589a27f4b4\",\"21740d70fdf5803d9807e94a97e877c9\",\"21740d70fdf580428548dda1c2b7ff6a\",\"21740d70fdf580e68ebfe97e77da4cf0\",\"21740d70fdf580ceb7d1e138749fece9\",\"21740d70fdf5802594fac1a34fc2b5be\",\"21740d70fdf580aca4b9f403a1abb511\",\"21740d70fdf5807685ece0dd6484ea2e\",\"21740d70fdf58026839cfc6fc508c42b\",\"21740d70fdf580efb9a6db405332f049\",\"21740d70fdf5800fae7ed100c5ecd009\",\"21740d70fdf580959badf2d68cc1f896\",\"21740d70fdf580c0b810f7633ce50e32\",\"21740d70fdf5807b8ea1e3f0ef8cce21\",\"21740d70fdf580408d4fc76f4eef75dc\",\"21740d70fdf580d78798c7861511ed2e\",\"21740d70fdf5805299ddcb51cebbc031\",\"21740d70fdf580f880d2ce188348e4e8\",\"21740d70fdf580778da5dc116592082c\",\"21740d70fdf580f39177ff83be84b5c3\",\"21740d70fdf58015b472e155c66d727f\",\"21740d70fdf580a68709d4be6ca361e1\",\"21740d70fdf580c4a559c3eff45d8257\",\"21740d70fdf5800585f8e60d24de38aa\",\"21740d70fdf580aebbeaf862f0f1bb84\",\"21740d70fdf580d8a90af2ad47b882d5\",\"21740d70fdf580ba874bff70dae33e6c\",\"21740d70fdf580a281e2d404fdd2bcc7\",\"21740d70fdf580848881d08f51d59cdf\",\"21740d70fdf5808f9238d67cfb9d9d82\",\"21740d70fdf58081a3ecd927972a059f\",\"21740d70fdf58058a46cfc4b81aaecc4\",\"21740d70fdf5807490e3ef6ba727c883\",\"21740d70fdf58003b9c1f7a840f17ca1\",\"21740d70fdf5807382e1cfca2e9abdb2\",\"21740d70fdf5802cb8e3e90f285484c4\",\"21740d70fdf5808da1edd71553e8d49c\",\"21740d70fdf5807d866be836369721de\",\"21740d70fdf58018b8d2ed528c1162a2\",\"21740d70fdf5807c929cdc484058c538\",\"21740d70fdf580f9821be4ec0e5fc9d4\",\"21740d70fdf5805f9dc2d7b972b2041d\",\"21740d70fdf5806b9a45ea2b164dfdd2\",\"21740d70fdf580d4ba66c5c4765273a0\",\"21740d70fdf580459646fcd5a2a23040\",\"21740d70fdf5804cb009d3d8e8e002a5\",\"21740d70fdf58004ad5fd2eeaf65e692\",\"21740d70fdf580ba9bdfd3bff3ab2e3e\",\"21740d70fdf580f7923dca2a57ce5607\",\"21740d70fdf580e2a4f9c97d9434d4cb\",\"21740d70fdf580d28441d48edb42e729\",\"21740d70fdf580139869c3006b44178b\",\"21740d70fdf58011839fd05b5333a270\",\"21740d70fdf5809dbe10fa8f1c49222f\",\"21740d70fdf580acad63f4eef3953f6b\",\"21740d70fdf580e4a606e6f4424b31e5\",\"21740d70fdf58093a210cb9d25926543\",\"21740d70fdf580ad845cdef42ba8a4e7\",\"21740d70fdf580948a38d453a10fe70d\",\"21740d70fdf58044bc06dbd3566e29b3\",\"21740d70fdf580c58320e68a6859b88f\",\"21740d70fdf580889f7bc0535e95d69a\",\"21740d70fdf5806baa0cd10113df6c6c\",\"21740d70fdf580cf802debf13bd62d5a\",\"21740d70fdf580d5978af3c7e30f6305\",\"21740d70fdf580c2b190c8654f824624\",\"21740d70fdf58090ae04ef67d7ee79bd\",\"21740d70fdf580e0a67ddf16d23ac95b\",\"21740d70fdf580bdab4cf02a41144b18\",\"21740d70fdf58085858fd4791e4f101b\",\"21740d70fdf58011a43df811886b1920\",\"21740d70fdf5800bb60debab93527a57\",\"21740d70fdf58049bce1f1a47e2d8051\",\"21740d70fdf580e49c36f4912bd6ac93\",\"21740d70fdf580e895f8f04cd3bdbc49\",\"21740d70fdf58025a2fec1c13e30b145\",\"21740d70fdf580b3bb55f18aef19ec66\",\"21740d70fdf580f4b7a0d4c4313da49c\",\"21740d70fdf58057bc2bc6c92d18067c\",\"21740d70fdf580c9a5c9cbbe9a3723c1\",\"21740d70fdf580059d5be7ecaceed316\",\"21740d70fdf580ca8329d34ae124a4ac\",\"21740d70fdf580bb9f60cd579f054e8f\",\"21740d70fdf58072ab0fc215757eb48b\",\"21740d70fdf5802cb5a5e4c8136d32b4\",\"21740d70fdf580438856f1a03608dfd0\",\"21740d70fdf58012ab78f973a8a870ee\",\"21740d70fdf580ba9d25c88f6cfa1e49\",\"21740d70fdf580aa9d37d06095234106\",\"21740d70fdf580388756d009905ff072\",\"21740d70fdf580fe8ae4f3d4e38c4ec2\"],\"hasContent\":true,\"parentId\":\"21040d70fdf580fb9cbb000b32f903d7\",\"title\":[[\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\",[[\"b\",null]]]],\"updatedAt\":1770351032755,\"type\":\"page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"createdTime\":1750303131238,\"lastEditedTime\":1770351032755,\"createdBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\",\"1\":\"cd4a93f4-929c-4ceb-b0ff-87f473045452\"}]}],\"lastEditedBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\"}]}],\"layout\":null,\"propertySort\":[{\"property\":\"]qQn\",\"visibility\":\"show\",\"type\":\"date\",\"name\":\"Date\"},{\"property\":\"]Qvn\",\"visibility\":\"show\",\"type\":\"person\",\"name\":\"Author\"}],\"uri\":\"/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"fullWidth\":false,\"smallText\":false,\"noCover\":true,\"superProperties\":{},\"propertyValues\":{\"]qQn\":[[\"\",[[\"d\",{\"type\":\"date\",\"start_date\":\"2025-06-19\"}]]]],\"]Qvn\":[{\"0\":\"\",\"1\":[{\"0\":\"u\",\"1\":\"cd4a93f4-929c-4ceb-b0ff-87f473045452\"}]}]},\"blockId\":\"21740d70-fdf5-80f2-81fb-e08a7014ce1d\"},\"37f7210c96014e2a9f56aecbc694e2c5\":{\"id\":\"37f7210c96014e2a9f56aecbc694e2c5\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Why the order of retrieved information can quietly change how AI reasons step by step\",[[\"i\",null]]]],\"updatedAt\":1769966955067,\"type\":\"text\"},\"6c8403d936fb490e8e24f1bf3563e2eb\":{\"id\":\"6c8403d936fb490e8e24f1bf3563e2eb\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"You give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.\"]],\"updatedAt\":1769966955068,\"type\":\"text\"},\"2cbe5c0189b147baaaf2eebd568f3e11\":{\"id\":\"2cbe5c0189b147baaaf2eebd568f3e11\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"This behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages (\\\"chunks\\\") shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, \\\"multi-step inference\\\" refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.\"]],\"updatedAt\":1769966955068,\"type\":\"text\"},\"4a8efe9c8c844c6099a647fae633e65a\":{\"id\":\"4a8efe9c8c844c6099a647fae633e65a\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"A key finding is that preserving the original document structure, as seen in Document's Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM's sequential processing.\"]],\"updatedAt\":1769966955069,\"type\":\"text\"},\"d95145e3a15c47179bb2c31dae95d2ef\":{\"id\":\"d95145e3a15c47179bb2c31dae95d2ef\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"For users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.\"]],\"updatedAt\":1769966955069,\"type\":\"text\"},\"975a6c020a53497d81afe6ed91c2ac38\":{\"id\":\"975a6c020a53497d81afe6ed91c2ac38\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The analysis further reveals that LLMs exhibit a \\\"cognitive linearity\\\", performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.\"]],\"updatedAt\":1769966955069,\"type\":\"text\"},\"15da42c0920c4fe5a384039880acbf25\":{\"id\":\"15da42c0920c4fe5a384039880acbf25\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Beyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.\"]],\"updatedAt\":1769966955069,\"type\":\"text\"},\"21740d70fdf5809e9b08e50fd38ec2c9\":{\"id\":\"21740d70fdf5809e9b08e50fd38ec2c9\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[],\"updatedAt\":1750308635332,\"type\":\"text\"},\"21740d70fdf5807cb076cbe47bde3bc5\":{\"id\":\"21740d70fdf5807cb076cbe47bde3bc5\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"divider\"},\"21740d70fdf5803ba492f8f7a0f1124b\":{\"id\":\"21740d70fdf5803ba492f8f7a0f1124b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"table_of_contents\",\"color\":\"ColorGray\"},\"21740d70fdf5804496dffff09e93c171\":{\"id\":\"21740d70fdf5804496dffff09e93c171\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"divider\"},\"21740d70fdf58019b4e5f82038300e4e\":{\"id\":\"21740d70fdf58019b4e5f82038300e4e\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580789318c4b68c9dc180\":{\"id\":\"21740d70fdf580789318c4b68c9dc180\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf580b59114ebb95e594cb4\":{\"id\":\"21740d70fdf580b59114ebb95e594cb4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580998476c769158dc800\":{\"id\":\"21740d70fdf580998476c769158dc800\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580ae8f73f007b0a8ec92\":{\"id\":\"21740d70fdf580ae8f73f007b0a8ec92\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Large Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.\"],[\"\",[[\"b\",null],[\"e\",\"^1\"]]],[\" These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.\"],[\"\",[[\"b\",null],[\"e\",\"^1\"]]],[\" LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.\"],[\"\",[[\"b\",null],[\"e\",\"^2\"]]],[\" Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.\"],[\"\",[[\"b\",null],[\"e\",\"^3\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5807ba813e037c8c3158e\":{\"id\":\"21740d70fdf5807ba813e037c8c3158e\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Despite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.\"],[\"\",[[\"b\",null],[\"e\",\"^2\"]]],[\" This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as \\\"hallucinations\\\", where the model generates factually incorrect or nonsensical information not present in its training data.\"],[\"\",[[\"b\",null],[\"e\",\"^2\"]]],[\" Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.\"],[\"\",[[\"b\",null],[\"e\",\"^5\"]]],[\" These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.\"]],\"updatedAt\":1750347459608,\"type\":\"text\"},\"21740d70fdf580e8a51ad82068f715f8\":{\"id\":\"21740d70fdf580e8a51ad82068f715f8\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5804ea1b3c814f4776242\":{\"id\":\"21740d70fdf5804ea1b3c814f4776242\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"1.2 Understanding Retrieval-Augmented Generation (RAG)\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf5804ea49fd96ef424e931\":{\"id\":\"21740d70fdf5804ea49fd96ef424e931\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"\\nRetrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58009ba9df8dfcba9a012\":{\"id\":\"21740d70fdf58009ba9df8dfcba9a012\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The core process of RAG typically involves two main stages. First, \"],[\"Retrieval and Pre-processing\",[[\"b\",null]]],[\" occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" The second stage is \"],[\"Grounded Generation\",[[\"b\",null]]],[\", where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM's context. This integration significantly enhances the LLM's understanding of the topic, allowing it to produce more precise, informative, and engaging responses.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580849758fee1679a61da\":{\"id\":\"21740d70fdf580849758fee1679a61da\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"RAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" This factual grounding is crucial for mitigating \\\"gen AI hallucinations\\\" by supplying verifiable facts as part of the input prompt.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM's outputs.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580b78f06c6a97022bc5c\":{\"id\":\"21740d70fdf580b78f06c6a97022bc5c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"1.3 The Essence of Multi-Step Inference in LLMs\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580e887d9cfc1af9f673c\":{\"id\":\"21740d70fdf580e887d9cfc1af9f673c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Multi-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM's capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.\"],[\"\",[[\"b\",null],[\"e\",\"^5\"]]],[\" This capability extends beyond merely following a single instruction or performing a singular task.\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580b3a0d7c0300ffd118a\":{\"id\":\"21740d70fdf580b3a0d7c0300ffd118a\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.\"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\" It is widely recognized as a key indicator of advanced intelligence in AI systems.\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]],[\" However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.\"],[\"\",[[\"b\",null],[\"e\",\"^5\"]]],[\" While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580d1b729df508408cc3d\":{\"id\":\"21740d70fdf580d1b729df508408cc3d\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"To address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]],[\" Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.\"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\" The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. \"],[\"This shift underscores that future advancements in LLMs and RAG systems must prioritize not just \",[[\"m\",\"21740d70-fdf5-8031-ba69-001ccc02048e\"],[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\"what\",[[\"_\",null],[\"b\",null],[\"i\",null],[\"m\",\"21740d70-fdf5-8031-ba69-001ccc02048e\"],[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\" information is retrieved, but \",[[\"m\",\"21740d70-fdf5-8031-ba69-001ccc02048e\"],[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\"how\",[[\"_\",null],[\"b\",null],[\"i\",null],[\"m\",\"21740d70-fdf5-8031-ba69-001ccc02048e\"],[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\" that information facilitates a structured\",[[\"m\",\"21740d70-fdf5-8031-ba69-001ccc02048e\"],[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\", step-by-step problem-solving process.\",[[\"m\",\"21740d70-fdf5-80dd-9dab-001c5c435173\"]]],[\" This elevates the importance of context organization and coherence within the input.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58034b829fad21730876f\":{\"id\":\"21740d70fdf58034b829fad21730876f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"1.4 Purpose and Scope of the Report\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580a7a5e1c5589a27f4b4\":{\"id\":\"21740d70fdf580a7a5e1c5589a27f4b4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM's capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5803d9807e94a97e877c9\":{\"id\":\"21740d70fdf5803d9807e94a97e877c9\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"2. Chunking and Retrieval in RAG Architectures\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf580428548dda1c2b7ff6a\":{\"id\":\"21740d70fdf580428548dda1c2b7ff6a\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580e68ebfe97e77da4cf0\":{\"id\":\"21740d70fdf580e68ebfe97e77da4cf0\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"2.1 Principles of Document Chunking for RAG\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580ceb7d1e138749fece9\":{\"id\":\"21740d70fdf580ceb7d1e138749fece9\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Chunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.\"],[\"\",[[\"b\",null],[\"e\",\"^{12}\"]]],[\" These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5802594fac1a34fc2b5be\":{\"id\":\"21740d70fdf5802594fac1a34fc2b5be\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580aca4b9f403a1abb511\":{\"id\":\"21740d70fdf580aca4b9f403a1abb511\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Several chunking strategies are employed, each with distinct advantages and use cases:\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5807685ece0dd6484ea2e\":{\"id\":\"21740d70fdf5807685ece0dd6484ea2e\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Fixed Size Chunking:\",[[\"b\",null]]],[\" This straightforward approach divides text into uniform chunks based on a predefined character or token count.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf58026839cfc6fc508c42b\":{\"id\":\"21740d70fdf58026839cfc6fc508c42b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Recursive-Based Chunking:\",[[\"b\",null]]],[\" A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580efb9a6db405332f049\":{\"id\":\"21740d70fdf580efb9a6db405332f049\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Sentence-based Chunking:\",[[\"b\",null]]],[\" This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5800fae7ed100c5ecd009\":{\"id\":\"21740d70fdf5800fae7ed100c5ecd009\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Document Structure-based Chunking:\",[[\"b\",null]]],[\" This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580959badf2d68cc1f896\":{\"id\":\"21740d70fdf580959badf2d68cc1f896\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Semantic Chunking:\",[[\"b\",null]]],[\" This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.\"],[\"\",[[\"b\",null],[\"e\",\"^{14}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580c0b810f7633ce50e32\":{\"id\":\"21740d70fdf580c0b810f7633ce50e32\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM's ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5807b8ea1e3f0ef8cce21\":{\"id\":\"21740d70fdf5807b8ea1e3f0ef8cce21\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"2.2 Overview of Retrieval Mechanisms in RAG\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580408d4fc76f4eef75dc\":{\"id\":\"21740d70fdf580408d4fc76f4eef75dc\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"A RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a \"],[\"Query Encoder\",[[\"b\",null]]],[\" transforms the user's input query into a representation suitable for searching the knowledge base.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\" Second, a \"],[\"Retriever\",[[\"b\",null]]],[\" takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\"Finally, a \"],[\"Generator\",[[\"b\",null]]],[\", typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\"\\n\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580d78798c7861511ed2e\":{\"id\":\"21740d70fdf580d78798c7861511ed2e\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Retrievers can be broadly categorized based on their underlying mechanisms:\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5805299ddcb51cebbc031\":{\"id\":\"21740d70fdf5805299ddcb51cebbc031\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Sparse Retrievers:\",[[\"b\",null]]],[\" These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580f880d2ce188348e4e8\":{\"id\":\"21740d70fdf580f880d2ce188348e4e8\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Dense Retrievers:\",[[\"b\",null]]],[\" Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580778da5dc116592082c\":{\"id\":\"21740d70fdf580778da5dc116592082c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Hybrid Search:\",[[\"b\",null]]],[\" Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\"\\nThe retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" After the initial retrieval, an essential step often involves \"]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580f39177ff83be84b5c3\":{\"id\":\"21740d70fdf580f39177ff83be84b5c3\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"re-rankers\",[[\"b\",null]]],[\". These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" The effectiveness of RAG is therefore highly dependent on the retriever's ability to provide relevant information and the re-ranker's capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM's performance can degrade, leading to responses that, while \\\"grounded\\\" in the provided context, might be off-topic or factually incorrect.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the  \"],[\"most relevant\",[[\"i\",null]]],[\" and \"],[\"least distracting\",[[\"i\",null]]],[\" content, a factor that profoundly influences the subsequent chunk ordering.\"]],\"updatedAt\":1750305059966,\"type\":\"bulleted_list\"},\"21740d70fdf58015b472e155c66d727f\":{\"id\":\"21740d70fdf58015b472e155c66d727f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"3. The Critical Role of Chunk Retrieval Sequence\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_sub_header\",\"depth\":3},\"21740d70fdf580a68709d4be6ca361e1\":{\"id\":\"21740d70fdf580a68709d4be6ca361e1\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM's ability to reason effectively.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580c4a559c3eff45d8257\":{\"id\":\"21740d70fdf580c4a559c3eff45d8257\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"3.1 Impact of Context Order on LLM Performance\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf5800585f8e60d24de38aa\":{\"id\":\"21740d70fdf5800585f8e60d24de38aa\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Observations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM's internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model's accuracy.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580aebbeaf862f0f1bb84\":{\"id\":\"21740d70fdf580aebbeaf862f0f1bb84\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Conversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" This \\\"ordering effect\\\" is further exacerbated when irrelevant premises are introduced into the prompt.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model's ability to synthesize information and produce coherent responses.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" This indicates that LLMs, despite their advanced capabilities, exhibit a form of \\\"cognitive linearity\\\" in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a \\\"ground truth proof\\\" \"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" suggests that the LLM's internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580d8a90af2ad47b882d5\":{\"id\":\"21740d70fdf580d8a90af2ad47b882d5\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"3.2 Document's Original Structure (DOS RAG) and its Benefits\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580ba874bff70dae33e6c\":{\"id\":\"21740d70fdf580ba874bff70dae33e6c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Document's Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580a281e2d404fdd2bcc7\":{\"id\":\"21740d70fdf580a281e2d404fdd2bcc7\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The benefits of DOS RAG are significant and empirically validated. It primarily \"],[\"preserves passage continuity\",[[\"b\",null]]],[\", maintaining the document's structural integrity and narrative flow.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves \\n\"],[\"improved accuracy\",[[\"b\",null]]],[\", outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including Bench, QuALITY, and NarrativeQA.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" For instance, on the Bench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG's 87.8%.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" Furthermore, DOS RAG demonstrates notable \\n\"],[\"efficiency\",[[\"b\",null]]],[\", often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]]],\"updatedAt\":1750303955702,\"type\":\"text\"},\"21740d70fdf580848881d08f51d59cdf\":{\"id\":\"21740d70fdf580848881d08f51d59cdf\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, \"],[\"contextual coherence\",[[\"i\",null]]],[\" and \"],[\"narrative flow\",[[\"i\",null]]],[\", as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" However, DOS RAG's consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the \"],[\"relationship\",[[\"i\",null]]],[\" between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.\"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\" Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM's processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of \\\"relevance\\\" for multi-step tasks should be broadened to include \\\"contextual relevance\\\" or \\\"narrative relevance\\\" in addition to traditional semantic similarity.\"]],\"updatedAt\":1750303419405,\"type\":\"text\"},\"21740d70fdf5808f9238d67cfb9d9d82\":{\"id\":\"21740d70fdf5808f9238d67cfb9d9d82\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"3.3 Reranking Strategies and Context Reordering\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf58081a3ecd927972a059f\":{\"id\":\"21740d70fdf58081a3ecd927972a059f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Reranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\" This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\"\\nVarious types of rerankers are employed, each with distinct characteristics:\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58058a46cfc4b81aaecc4\":{\"id\":\"21740d70fdf58058a46cfc4b81aaecc4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Cross-Encoders:\",[[\"b\",null]]],[\" These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" Examples include Sentence Transformers, Flashrank, and BGE-M3.\"],[\"\",[[\"b\",null],[\"e\",\"^{15}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5807490e3ef6ba727c883\":{\"id\":\"21740d70fdf5807490e3ef6ba727c883\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Multi-Vector Rerankers:\",[[\"b\",null]]],[\" Models like ColBERT use a \\\"late interaction\\\" approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf58003b9c1f7a840f17ca1\":{\"id\":\"21740d70fdf58003b9c1f7a840f17ca1\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Fine-tuned LLM Rerankers:\",[[\"b\",null]]],[\" Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5807382e1cfca2e9abdb2\":{\"id\":\"21740d70fdf5807382e1cfca2e9abdb2\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"LLM as a Judge:\",[[\"b\",null]]],[\" This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" Examples include GPT, Claude, and Gemini.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5802cb8e3e90f285484c4\":{\"id\":\"21740d70fdf5802cb8e3e90f285484c4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Reranking APIs:\",[[\"b\",null]]],[\" Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" Examples include Cohere, Jina, and Mixedbread.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5808da1edd71553e8d49c\":{\"id\":\"21740d70fdf5808da1edd71553e8d49c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Beyond simple relevance scoring, context reordering within the reranking process also plays a role. \"],[\"Inverted Context Ordering\",[[\"b\",null]]],[\" is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.\"],[\"\",[[\"b\",null],[\"e\",\"^{16}\"]]],[\" This method has demonstrated a performance increase in correctness for multi-hop QA tasks.\"],[\"\",[[\"b\",null],[\"e\",\"^{24}\"]]],[\" Other advanced approaches include \"],[\"Fusion-based Reranking\",[[\"b\",null]]],[\", which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks \"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\", and \"],[\"Adaptive Reranking\",[[\"b\",null]]],[\", which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5807d866be836369721de\":{\"id\":\"21740d70fdf5807d866be836369721de\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"While reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where \\\"better\\\" relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the \"],[\"most\",[[\"i\",null]]],[\" relevant context.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]],[\" However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.\"],[\"\",[[\"b\",null],[\"e\",\"^{15}\"]]],[\" This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system's usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This implies that the \\\"best\\\" reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM.\\n\"],[\"\\nTable 1: Comparison of Key Chunking and Reordering Strategies in RAG\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58018b8d2ed528c1162a2\":{\"id\":\"21740d70fdf58018b8d2ed528c1162a2\",\"children\":[\"21740d70fdf580e1a5b0d2ac98b3c690\",\"21740d70fdf5805080a5c3692e5eeb09\",\"21740d70fdf5807ca905e19a618e0a48\",\"21740d70fdf5804da0c1f4d6f02bbd23\",\"21740d70fdf580338f32d47185f32232\",\"21740d70fdf5805b9621fff137881fa9\",\"21740d70fdf580f59c37e3f38a6cc678\"],\"hasContent\":true,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"table\",\"colOrder\":[\"I{;C\",\"Bmzb\",\"h]Bv\",\"a|Q{\",\"cJ@_\",\"MGks\",\"JNqP\"],\"items\":[{\"id\":\"21740d70-fdf5-80e1-a5b0-d2ac98b3c690\",\"properties\":{\"Bmzb\":[[\"Description\",[[\"b\",null]]]],\"I{;C\":[[\"Strategy\",[[\"b\",null]]]],\"JNqP\":[[\"Relevant Snippets\",[[\"b\",null]]]],\"MGks\":[[\"Disadvantages\",[[\"b\",null]]]],\"a|Q{\":[[\"Impact on Multi-Step Inference\",[[\"b\",null]]]],\"cJ@_\":[[\"Advantages\",[[\"b\",null]]]],\"h]Bv\":[[\"Primary Goal\",[[\"b\",null]]]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-8050-80a5-c3692e5eeb09\",\"properties\":{\"Bmzb\":[[\"Divides text into uniform segments (e.g., 500 tokens), often with overlap.\"]],\"I{;C\":[[\"Fixed Size Chunking\"]],\"JNqP\":[[\"13\"]],\"MGks\":[[\"Context fragmentation, information loss, inflexible.\"]],\"a|Q{\":[[\"Can fragment context, hindering logical flow.\"]],\"cJ@_\":[[\"Easy to implement, fast, consistent.\"]],\"h]Bv\":[[\"Efficiency, Simplicity\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-807c-a905-e19a618e0a48\",\"properties\":{\"Bmzb\":[[\"Uses multiple separators (paragraphs, sentences) to find meaningful boundaries.\"]],\"I{;C\":[[\"Recursive-Based Chunking\"]],\"JNqP\":[[\"13\"]],\"MGks\":[[\"More complex to implement.\"]],\"a|Q{\":[[\"Better at maintaining logical units for sequential understanding.\"]],\"cJ@_\":[[\"Adaptive, preserves logical flow.\"]],\"h]Bv\":[[\"Context Preservation\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-804d-a0c1-f4d6f02bbd23\",\"properties\":{\"Bmzb\":[[\"Divides text into complete sentences.\"]],\"I{;C\":[[\"Sentence-based Chunking\"]],\"JNqP\":[[\"13\"]],\"MGks\":[[\"May create very small chunks, less efficient for long documents.\"]],\"a|Q{\":[[\"Supports logical flow, good for connecting ideas.\"]],\"cJ@_\":[[\"Ensures complete thoughts, natural boundaries.\"]],\"h]Bv\":[[\"Preserve Complete Thoughts\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-8033-8f32-d47185f32232\",\"properties\":{\"Bmzb\":[[\"Retrieves chunks and reorders them to match their original document sequence.\"]],\"I{;C\":[[\"Document's Original Structure (DOS RAG)\"]],\"JNqP\":[[\"17\"]],\"MGks\":[[\"Requires tracking chunk positions; may include less relevant chunks if not filtered.\"]],\"a|Q{\":[[\"Significantly improves performance by maintaining logical progression; crucial for multi-hop QA.\"]],\"cJ@_\":[[\"Preserves narrative, robust QA, often outperforms relevance-based sorting.\"]],\"h]Bv\":[[\"Narrative Continuity, Contextual Coherence\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-805b-9621-fff137881fa9\",\"properties\":{\"Bmzb\":[[\"Arranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.\"]],\"I{;C\":[[\"Inverted Context Ordering\"]],\"JNqP\":[[\"16\"]],\"MGks\":[[\"Still relies on relevance score, may disrupt original narrative flow.\"]],\"a|Q{\":[[\"Can improve correctness; focuses LLM on key information.\"]],\"cJ@_\":[[\"Directs LLM to top relevant info immediately.\"]],\"h]Bv\":[[\"Prioritize Most Relevant\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80f5-9c37-e3f38a6cc678\",\"properties\":{\"Bmzb\":[[\"Divides documents into semantically coherent and non-overlapping chunks.\"]],\"I{;C\":[[\"Semantic Chunking\"]],\"JNqP\":[[\"14\"]],\"MGks\":[[\"Requires sophisticated LLM-based relevance scoring.\"]],\"a|Q{\":[[\"Enhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.\"]],\"cJ@_\":[[\"Reduces hallucinations, improves factual accuracy, aligned with query needs.\"]],\"h]Bv\":[[\"Reduce Irrelevance, Improve Accuracy\"]]},\"color\":\"color-default\"}]},\"21740d70fdf5807c929cdc484058c538\":{\"id\":\"21740d70fdf5807c929cdc484058c538\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"4. Factors Influencing Multi-Step Inference Performance in RAG\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf580f9821be4ec0e5fc9d4\":{\"id\":\"21740d70fdf580f9821be4ec0e5fc9d4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Beyond the direct ordering of retrieved chunks, several other factors interact with the LLM's context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5805f9dc2d7b972b2041d\":{\"id\":\"21740d70fdf5805f9dc2d7b972b2041d\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"4.1 Positional Bias and the \\\"Lost-in-the-Middle\\\" Effect\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf5806b9a45ea2b164dfdd2\":{\"id\":\"21740d70fdf5806b9a45ea2b164dfdd2\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Positional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" A specific manifestation of this is the \\\"lost-in-the-middle\\\" effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" This bias can affect both the LLM's capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580d4ba66c5c4765273a0\":{\"id\":\"21740d70fdf580d4ba66c5c4765273a0\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"While earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM's presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.\"],[\"\",[[\"b\",null],[\"e\",\"^{26}\"]]],[\" This is attributed to a \\\"contrastive effect\\\", where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.\"],[\"\",[[\"b\",null],[\"e\",\"^{27}\"]]],[\" Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a \\\"parametric bias\\\", relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.\"],[\"\",[[\"b\",null],[\"e\",\"^{27}\"]]]],\"updatedAt\":1750347483269,\"type\":\"text\"},\"21740d70fdf580459646fcd5a2a23040\":{\"id\":\"21740d70fdf580459646fcd5a2a23040\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The \\\"lost-in-the-middle\\\" effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to \\\"trick\\\" the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the \"],[\"quality\",[[\"i\",null]]],[\" of retrieved content and enhancing the LLM's inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" However, in practical RAG systems, where retrievers often fetch \"],[\"both\",[[\"i\",null]]],[\" relevant and highly distracting passages \"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\", the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the \\\"best\\\" chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.\"],[\"\",[[\"b\",null],[\"e\",\"^{27}\"]]],[\" This shifts the focus from \"],[\"where\",[[\"i\",null]]],[\" to place chunks to \"],[\"what\",[[\"i\",null]]],[\" chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5804cb009d3d8e8e002a5\":{\"id\":\"21740d70fdf5804cb009d3d8e8e002a5\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"4.2 The Detrimental Impact of Irrelevant and Distracting Information\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf58004ad5fd2eeaf65e692\":{\"id\":\"21740d70fdf58004ad5fd2eeaf65e692\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"A well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, \\\"distracting passages\\\", contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]]],\"updatedAt\":1750347452375,\"type\":\"text\"},\"21740d70fdf580ba9bdfd3bff3ab2e3e\":{\"id\":\"21740d70fdf580ba9bdfd3bff3ab2e3e\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]],[\" Studies have shown that \\\"hard distracting passages\\\", those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to \\\"weak\\\" ones, and this detrimental effect persists even in larger LLMs.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]],[\"\\nParadoxically, \\\"stronger\\\" retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver \"],[\"more harmful distractors\",[[\"i\",null]]],[\".\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]],[\" Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]]],\"updatedAt\":1750303827098,\"type\":\"text\"},\"21740d70fdf580f7923dca2a57ce5607\":{\"id\":\"21740d70fdf580f7923dca2a57ce5607\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The very act of retrieval, especially with \\\"stronger\\\" retrievers, presents a \\\"double-edged sword.\\\" While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG's core purpose is to provide relevant external knowledge.\"],[\"\",[[\"b\",null],[\"e\",\"^4\"]]],[\" However, no retriever is perfect, and they often return irrelevant or \\\"distracting\\\" passages.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" The critical observation here is that \"],[\"stronger\",[[\"i\",null]]],[\" retrievers, which are designed to find more relevant information, also tend to retrieve \"],[\"more harmful\",[[\"i\",null]]],[\" distracting passages.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" This creates a paradox: improving the retriever's primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for \\\"relevance\\\" (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the \"],[\"distracting effect\",[[\"i\",null]]],[\" to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580e2a4f9c97d9434d4cb\":{\"id\":\"21740d70fdf580e2a4f9c97d9434d4cb\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"4.3 Cognitive Load and Context Window Management\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580d28441d48edb42e729\":{\"id\":\"21740d70fdf580d28441d48edb42e729\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Large Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" The concept of \\\"cognitive load\\\", analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).\"],[\"\",[[\"b\",null],[\"e\",\"^{31}\"]]],[\" For LLMs, the inherent \\\"content complexity\\\" of the input is a dominant factor influencing their processing efficiency.\"],[\"\",[[\"b\",null],[\"e\",\"^{31}\"]]]],\"updatedAt\":1750347466192,\"type\":\"text\"},\"21740d70fdf580139869c3006b44178b\":{\"id\":\"21740d70fdf580139869c3006b44178b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Presenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its \\\"cognitive load\\\".\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" This overload can lead to reduced selection accuracy and an increase in hallucinations.\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" Conversely, supplying \"],[\"only\",[[\"i\",null]]],[\" the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of \\\"prompt bloat\\\" directly lowers the LLM's cognitive load.\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM's decision-making capabilities are markedly improved.\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]]],\"updatedAt\":1750303834719,\"type\":\"text\"},\"21740d70fdf58011839fd05b5333a270\":{\"id\":\"21740d70fdf58011839fd05b5333a270\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"While long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" The key lies in identifying the \\\"sweet spot\\\" for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5809dbe10fa8f1c49222f\":{\"id\":\"21740d70fdf5809dbe10fa8f1c49222f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The concept of \\\"cognitive load\\\" in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the \"],[\"quality and conciseness\",[[\"i\",null]]],[\" of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts \"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\", the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" This is framed in terms of \\\"cognitive load\\\".\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" If the LLM is forced to \\\"sift through hundreds of distractors\\\" \"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\", it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about \"],[\"what\",[[\"i\",null]]],[\" to retrieve, but \"],[\"how much\",[[\"b\",null],[\"i\",null]]],[\" and \"],[\"how clean\",[[\"b\",null],[\"i\",null]]],[\" that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance.\\n\"],[\"\\nTable 2: Factors Affecting LLM Performance in Long Contexts\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580acad63f4eef3953f6b\":{\"id\":\"21740d70fdf580acad63f4eef3953f6b\",\"children\":[\"21740d70fdf580cbbd66d2a097e2ea7c\",\"21740d70fdf580b58848d3af96845184\",\"21740d70fdf580e28903da195f82d8ec\",\"21740d70fdf580d6929fcad27d2c4a26\",\"21740d70fdf580598b35dc40007ce750\"],\"hasContent\":true,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"table\",\"colOrder\":[\"CVMg\",\"xXUs\",\"MdJE\",\"sWuZ\",\";h~{\",\"TSJy\"],\"items\":[{\"id\":\"21740d70-fdf5-80cb-bd66-d2a097e2ea7c\",\"properties\":{\";h~{\":[[\"Mitigation Strategies\",[[\"b\",null]]]],\"CVMg\":[[\"Factor\",[[\"b\",null]]]],\"MdJE\":[[\"Impact on Multi-Step Inference\",[[\"b\",null]]]],\"TSJy\":[[\"Relevant Snippets\",[[\"b\",null]]]],\"sWuZ\":[[\"Interaction with Chunk Order\",[[\"b\",null]]]],\"xXUs\":[[\"Description\",[[\"b\",null]]]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80b5-8848-d3af96845184\",\"properties\":{\";h~{\":[[\"Improve retrieval quality, LLM robustness to distraction, avoid simple rearrangement.\"]],\"CVMg\":[[\"Positional Bias\",[[\"b\",null]]]],\"MdJE\":[[\"Can cause LLMs to ignore relevant info or be misled by distractors in middle positions.\"]],\"TSJy\":[[\"25\"]],\"sWuZ\":[[\"Reordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.\"]],\"xXUs\":[[\"LLMs weigh information differently based on its position (e.g., \\\"lost-in-the-middle\\\" effect).\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80e2-8903-da195f82d8ec\",\"properties\":{\";h~{\":[[\"Robust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.\"]],\"CVMg\":[[\"Irrelevant/Distracting Information\",[[\"b\",null]]]],\"MdJE\":[[\"Significantly degrades accuracy, even when relevant info is present; can derail reasoning chain.\"]],\"TSJy\":[[\"14\"]],\"sWuZ\":[[\"Strong retrievers can inadvertently bring more harmful distractors to top ranks.\"]],\"xXUs\":[[\"Passages that are semantically similar but do not contain the answer or mislead the LLM.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80d6-929f-cad27d2c4a26\",\"properties\":{\";h~{\":[[\"Supplying only relevant context, precise chunking, adaptive streaming, efficient filtering.\"]],\"CVMg\":[[\"Cognitive Load/Context Window Overload\",[[\"b\",null]]]],\"MdJE\":[[\"Reduces selection accuracy, increases hallucinations, hinders efficient reasoning.\"]],\"TSJy\":[[\"17\"]],\"sWuZ\":[[\"Too many chunks (even if somewhat relevant) can overwhelm the model.\"]],\"xXUs\":[[\"LLM struggles to process excessive or noisy information within its limited context.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-8059-8b35-dc40007ce750\",\"properties\":{\";h~{\":[[\"Preserving original document structure (DOS RAG) or logical flow.\"]],\"CVMg\":[[\"Lack of Narrative Continuity\",[[\"b\",null]]]],\"MdJE\":[[\"Impairs sequential reasoning, makes it harder for LLM to build a coherent understanding.\"]],\"TSJy\":[[\"17\"]],\"sWuZ\":[[\"Direct result of relevance-only sorting; addressed by DOS RAG.\"]],\"xXUs\":[[\"Disjointed or shuffled presentation of information.\"]]},\"color\":\"color-default\"}]},\"21740d70fdf580e4a606e6f4424b31e5\":{\"id\":\"21740d70fdf580e4a606e6f4424b31e5\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"5. Empirical Evidence and Performance Analysis\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf58093a210cb9d25926543\":{\"id\":\"21740d70fdf58093a210cb9d25926543\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Empirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580ad845cdef42ba8a4e7\":{\"id\":\"21740d70fdf580ad845cdef42ba8a4e7\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"5.1 Case Studies on Chunk Order and Multi-Step Question Answering\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580948a38d453a10fe70d\":{\"id\":\"21740d70fdf580948a38d453a10fe70d\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Comparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including Bench, QuALITY, and NarrativeQA.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" For example, on the Bench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs' multi-step reasoning capabilities are profoundly tied to the \\n\"],[\"narrative and structural coherence\",[[\"i\",null]]],[\" of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.\"]],\"updatedAt\":1750303839448,\"type\":\"text\"},\"21740d70fdf58044bc06dbd3566e29b3\":{\"id\":\"21740d70fdf58044bc06dbd3566e29b3\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Furthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\"],[\"\",[[\"b\",null],[\"e\",\"^{20}\"]]],[\" However, there is a \\\"sweet spot\\\" for context length: DOS RAG's performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]],[\" This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580c58320e68a6859b88f\":{\"id\":\"21740d70fdf580c58320e68a6859b88f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Research on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.\"],[\"\",[[\"b\",null],[\"e\",\"^{18}\"]]],[\" In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in \"],[\"correctness\",[[\"c\",null]]],[\".\"],[\"\",[[\"b\",null],[\"e\",\"^{24}\"]]],[\" When rerankers like BGE-M3 are combined with higher \"],[\"retrieval@k\",[[\"c\",null]]],[\" values, more \\\"gold documents\\\" (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.\"],[\"\",[[\"b\",null],[\"e\",\"^{24}\"]]],[\" However, increasing \"],[\"rerank@k\",[[\"c\",null]]],[\" with a fixed \"],[\"retrieval@k\",[[\"c\",null]]],[\" can introduce higher variation in \"],[\"correctness\",[[\"c\",null]]],[\" scores, ranging from 1% to 25%.\"],[\"\",[[\"b\",null],[\"e\",\"^{24}\"]]]],\"updatedAt\":1750303661454,\"type\":\"text\"},\"21740d70fdf580889f7bc0535e95d69a\":{\"id\":\"21740d70fdf580889f7bc0535e95d69a\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"5.2 Evaluation Benchmarks and Metrics\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf5806baa0cd10113df6c6c\":{\"id\":\"21740d70fdf5806baa0cd10113df6c6c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]],[\" This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.\"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580cf802debf13bd62d5a\":{\"id\":\"21740d70fdf580cf802debf13bd62d5a\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Another important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.\"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\" Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.\"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\" DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.\"],[\"\",[[\"b\",null],[\"e\",\"^{15}\"]]],[\" For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval \"],[\"accuracy\",[[\"c\",null]]],[\".\"],[\"\",[[\"b\",null],[\"e\",\"^{35}\"]]],[\" Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.\"],[\"\",[[\"b\",null],[\"e\",\"^{36}\"]]]],\"updatedAt\":1750303761739,\"type\":\"text\"},\"21740d70fdf580d5978af3c7e30f6305\":{\"id\":\"21740d70fdf580d5978af3c7e30f6305\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"These specialized benchmarks employ a variety of evaluation metrics. \"],[\"Accuracy\",[[\"c\",null]]],[\" is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.\"],[\"\",[[\"b\",null],[\"e\",\"^{14}\"]]],[\" For more nuanced assessments, metrics like \"],[\"F1\",[[\"c\",null]]],[\", \"],[\"BLEU-1\",[[\"c\",null]]],[\", \"],[\"BLEU-4\",[[\"c\",null]]],[\", \"],[\"ROUGE-L\",[[\"c\",null]]],[\", and \"],[\"METEOR\",[[\"c\",null]]],[\" are employed, particularly for tasks like NarrativeQA.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]],[\" In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.\"],[\"\",[[\"b\",null],[\"e\",\"^{16}\"]]],[\" The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks \"],[\"\",[[\"b\",null],[\"e\",\"^{10}\"]]],[\", step-by-step procedure following \"],[\"\",[[\"b\",null],[\"e\",\"^{11}\"]]],[\", and multi-hop questions \"],[\"\",[[\"b\",null],[\"e\",\"^{15}\"]]],[\", demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like \\\"\"],[\"correctness\",[[\"c\",null]]],[\"\\\" and \\\"\"],[\"faithfulness\",[[\"c\",null]]],[\"\\\" further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.\\n\"]],\"updatedAt\":1750303889536,\"type\":\"text\"},\"21740d70fdf580c2b190c8654f824624\":{\"id\":\"21740d70fdf580c2b190c8654f824624\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"\\nTable 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58090ae04ef67d7ee79bd\":{\"id\":\"21740d70fdf58090ae04ef67d7ee79bd\",\"children\":[\"21740d70fdf580c79fdcd8f4809988c3\",\"21740d70fdf580afac80ed2acd69217b\",\"21740d70fdf58054a514cc91e8cb3ec0\",\"21740d70fdf580ffa82cda40a0c79cfd\",\"21740d70fdf580b0b60ef81f2bec4b6b\",\"21740d70fdf580fda60dd6416e47442a\",\"21740d70fdf580ef9223e24767ecd4dc\",\"21740d70fdf580bfa7dcd61c06a605c9\",\"21740d70fdf5801a8113e4f7fb6fa68d\"],\"hasContent\":true,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":null,\"updatedAt\":1750303136567,\"type\":\"table\",\"colOrder\":[\"@KyG\",\"`hHL\",\"cE^M\",\"uaj?\",\"JygU\"],\"items\":[{\"id\":\"21740d70-fdf5-80c7-9fdc-d8f4809988c3\",\"properties\":{\"@KyG\":[[\"Benchmark\",[[\"b\",null]]]],\"JygU\":[[\"Relevant Snippets\",[[\"b\",null]]]],\"`hHL\":[[\"Primary Focus\",[[\"b\",null]]]],\"cE^M\":[[\"Key Features Relevant to Chunk Order/Multi-Step QA\",[[\"b\",null]]]],\"uaj?\":[[\"Key Findings Related to Chunk Order\",[[\"b\",null]]]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80af-ac80-ed2acd69217b\",\"properties\":{\"@KyG\":[[\"MTI Bench\",[[\"b\",null]]]],\"JygU\":[[\"10\"]],\"`hHL\":[[\"Multi-Task Inference (sequential \u0026 non-sequential sub-tasks)\"]],\"cE^M\":[[\"Evaluates LLMs' ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.\"]],\"uaj?\":[[\"Stronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-8054-a514-cc91e8cb3ec0\",\"properties\":{\"@KyG\":[[\"ProcBench\",[[\"b\",null]]]],\"JygU\":[[\"11\"]],\"`hHL\":[[\"Multi-Step Reasoning \u0026 Procedure Following\"]],\"cE^M\":[[\"Dataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.\"]],\"uaj?\":[[\"Highlights critical gap in current assessments focusing exclusively on multi-step inference.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80ff-a82c-da40a0c79cfd\",\"properties\":{\"@KyG\":[[\"Bench\",[[\"b\",null]]]],\"JygU\":[[\"19\"]],\"`hHL\":[[\"Long-Context Question Answering\"]],\"cE^M\":[[\"Evaluates performance under varying retrieval token budgets (1.5K to 40K tokens).\"]],\"uaj?\":[[\"DOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80b0-b60e-f81f2bec4b6b\",\"properties\":{\"@KyG\":[[\"QuALITY\",[[\"b\",null]]]],\"JygU\":[[\"19\"]],\"`hHL\":[[\"Long-Context Question Answering (narrative understanding)\"]],\"cE^M\":[[\"Requires understanding underlying narrative rather than shallow pattern matching.\"]],\"uaj?\":[[\"Full-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80fd-a60d-d6416e47442a\",\"properties\":{\"@KyG\":[[\"NarrativeQA\",[[\"b\",null]]]],\"JygU\":[[\"19\"]],\"`hHL\":[[\"Long-Context Question Answering (narrative understanding)\"]],\"cE^M\":[[\"Questions require understanding the underlying narrative.\"]],\"uaj?\":[[\"DOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80ef-9223-e24767ecd4dc\",\"properties\":{\"@KyG\":[[\"DataMorgana\",[[\"b\",null]]]],\"JygU\":[[\"15\"]],\"`hHL\":[[\"QA-pair Generation (single-hop \u0026 multi-hop)\"]],\"cE^M\":[[\"Creates highly customizable synthetic benchmarks; used in LiveRAG Challenge.\"]],\"uaj?\":[[\"Used to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-80bf-a7dc-d61c06a605c9\",\"properties\":{\"@KyG\":[[\"mmRAG\",[[\"b\",null]]]],\"JygU\":[[\"35\"]],\"`hHL\":[[\"Multi-modal RAG Evaluation\"]],\"cE^M\":[[\"Modular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.\"]],\"uaj?\":[[\"Provides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.\"]]},\"color\":\"color-default\"},{\"id\":\"21740d70-fdf5-801a-8113-e4f7fb6fa68d\",\"properties\":{\"@KyG\":[[\"ChunkRAG\",[[\"b\",null]]]],\"JygU\":[[\"14\"]],\"`hHL\":[[\"LLM-driven Chunk Filtering\"]],\"cE^M\":[[\"Enhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.\"]],\"uaj?\":[[\"Outperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.\"]]},\"color\":\"color-default\"}]},\"21740d70fdf580e0a67ddf16d23ac95b\":{\"id\":\"21740d70fdf580e0a67ddf16d23ac95b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf580bdab4cf02a41144b18\":{\"id\":\"21740d70fdf580bdab4cf02a41144b18\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Translating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58085858fd4791e4f101b\":{\"id\":\"21740d70fdf58085858fd4791e4f101b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"6.1 Best Practices for Chunking and Reordering\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf58011a43df811886b1920\":{\"id\":\"21740d70fdf58011a43df811886b1920\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"To optimize chunk retrieval sequence, a primary focus must be placed on \"],[\"prioritizing contextual coherence\",[[\"b\",null]]],[\". For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.\"],[\"\",[[\"b\",null],[\"e\",\"^{13}\"]]],[\" Given its consistent outperformance across various benchmarks, \"],[\"adopting DOS RAG as a baseline\",[[\"b\",null]]],[\" is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.\"],[\"\",[[\"b\",null],[\"e\",\"^{19}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf5800bb60debab93527a57\":{\"id\":\"21740d70fdf5800bb60debab93527a57\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"While initial retrieval provides a set of relevant chunks, a \"],[\"strategic reranking\",[[\"b\",null]]],[\" step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.\"],[\"\",[[\"b\",null],[\"e\",\"^{22}\"]]],[\" Furthermore, implementing \"],[\"inverted context ordering\",[[\"b\",null]]],[\", where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.\"],[\"\",[[\"b\",null],[\"e\",\"^{16}\"]]]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58049bce1f1a47e2d8051\":{\"id\":\"21740d70fdf58049bce1f1a47e2d8051\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Optimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure \"],[\"\",[[\"b\",null],[\"e\",\"^{20}\"]]],[\", effective reranking can still improve the \\n\"],[\"selection\",[[\"i\",null]]],[\" of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).\"],[\"\",[[\"b\",null],[\"e\",\"^{16}\"]]],[\" This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580e49c36f4912bd6ac93\":{\"id\":\"21740d70fdf580e49c36f4912bd6ac93\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"6.2 Strategies for Mitigating Positional Bias and Distraction\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580e895f8f04cd3bdbc49\":{\"id\":\"21740d70fdf580e895f8f04cd3bdbc49\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"To effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing \"],[\"retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages\",[[\"b\",null]]],[\".\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, \"],[\"robust LLM fine-tuning\",[[\"b\",null]]],[\" with carefully selected \\\"hard distracting passages\\\" can significantly increase the LLM's accuracy and resilience against noise.\"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]],[\" \",[[\"b\",null]]],[\"Third, implementing \"],[\"LLM-driven chunk filtering\",[[\"b\",null]]],[\" (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.\"],[\"\",[[\"b\",null],[\"e\",\"^{14}\"]]],[\" Fourth, for complex multi-step queries, \"],[\"query rewriting or decomposition\",[[\"b\",null]]],[\" into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.\"],[\"\",[[\"b\",null],[\"e\",\"^{14}\"]]],[\" Finally, active \"],[\"context window management\",[[\"b\",null]]],[\" is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.\"],[\"\",[[\"b\",null],[\"e\",\"^{30}\"]]],[\" Identifying the \\\"sweet spot\\\" for context length, where recall is maximized without introducing excessive noise, is also paramount.\"],[\"\",[[\"b\",null],[\"e\",\"^{17}\"]]]],\"updatedAt\":1750303784927,\"type\":\"text\"},\"21740d70fdf58025a2fec1c13e30b145\":{\"id\":\"21740d70fdf58025a2fec1c13e30b145\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Mitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the \"],[\"quality and focus\",[[\"i\",null]]],[\" of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.\"],[\"\",[[\"b\",null],[\"e\",\"^{25}\"]]],[\" Simply reordering \"],[\"after\",[[\"i\",null]]],[\" retrieval is often insufficient to address these issues.\"],[\"\",[[\"b\",null],[\"e\",\"^{26}\"]]],[\" Therefore, the solution must involve proactive measures. This includes improving the \"],[\"initial retrieval\",[[\"i\",null]]],[\" to be less prone to fetching distractors, and then employing \"],[\"strong filtering\",[[\"i\",null]]],[\" (like ChunkRAG) to eliminate noise before it ever reaches the LLM's context window.\"],[\"\",[[\"b\",null],[\"e\",\"^{14}\"]]],[\" Furthermore, making the LLM itself more robust through fine-tuning with challenging examples \"],[\"\",[[\"b\",null],[\"e\",\"^{28}\"]]],[\" creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.\"]],\"updatedAt\":1750303797801,\"type\":\"text\"},\"21740d70fdf580b3bb55f18aef19ec66\":{\"id\":\"21740d70fdf580b3bb55f18aef19ec66\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"6.3 Advanced Techniques for Multi-Hop Reasoning\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"sub_header\",\"depth\":2},\"21740d70fdf580f4b7a0d4c4313da49c\":{\"id\":\"21740d70fdf580f4b7a0d4c4313da49c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Addressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, \"],[\"iterative retrieval\",[[\"b\",null]]],[\" becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.\"],[\"\",[[\"b\",null],[\"e\",\"^{16}\"]]],[\" Adaptive retrieval\",[[\"b\",null]]],[\" mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.\"],[\"\",[[\"b\",null],[\"e\",\"^{38}\"]]],[\" The integration of structured knowledge, such as \"],[\"graph-based RAG\",[[\"b\",null]]],[\" (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.\"],[\"\",[[\"b\",null],[\"e\",\"^{35,39}\"]]],[\" This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of \"],[\"prompt-based reasoning chains\",[[\"b\",null]]],[\" like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM's reasoning process step-by-step, enhancing its ability to perform complex deductions.\"],[\"\",[[\"b\",null],[\"e\",\"^6\"]]],[\" These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf58057bc2bc6c92d18067c\":{\"id\":\"21740d70fdf58057bc2bc6c92d18067c\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"7. Conclusion and Future Directions\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"heading\",\"subType\":\"header\",\"depth\":1},\"21740d70fdf580c9a5c9cbbe9a3723c1\":{\"id\":\"21740d70fdf580c9a5c9cbbe9a3723c1\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model's multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580059d5be7ecaceed316\":{\"id\":\"21740d70fdf580059d5be7ecaceed316\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"The implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580ca8329d34ae124a4ac\":{\"id\":\"21740d70fdf580ca8329d34ae124a4ac\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Future research and development should focus on several promising directions:\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580bb9f60cd579f054e8f\":{\"id\":\"21740d70fdf580bb9f60cd579f054e8f\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Adaptive Retrieval Architectures:\",[[\"b\",null]]],[\" Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.\"],[\"\",[[\"b\",null],[\"e\",\"^8\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf58072ab0fc215757eb48b\":{\"id\":\"21740d70fdf58072ab0fc215757eb48b\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Real-time Retrieval Integration:\",[[\"b\",null]]],[\" Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.\"],[\"\",[[\"b\",null],[\"e\",\"^9\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf5802cb5a5e4c8136d32b4\":{\"id\":\"21740d70fdf5802cb5a5e4c8136d32b4\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Structured Reasoning over Multi-Hop Evidence:\",[[\"b\",null]]],[\" Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.\"],[\"\",[[\"b\",null],[\"e\",\"^{6,39}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580438856f1a03608dfd0\":{\"id\":\"21740d70fdf580438856f1a03608dfd0\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Robustness to Adversarial Inputs:\",[[\"b\",null]]],[\" Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.\"],[\"\",[[\"b\",null],[\"e\",\"^{8,28}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf58012ab78f973a8a870ee\":{\"id\":\"21740d70fdf58012ab78f973a8a870ee\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Cross-Modal and Multi-Lingual RAG:\",[[\"b\",null]]],[\" Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.\"],[\"\",[[\"b\",null],[\"e\",\"^{4,35,40}\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580ba9d25c88f6cfa1e49\":{\"id\":\"21740d70fdf580ba9d25c88f6cfa1e49\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"Evaluation Methodologies:\",[[\"b\",null]]],[\" Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.\"],[\"\",[[\"b\",null],[\"e\",\"^9\"]]]],\"updatedAt\":1750303136567,\"type\":\"bulleted_list\"},\"21740d70fdf580aa9d37d06095234106\":{\"id\":\"21740d70fdf580aa9d37d06095234106\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"These future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.\"]],\"updatedAt\":1750303136567,\"type\":\"text\"},\"21740d70fdf580cbb88acbb7fa5d007e\":{\"id\":\"21740d70fdf580cbb88acbb7fa5d007e\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"IBM. (n.d.). \\n\"],[\"What Are Large Language Models (LLMs)?\",[[\"i\",null]]],[\". Retrieved from https://www.ibm.com/think/topics/large-language-models#:~:text=LLMs%20consist%20of%20multiple%20layers,specific%20parts%20of%20data%20sets.\"]],\"updatedAt\":1750304163168,\"type\":\"numbered_list\"},\"21740d70fdf580a2af76f5bd3e1d8ed2\":{\"id\":\"21740d70fdf580a2af76f5bd3e1d8ed2\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Kasneci, E., et al. (2023). \\n\"],[\"Editorial  The Use of Large Language Models in Science: Opportunities and Challenges.\",[[\"i\",null]]],[\" PMC. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC10485814/.\"]],\"updatedAt\":1750304167502,\"type\":\"numbered_list\"},\"21740d70fdf580dcba69e2d2b53e2344\":{\"id\":\"21740d70fdf580dcba69e2d2b53e2344\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"IBM. (2023, November 2). \\n\"],[\"What are Large Language Models (LLMs)?\",[[\"i\",null]]],[\". Retrieved from https://www.ibm.com/think/topics/large-language-models.\"]],\"updatedAt\":1750304175291,\"type\":\"numbered_list\"},\"21740d70fdf58009bdead36180daf5d1\":{\"id\":\"21740d70fdf58009bdead36180daf5d1\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Google Cloud. (n.d.). \\n\"],[\"What is Retrieval-Augmented Generation (RAG)?\",[[\"i\",null]]],[\". Retrieved from https://cloud.google.com/use-cases/retrieval-augmented-generation.\"]],\"updatedAt\":1750304178015,\"type\":\"numbered_list\"},\"21740d70fdf580ff9eb4c885d46d800a\":{\"id\":\"21740d70fdf580ff9eb4c885d46d800a\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Zhang, Y., et al. (2025). \\n\"],[\"Empowering LLMs with Logical Reasoning: A Comprehensive Survey\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2502.15652v2.\"]],\"updatedAt\":1750304181215,\"type\":\"numbered_list\"},\"21740d70fdf58036811defbe9753cd2e\":{\"id\":\"21740d70fdf58036811defbe9753cd2e\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Zhang, Y., et al. (2025). \\n\"],[\"Empowering LLMs with Logical Reasoning: A Comprehensive Survey\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2502.15652v3.\"]],\"updatedAt\":1750304183870,\"type\":\"numbered_list\"},\"21740d70fdf5806da33bea329cf410a0\":{\"id\":\"21740d70fdf5806da33bea329cf410a0\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"AWS. (n.d.). \\n\"],[\"What is RAG (Retrieval-Augmented Generation)?\",[[\"i\",null]]],[\". Retrieved from https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.\"]],\"updatedAt\":1750304188576,\"type\":\"numbered_list\"},\"21740d70fdf580a6be25eaf64652eaae\":{\"id\":\"21740d70fdf580a6be25eaf64652eaae\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Sharma, C. (2025). \\n\"],[\"Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2506.00054v1.\"]],\"updatedAt\":1750304191883,\"type\":\"numbered_list\"},\"21740d70fdf5804dbeeeffd1f4ae88c4\":{\"id\":\"21740d70fdf5804dbeeeffd1f4ae88c4\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Sharma, C. (2025). \\n\"],[\"Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\",[[\"i\",null]]],[\". arXiv. Retrieved from \"],[\"https://arxiv.org/abs/2506.00054\",[[\"a\",\"https://arxiv.org/abs/2506.00054\"]]],[\".\"]],\"updatedAt\":1750303136567,\"type\":\"numbered_list\"},\"21740d70fdf580259622f329546dfb2f\":{\"id\":\"21740d70fdf580259622f329546dfb2f\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Xu, K., et al. (2024). \\n\"],[\"Multi-Task Inference: Do LLMs Hold the Capability to Handle Multiple Instructions Simultaneously?\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2402.11597v2.\"]],\"updatedAt\":1750304198255,\"type\":\"numbered_list\"},\"21740d70fdf58095905ac7ad4b41ae74\":{\"id\":\"21740d70fdf58095905ac7ad4b41ae74\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"About AI. (2024, May 22). \\n\"],[\"How Well Do Large Language Models (LLMs) Actually Handle Multi-Step Reasoning?\",[[\"i\",null]]],[\". Medium. Retrieved from https://medium.com/about-ai/how-well-do-large-language-models-llms-actually-handle-multi-step-reasoning-83ce37f7fc32.\"]],\"updatedAt\":1750304201136,\"type\":\"numbered_list\"},\"21740d70fdf5807ab0f4d6304119f55e\":{\"id\":\"21740d70fdf5807ab0f4d6304119f55e\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"F22 Labs. (n.d.). \\n\"],[\"7 Chunking Strategies in RAG You Need To Know\",[[\"i\",null]]],[\". Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/#:~:text=Retrieval%20Augmented%20Generation%20(RAG)%20enhances,for%20faster%20retrieval%20and%20processing.\"]],\"updatedAt\":1750304205406,\"type\":\"numbered_list\"},\"21740d70fdf580679df9f9f2652d4f26\":{\"id\":\"21740d70fdf580679df9f9f2652d4f26\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"F22 Labs. (n.d.). \\n\"],[\"7 Chunking Strategies in RAG You Need To Know\",[[\"i\",null]]],[\". Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/.\"]],\"updatedAt\":1750304208963,\"type\":\"numbered_list\"},\"21740d70fdf580c684aaf266ad8e32a5\":{\"id\":\"21740d70fdf580c684aaf266ad8e32a5\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Mallen, E., et al. (2024). \\n\"],[\"ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2410.19572v2.\"]],\"updatedAt\":1750304212248,\"type\":\"numbered_list\"},\"21740d70fdf5807d9c4dfa847b6cef28\":{\"id\":\"21740d70fdf5807d9c4dfa847b6cef28\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Filice, S., et al. (2025). \\n\"],[\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2506.14412v1.\"]],\"updatedAt\":1750304217154,\"type\":\"numbered_list\"},\"21740d70fdf58086b271ef77f650390a\":{\"id\":\"21740d70fdf58086b271ef77f650390a\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Filice, S., et al. (2025). \\n\"],[\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",[[\"i\",null]]],[\". arXiv. Retrieved from https://www.arxiv.org/pdf/2506.14412.\"]],\"updatedAt\":1750304220844,\"type\":\"numbered_list\"},\"21740d70fdf58078b78cdffcd8fd875e\":{\"id\":\"21740d70fdf58078b78cdffcd8fd875e\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"SuperAnnotate. (n.d.). \\n\"],[\"RAG vs. Long Context LLMs\",[[\"i\",null]]],[\". Retrieved from https://www.superannotate.com/blog/rag-vs-long-context-llms.\"]],\"updatedAt\":1750304225238,\"type\":\"numbered_list\"},\"21740d70fdf58059a4e7c03e14bd6215\":{\"id\":\"21740d70fdf58059a4e7c03e14bd6215\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Liu, Y., et al. (2024). \\n\"],[\"The Impact of Premise Order on LLM Reasoning\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2402.08939v1.\"]],\"updatedAt\":1750304228584,\"type\":\"numbered_list\"},\"21740d70fdf580d08492fb9b92c328c8\":{\"id\":\"21740d70fdf580d08492fb9b92c328c8\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Cuconasu, F., et al. (2025, June 4). \\n\"],[\"Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\",[[\"i\",null]]],[\". arXiv. Retrieved from https://www.arxiv.org/pdf/2506.03989.\"]],\"updatedAt\":1750304231719,\"type\":\"numbered_list\"},\"21740d70fdf5808ab1dbe83434ff1e1f\":{\"id\":\"21740d70fdf5808ab1dbe83434ff1e1f\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Cuconasu, F., et al. (2025). \\n\"],[\"Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2506.03989v1.\"]],\"updatedAt\":1750304238112,\"type\":\"numbered_list\"},\"21740d70fdf5809ba01edf97c0736c92\":{\"id\":\"21740d70fdf5809ba01edf97c0736c92\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"DataCamp. (n.d.). \\n\"],[\"Advanced RAG Techniques\",[[\"i\",null]]],[\". Retrieved from https://www.datacamp.com/blog/rag-advanced.\"]],\"updatedAt\":1750304140577,\"type\":\"numbered_list\"},\"21740d70fdf580f999e2db1523518c77\":{\"id\":\"21740d70fdf580f999e2db1523518c77\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Analytics Vidhya. (2025, March 28). \\n\"],[\"Comprehensive Guide on Reranker for RAG\",[[\"i\",null]]],[\". Retrieved from https://www.analyticsvidhya.com/blog/2025/03/reranker-for-rag/.\"]],\"updatedAt\":1750304137952,\"type\":\"numbered_list\"},\"21740d70fdf5807ba8dbdede8f9371bd\":{\"id\":\"21740d70fdf5807ba8dbdede8f9371bd\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Galileo AI. (n.d.). \\n\"],[\"Mastering RAG: How to Select a Reranking Model\",[[\"i\",null]]],[\". Retrieved from https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model.\"]],\"updatedAt\":1750304135448,\"type\":\"numbered_list\"},\"21740d70fdf580ffbfd5e1c2b853d383\":{\"id\":\"21740d70fdf580ffbfd5e1c2b853d383\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Filice, S., et al. (2025). \\n\"],[\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2506.14412.\"]],\"updatedAt\":1750304132849,\"type\":\"numbered_list\"},\"21740d70fdf580b1aa1afd59d3f6ac19\":{\"id\":\"21740d70fdf580b1aa1afd59d3f6ac19\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Cuconasu, F., et al. (2025). \\n\"],[\"Do RAG Systems Suffer From Positional Bias?\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2505.15561v1.\"]],\"updatedAt\":1750304119265,\"type\":\"numbered_list\"},\"21740d70fdf58020b6c2f23ce562d30d\":{\"id\":\"21740d70fdf58020b6c2f23ce562d30d\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Cuconasu, F., et al. (2025). \\n\"],[\"Do RAG Systems Suffer From Positional Bias?\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/abs/2505.15561.\"]],\"updatedAt\":1750304116336,\"type\":\"numbered_list\"},\"21740d70fdf58084b30fdb94d2ea101e\":{\"id\":\"21740d70fdf58084b30fdb94d2ea101e\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Cuconasu, F., et al. (2025). \\n\"],[\"Investigate positional bias in RAG systems, including the 'lost-in-the-middle' effect and impact of distracting passages\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/pdf/2505.15561.\"]],\"updatedAt\":1750304113443,\"type\":\"numbered_list\"},\"21740d70fdf58077896bcd067145878b\":{\"id\":\"21740d70fdf58077896bcd067145878b\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Amiraz, C., et al. (2025, May 11). \\n\"],[\"Describe the 'distracting effect' of irrelevant passages in RAG and its impact on LLM accuracy\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/abs/2505.06914.\"]],\"updatedAt\":1750304109870,\"type\":\"numbered_list\"},\"21740d70fdf58073ae6dd2cc789c3e2f\":{\"id\":\"21740d70fdf58073ae6dd2cc789c3e2f\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Amiraz, C., et al. (2025). \\n\"],[\"The Distracting Effect: Understanding Irrelevant Passages in RAG\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2505.06914v1.\"]],\"updatedAt\":1750304107034,\"type\":\"numbered_list\"},\"21740d70fdf580968828d188c0c8ee1d\":{\"id\":\"21740d70fdf580968828d188c0c8ee1d\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Li, Y., et al. (2025). \\n\"],[\"RAG-MCP: Retrieval-Augmented Generation for Model Context Protocol\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2505.03275v1.\"]],\"updatedAt\":1750304101562,\"type\":\"numbered_list\"},\"21740d70fdf5807897acec3822f47bf9\":{\"id\":\"21740d70fdf5807897acec3822f47bf9\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Zhang, M., et al. (2025). \\n\"],[\"Cognitive-Aware LLM Streaming\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2504.17999v1.\"]],\"updatedAt\":1750304098635,\"type\":\"numbered_list\"},\"21740d70fdf5802486baf24d75884d9a\":{\"id\":\"21740d70fdf5802486baf24d75884d9a\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Li, Y., et al. (2025). \\n\"],[\"How does supplying only relevant context in RAG reduce cognitive load and improve LLM decision making?\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/pdf/2505.03275.\"]],\"updatedAt\":1750304095173,\"type\":\"numbered_list\"},\"21740d70fdf5809e9c01f527c2c10ef8\":{\"id\":\"21740d70fdf5809e9c01f527c2c10ef8\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Wang, Y., et al. (2024). \\n\"],[\"InfLLM: Enabling LLMs to Understand Extremely Long Sequences Without Any Fine-tuning\",[[\"i\",null]]],[\". NeurIPS. Retrieved from https://neurips.cc/virtual/2024/poster/94480.\"]],\"updatedAt\":1750304091842,\"type\":\"numbered_list\"},\"21740d70fdf5800facf1d89be0a5874c\":{\"id\":\"21740d70fdf5800facf1d89be0a5874c\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Xu, J., et al. (2025). \\n\"],[\"Long Context vs. RAG for LLMs: An Evaluation and Revisits\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2501.01880v1.\"]],\"updatedAt\":1750304089199,\"type\":\"numbered_list\"},\"21740d70fdf580c89d4df57ac8cd0fd7\":{\"id\":\"21740d70fdf580c89d4df57ac8cd0fd7\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Chen, C., et al. (2025). \\n\"],[\"mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs\",[[\"i\",null]]],[\". arXiv. Retrieved from https://arxiv.org/html/2505.11180v1.\"]],\"updatedAt\":1750304085870,\"type\":\"numbered_list\"},\"21740d70fdf58026936ddc81616067a1\":{\"id\":\"21740d70fdf58026936ddc81616067a1\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Zhang, Z., et al. (2024). \\n\"],[\"RAGChecker: A Fine-grained Evaluation Framework for Retrieval-Augmented Generation\",[[\"i\",null]]],[\". NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html.\"]],\"updatedAt\":1750304082482,\"type\":\"numbered_list\"},\"21740d70fdf580e3a8fae2409c074f7a\":{\"id\":\"21740d70fdf580e3a8fae2409c074f7a\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Diamant, N. (n.d.). \\n\"],[\"RAG_Techniques\",[[\"i\",null]]],[\". GitHub. Retrieved from https://github.com/NirDiamant/RAG_Techniques.\"]],\"updatedAt\":1750304079032,\"type\":\"numbered_list\"},\"21740d70fdf580449387e309ff20589a\":{\"id\":\"21740d70fdf580449387e309ff20589a\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Mallen, E., et al. (2024). \\n\"],[\"Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models\",[[\"i\",null]]],[\". ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-emnlp.831/.\"]],\"updatedAt\":1750304075441,\"type\":\"numbered_list\"},\"21740d70fdf5802b9c93c4c6f0a88a9b\":{\"id\":\"21740d70fdf5802b9c93c4c6f0a88a9b\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Li, H., et al. (2024). \\n\"],[\"RAGRAPH: A General Retrieval-Augmented Graph Learning Framework\",[[\"i\",null]]],[\". NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/file/34d6c7090bc5af0b96aeaf92fa074899-Paper-Conference.pdf.\"]],\"updatedAt\":1750304071800,\"type\":\"numbered_list\"},\"21740d70fdf5800a8c63e200b143bcc2\":{\"id\":\"21740d70fdf5800a8c63e200b143bcc2\",\"children\":[],\"hasContent\":false,\"parentId\":\"21740d70fdf580388756d009905ff072\",\"title\":[[\"Reddit. (2024, March 17). \\n\"],[\"Advanced Chunking/Retrieving Strategies for Legal Documents\",[[\"i\",null]]],[\". Retrieved from https://www.reddit.com/r/Rag/comments/1jdi4sg/advanced_chunkingretrieving_strategies_for_legal/.\"]],\"updatedAt\":1750304068235,\"type\":\"numbered_list\"},\"21740d70fdf580388756d009905ff072\":{\"id\":\"21740d70fdf580388756d009905ff072\",\"children\":[\"21740d70fdf580cbb88acbb7fa5d007e\",\"21740d70fdf580a2af76f5bd3e1d8ed2\",\"21740d70fdf580dcba69e2d2b53e2344\",\"21740d70fdf58009bdead36180daf5d1\",\"21740d70fdf580ff9eb4c885d46d800a\",\"21740d70fdf58036811defbe9753cd2e\",\"21740d70fdf5806da33bea329cf410a0\",\"21740d70fdf580a6be25eaf64652eaae\",\"21740d70fdf5804dbeeeffd1f4ae88c4\",\"21740d70fdf580259622f329546dfb2f\",\"21740d70fdf58095905ac7ad4b41ae74\",\"21740d70fdf5807ab0f4d6304119f55e\",\"21740d70fdf580679df9f9f2652d4f26\",\"21740d70fdf580c684aaf266ad8e32a5\",\"21740d70fdf5807d9c4dfa847b6cef28\",\"21740d70fdf58086b271ef77f650390a\",\"21740d70fdf58078b78cdffcd8fd875e\",\"21740d70fdf58059a4e7c03e14bd6215\",\"21740d70fdf580d08492fb9b92c328c8\",\"21740d70fdf5808ab1dbe83434ff1e1f\",\"21740d70fdf5809ba01edf97c0736c92\",\"21740d70fdf580f999e2db1523518c77\",\"21740d70fdf5807ba8dbdede8f9371bd\",\"21740d70fdf580ffbfd5e1c2b853d383\",\"21740d70fdf580b1aa1afd59d3f6ac19\",\"21740d70fdf58020b6c2f23ce562d30d\",\"21740d70fdf58084b30fdb94d2ea101e\",\"21740d70fdf58077896bcd067145878b\",\"21740d70fdf58073ae6dd2cc789c3e2f\",\"21740d70fdf580968828d188c0c8ee1d\",\"21740d70fdf5807897acec3822f47bf9\",\"21740d70fdf5802486baf24d75884d9a\",\"21740d70fdf5809e9c01f527c2c10ef8\",\"21740d70fdf5800facf1d89be0a5874c\",\"21740d70fdf580c89d4df57ac8cd0fd7\",\"21740d70fdf58026936ddc81616067a1\",\"21740d70fdf580e3a8fae2409c074f7a\",\"21740d70fdf580449387e309ff20589a\",\"21740d70fdf5802b9c93c4c6f0a88a9b\",\"21740d70fdf5800a8c63e200b143bcc2\"],\"hasContent\":true,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[[\"References\",[[\"b\",null]]]],\"updatedAt\":1750303136567,\"type\":\"toggle\",\"subType\":\"header\",\"depth\":1,\"color\":null},\"21740d70fdf580fe8ae4f3d4e38c4ec2\":{\"id\":\"21740d70fdf580fe8ae4f3d4e38c4ec2\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":[],\"updatedAt\":1750303136567,\"type\":\"text\"},\"blog\":{\"id\":\"blog\",\"children\":[],\"hasContent\":true,\"parentId\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"title\":[[\"Blog\"]],\"updatedAt\":1770353114441,\"type\":\"page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"createdTime\":1749695822906,\"lastEditedTime\":1770353114441,\"createdBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\"}]}],\"lastEditedBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\"}]}],\"description\":null,\"uri\":\"/blog\",\"blockId\":\"21040d70-fdf5-806a-ad08-fe42515fe790\"},\"8d6dfeef4c7f4d678b4899d2198877cb\":{\"id\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"children\":[],\"hasContent\":true,\"parentId\":\"2cd40d70fdf5808e95fddeb3724556df\",\"title\":[[\"Hi there!\"]],\"updatedAt\":1770353114441,\"type\":\"page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"coverPosition\":1,\"createdTime\":1650079140000,\"lastEditedTime\":1770353114441,\"createdBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\"}]}],\"lastEditedBy\":[{\"0\":\"\",\"1\":[{\"0\":\"u\"}]}],\"description\":null,\"uri\":\"/8d6dfeef4c7f4d678b4899d2198877cb\",\"blockId\":\"8d6dfeef-4c7f-4d67-8b48-99d2198877cb\"},\"blog-list\":{\"id\":\"blog-list\",\"children\":[],\"hasContent\":false,\"parentId\":\"blog\",\"title\":[[\"List\"]],\"updatedAt\":1770351001734,\"type\":\"collection_page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"uri\":\"/blog/list\",\"collectionId\":\"21040d70-fdf5-80fb-9cbb-000b32f903d7\",\"views\":[],\"cover\":null,\"description\":[],\"coverPosition\":1},\"21740d70fdf580e1a5b0d2ac98b3c690\":null,\"21740d70fdf5805080a5c3692e5eeb09\":null,\"21740d70fdf5807ca905e19a618e0a48\":null,\"21740d70fdf5804da0c1f4d6f02bbd23\":null,\"21740d70fdf580338f32d47185f32232\":null,\"21740d70fdf5805b9621fff137881fa9\":null,\"21740d70fdf580f59c37e3f38a6cc678\":null,\"21740d70fdf580cbbd66d2a097e2ea7c\":null,\"21740d70fdf580b58848d3af96845184\":null,\"21740d70fdf580e28903da195f82d8ec\":null,\"21740d70fdf580d6929fcad27d2c4a26\":null,\"21740d70fdf580598b35dc40007ce750\":null,\"21740d70fdf580c79fdcd8f4809988c3\":null,\"21740d70fdf580afac80ed2acd69217b\":null,\"21740d70fdf58054a514cc91e8cb3ec0\":null,\"21740d70fdf580ffa82cda40a0c79cfd\":null,\"21740d70fdf580b0b60ef81f2bec4b6b\":null,\"21740d70fdf580fda60dd6416e47442a\":null,\"21740d70fdf580ef9223e24767ecd4dc\":null,\"21740d70fdf580bfa7dcd61c06a605c9\":null,\"21740d70fdf5801a8113e4f7fb6fa68d\":null},\"form_question\":{},\"entity\":{\"cd4a93f4-929c-4ceb-b0ff-87f473045452\":{\"id\":\"cd4a93f4-929c-4ceb-b0ff-87f473045452\",\"name\":\"Jinkun Chen\"}},\"pageId\":\"21740d70fdf580f281fbe08a7014ce1d\",\"pagesToCreate\":[]},\"settings\":{\"siteId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb\",\"userId\":\"ef6c16b7-79e4-44d2-99bf-fe8a479479b5\",\"domainName\":\"jinkunchen.com\",\"name\":\"Jinkun Chen\",\"active\":true,\"free\":false,\"tier\":\"personal\",\"favicon\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\",\"fontFamily\":\"Inter\",\"legacyTheme\":null,\"language\":\"en\",\"languages\":[],\"notionPage\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"indexPageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"pageProperties\":true,\"pageSyncing\":true,\"viewSwitcher\":true,\"layoutSidePanel\":true,\"siteSearch\":true,\"advancedSearch\":false,\"themeToggle\":false,\"mondayFirst\":false,\"loadLimits\":true,\"hidePersonInfo\":false,\"showPropertyIcons\":true,\"filterRedirectsFromSearch\":false,\"googleAnalyticsId\":\"G-QJVQ934892\",\"redirectSubdomain\":null,\"noIndex\":false,\"viewsExceeded\":false,\"cacheTTL\":0,\"manualSync\":false,\"syncStatus\":\"unsynced\",\"publishedAt\":null,\"custom404Enabled\":false,\"navbar\":{\"type\":\"simple\",\"style\":{\"height\":56,\"textColor\":\"var(--color-text-default)\",\"backgroundColor\":\"var(--color-bg-default)\",\"ctaTextColor\":\"var(--color-text-default)\",\"ctaBackgroundColor\":\"var(--color-bg-default)\",\"isSticky\":true,\"shadow\":{\"type\":\"none\",\"color\":\"rgba(0,0,0,1)\",\"opacity\":12}},\"links\":[{\"id\":\"13abc3b9-9aea-4041-bc24-1911d867cc9b\",\"type\":\"page\",\"label\":\"Home\",\"link\":\"/\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"list\":[]},{\"id\":\"767070b4-5ee7-425f-b4cb-af9097fd0d1c\",\"type\":\"page\",\"label\":\"News\",\"link\":\"/news\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::a258b05f-4680-4449-94d7-c6cef822f3a8\",\"icon\":\"\",\"list\":[]},{\"id\":\"b9351034-5c9f-4d8b-8612-0dd720c4e549\",\"type\":\"page\",\"label\":\"Publications\",\"link\":\"/publications\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::5d6544b2-7666-4fec-b572-7efb772d6c72\",\"list\":[]},{\"id\":\"528edeca-f90b-43b5-bd30-20fb90d1d3ff\",\"type\":\"page\",\"label\":\"Works\",\"link\":\"/works\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::493262ef-7a89-4b2a-af4a-afbefbd76c62\",\"list\":[]},{\"id\":\"aac48447-4e23-460e-a73d-2f63407c84a5\",\"type\":\"list\",\"label\":\"More\",\"icon\":\"ChevronDown\",\"list\":[{\"id\":\"bc44ad1a-6eb5-4955-b3b7-07ca6b4961a5\",\"type\":\"page\",\"label\":\"Blog\",\"link\":\"/blog\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::48ab9899-7e57-4426-8a4a-456ef42960e3\",\"icon\":\"\",\"list\":[]},{\"id\":\"f04ec711-4e1a-4761-965c-67fa982f7e67\",\"type\":\"page\",\"label\":\"Teaching\",\"link\":\"/teaching\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::6dc1e535-0a32-43a0-baf9-5c35aeb7f3f9\",\"list\":[]},{\"id\":\"af007f7f-e443-4c6e-becf-b33b303b1847\",\"type\":\"page\",\"label\":\"BIO\",\"link\":\"/bio\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::b6059c83-f5b7-4cb0-93c8-fd2169925d49\",\"icon\":\"\",\"description\":\"\",\"list\":[]},{\"id\":\"535c171b-da61-44b4-9729-e5e13908b47e\",\"type\":\"page\",\"label\":\"Notice\",\"link\":\"/notice\",\"pageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::2b1b7936-a021-4100-a611-0a657d643aa4\",\"icon\":\"\",\"description\":\"\",\"list\":[]}]}],\"logo\":{\"type\":\"text\",\"width\":90,\"fontSize\":16,\"textContent\":\"Jinkun Chen.\",\"imageContent\":null,\"imageContentDark\":null,\"disabled\":false},\"cta\":{\"type\":\"page\",\"label\":\"\",\"link\":\"/\",\"pageId\":\"\"},\"isSticky\":false,\"breadcrumbs\":true},\"footer\":{\"type\":\"stack\",\"style\":{\"textColor\":\"var(--color-text-default)\",\"backgroundColor\":\"var(--color-bg-default)\"},\"links\":[],\"logo\":{\"type\":\"image\",\"width\":180,\"fontSize\":16,\"textContent\":\"Jinkun Chen\",\"imageContent\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/logo/712f74e3-00ca-453b-9511-39896485699f.png\",\"imageContentDark\":null,\"disabled\":false},\"socials\":[{\"type\":\"github\",\"link\":\"https://github.com/Jinnkunn\"},{\"type\":\"linkedin\",\"link\":\"https://www.linkedin.com/in/jinkun-chen/\"},{\"type\":\"twitter\",\"link\":\"https://twitter.com/_jinnkunn\"},{\"type\":\"discord\",\"link\":\"https://discord.com/users/_jinnkunn\"}],\"footnote\":\"\",\"divider\":false},\"sidebar\":{\"enabled\":false,\"links\":[],\"cta\":[],\"logo\":null,\"searchEnabled\":true},\"theme\":{\"colorMode\":\"light\",\"layout\":{\"paddingLayout\":0.6,\"layoutMaxWidth\":900,\"columnSpacing\":46,\"borderThicknessLayout\":1,\"borderTypeLayout\":\"solid\",\"borderRadiiLayout\":5,\"pageDisplay\":\"none\"},\"header\":{\"coverHeight\":30,\"titleAlign\":\"left\",\"iconAlign\":\"-112px auto auto auto\",\"display\":\"block\"},\"collectionCard\":{\"gap\":10,\"shadow\":\"rgba(15, 15, 15, 0.1) 0px 0px 0px 1px, rgba(15, 15, 15, 0.1) 0px 2px 4px\",\"coverHeightLarge\":200,\"coverHeightMedium\":200,\"coverHeightSmall\":128,\"titleSize\":0.875,\"coverSizeSmall\":172,\"coverSizeMedium\":260,\"coverSizeLarge\":320,\"iconDisplay\":\"inline-flex\"},\"callout\":{\"iconDisplay\":\"block\",\"shadow\":\"none\"},\"typography\":{\"primaryFont\":\"Noto Sans\",\"secondaryFont\":\"Noto Sans\",\"fonts\":[],\"baseSize\":16,\"titleSize\":2.5,\"headingSize\":1,\"quoteSize\":1.2,\"quoteSizeLarge\":1.4,\"textWeight\":400,\"headingWeight\":600},\"scrollbar\":{\"width\":15},\"navbar\":{\"height\":56,\"shadow\":\"none\"},\"footer\":{\"height\":0},\"sidebar\":{\"width\":280,\"shadow\":\"none\"},\"colors\":{\"light\":{\"text\":\"#37352F\",\"textLight\":\"#7d7c78\",\"background\":\"#ffffff\",\"borderColor\":\"#E9E9E7\",\"checkboxBackground\":\"#2EAADC\",\"hoverBackground\":\"#efefef\",\"scrollbar\":{\"background\":\"#FAFAFA\",\"handle\":\"#C1C1C1\",\"border\":\"#E8E8E8\"},\"navbar\":{\"text\":\"#37352F\",\"background\":\"#ffffff\",\"buttonText\":\"#ffffff\",\"buttonBackground\":\"#37352F\"},\"footer\":{\"text\":\"#37352F\",\"background\":\"#ffffff\",\"buttonText\":\"#ffffff\",\"buttonBackground\":\"#37352F\"},\"sidebar\":{\"text\":\"#37352F\",\"ctaText\":\"#37352F\",\"background\":\"#ffffff\",\"hoverBackground\":\"#efefef\",\"ctaBackground\":\"#ffffff\",\"border\":\"#E9E9E7\"},\"database\":{\"cardBackground\":\"#ffffff\",\"cardHoverBackground\":\"#f9f9f8\",\"calendarWeekendBackground\":\"#f7f6f3\"},\"default\":[45,8,20],\"gray\":[36,2,46],\"brown\":[19,31,47],\"orange\":[30,87,45],\"yellow\":[38,62,49],\"green\":[149,31,39],\"blue\":[202,53,43],\"purple\":[274,32,54],\"pink\":[328,48,53],\"red\":[2,62,55],\"form\":{\"submitButton\":{\"default\":\"#55534E\",\"gray\":\"#A7A39A\",\"brown\":\"#9A6851\",\"orange\":\"#D9730D\",\"yellow\":\"#CA922F\",\"green\":\"#448361\",\"blue\":\"#327DA9\",\"purple\":\"#8F64AF\",\"pink\":\"#C24C8B\",\"red\":\"#D44E49\"}}},\"dark\":{\"text\":\"#e1e1e1\",\"textLight\":\"#9b9b9b\",\"background\":\"#191919\",\"borderColor\":\"#373737\",\"checkboxBackground\":\"#2EAADC\",\"hoverBackground\":\"#262626\",\"scrollbar\":{\"background\":\"#FAFAFA\",\"handle\":\"#C1C1C1\",\"border\":\"#E8E8E8\"},\"navbar\":{\"text\":\"#e1e1e1\",\"background\":\"#191919\",\"buttonText\":\"#191919\",\"buttonBackground\":\"#e1e1e1\"},\"footer\":{\"text\":\"#e1e1e1\",\"background\":\"#191919\",\"buttonText\":\"#191919\",\"buttonBackground\":\"#e1e1e1\"},\"sidebar\":{\"text\":\"#e1e1e1\",\"ctaText\":\"#e1e1e1\",\"background\":\"#191919\",\"hoverBackground\":\"#292929\",\"ctaBackground\":\"#191919\",\"border\":\"#373737\"},\"database\":{\"cardBackground\":\"#262626\",\"cardHoverBackground\":\"#2f2f2f\",\"calendarWeekendBackground\":\"#202020\"},\"form\":{\"submitButton\":{\"default\":\"#55534E\",\"gray\":\"#A7A39A\",\"brown\":\"#9A6851\",\"orange\":\"#D9730D\",\"yellow\":\"#CA922F\",\"green\":\"#448361\",\"blue\":\"#327DA9\",\"purple\":\"#8F64AF\",\"pink\":\"#C24C8B\",\"red\":\"#D44E49\"}},\"default\":[45,8,20],\"gray\":[0,0,61],\"brown\":[18,35,58],\"orange\":[25,54,53],\"yellow\":[38,54,54],\"green\":[146,32,47],\"blue\":[217,50,58],\"purple\":[270,55,62],\"pink\":[329,57,58],\"red\":[1,69,60]}}},\"code\":{\"head\":[],\"body\":[],\"style\":[],\"css\":\"\"},\"files\":[],\"hasPassword\":false,\"hasFeed\":false},\"smallText\":false,\"config\":\"$undefined\",\"pageLocation\":\"/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"passwords\":[],\"styles\":\"\",\"children\":[\"$L21\",\"$L22\"]}]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"title\",\"0\",{\"children\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Why the order of retrieved information can quietly change how AI reasons step by step\"}],[\"$\",\"meta\",\"2\",{\"name\":\"generator\",\"content\":\"Super\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Why the order of retrieved information can quietly change how AI reasons step by step\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://jinkunchen.com/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:description\",\"content\":\"Why the order of retrieved information can quietly change how AI reasons step by step\"}]]\n"])</script><script>self.__next_f.push([1,"23:I[479520,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"\"]\n24:I[920494,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"ThemeProvider\"]\n2a:I[678415,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"NavigationEvents\"]\n2b:I[982642,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"LinkUpgrader\"]\n2c:I[245155,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"LightboxEnhancer\"]\n2d:I[957215,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"SpeedInsights\"]\n2e:I[130020,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"NavbarBreadcrumbs\"]\n2f:I[618378,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"PageRoot\"]\n93:I[10042,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"PageDetails\"]\n1c:[[\"$\",\"$L23\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-QJVQ934892\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$L23\",null,{\"id\":\"google-analytics\",\"strategy\":\"afterInteractive\",\"children\":\"\\n          window.dataLayer = window.dataLayer || [];\\n          function gtag(){dataLayer.push(arguments);}\\n          gtag('js', new Date());\\n          gtag('config', 'G-QJVQ934892');\\n        \"}]]\n25:T16f3,"])</script><script>self.__next_f.push([1,"\n    :root {\n      /* Layout */\n      --padding-layout: 0.6rem;\n      --border-radii-layout: 5px;\n      --border-thickness-layout: 1px;\n      --border-type-layout: solid;\n      --border-layout: var(--border-thickness-layout)\n        var(--border-type-layout) var(--color-border-default);\n      --layout-max-width: 900px;\n      --column-spacing: 46px;\n      --page-display: none;\n      --padding-right: calc(env(safe-area-inset-right) + 96px);\n      --padding-left: calc(env(safe-area-inset-left) + 96px);\n      --padding-right-mobile: calc(env(safe-area-inset-right) + 24px);\n      --padding-left-mobile: calc(env(safe-area-inset-left) + 24px);\n      /* Header */\n      --header-cover-height: 30vh;\n      --header-title-align: start;\n      --header-icon-align: -112px auto auto auto;\n      --header-display: block;\n      /* Collection header */\n      --collection-header-border: var(--border-layout);\n      /* Collection table */\n      --collection-table-cell-padding: calc(var(--padding-layout) - 0.3rem)\n        calc(var(--padding-layout) - 0.1rem);\n      /* Collection list */\n      --collection-list-item-padding: calc(var(--padding-layout) - 0.5rem);\n      --collection-list-item-border-radii: calc(\n        var(--border-radii-layout) - 1px\n      );\n      /* Collection card */\n      --collection-card-padding: 0px;\n      --collection-card-title-padding: 0px;\n      --collection-card-content-padding: var(--padding-layout);\n      --collection-card-border-radii: var(--border-radii-layout);\n      --collection-card-gap: 10px;\n      --collection-card-shadow: rgba(15, 15, 15, 0.1) 0px 0px 0px 1px, rgba(15, 15, 15, 0.1) 0px 2px 4px;\n      --collection-card-title-size: 0.875rem;\n      --collection-card-cover-height-small: 128px;\n      --collection-card-cover-size-small: 172px;\n      --collection-card-cover-height-medium: 200px;\n      --collection-card-cover-size-medium: 260px;\n      --collection-card-cover-height-large: 200px;\n      --collection-card-cover-size-large: 320px;\n      --collection-card-icon-display: inline-flex;\n      /* Callout */\n      --callout-padding: calc(var(--padding-layout) + 0.4rem)\n        calc(var(--padding-layout) + 0.4rem)\n        calc(var(--padding-layout) + 0.4rem)\n        calc(var(--padding-layout) + 0.1em);\n      --callout-border-radii: calc(var(--border-radii-layout) - 2px);\n      --callout-border: var(--border-layout);\n      --callout-icon-display: block;\n      --callout-shadow: none;\n      /* File */\n      --file-border-radii: calc(var(--border-radii-layout) - 2px);\n      /* Equation */\n      --equation-border-radii: calc(var(--border-radii-layout) - 2px);\n      /* Divider */\n      --divider-border: var(--border-layout);\n      /* Quote */\n      --quote-border: calc(var(--border-thickness-layout) + 2px) solid\n        currentcolor;\n      /* Code */\n      --code-padding: calc(var(--padding-layout) + 1.4rem);\n      --code-border-radii: var(--border-radii-layout);\n      /* Tweet */\n      --tweet-padding: calc(var(--padding-layout) + 0.65rem)\n        calc(var(--padding-layout) + 0.65rem)\n        calc(var(--padding-layout) + 0.05rem)\n        calc(var(--padding-layout) + 0.65rem);\n      --tweet-border-radii: var(--border-radii-layout);\n      --tweet-border: var(--border-layout);\n      /* Bookmark */\n      --bookmark-padding: calc(var(--padding-layout) + 0.15rem) 0px\n        calc(var(--padding-layout) + 0.025rem)\n        calc(var(--padding-layout) + 0.275rem);\n      --bookmark-border-radii: var(--border-radii-layout);\n      --bookmark-border: var(--border-layout);\n      --bookmark-image-border-radii: 0px\n        calc(var(--border-radii-layout) - 1px)\n        calc(var(--border-radii-layout) - 1px) 0px;\n      /* Embed */\n      --embed-border-radii: calc(var(--border-radii-layout) - 5px);\n      /* Image */\n      --image-border-radii: calc(var(--border-radii-layout) - 5px);\n      /* Typography */\n      --title-size: 2.5rem;\n      --quote-size: 1.2rem;\n      --quote-size-large: 1.4rem;\n      --heading-size: 1rem;\n      --primary-font: Noto Sans, Noto Sans-fallback, Helvetica, Apple Color Emoji,\n        Segoe UI Emoji, NotoColorEmoji, Noto Color Emoji,\n        Segoe UI Symbol, Android Emoji, EmojiSymbols, -apple-system,\n        BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Noto Sans,\n        sans-serif;\n        --secondary-font: Noto Sans, Noto Sans-fallback, Helvetica, Apple Color Emoji,\n        Segoe UI Emoji, NotoColorEmoji, Noto Color Emoji,\n        Segoe UI Symbol, Android Emoji, EmojiSymbols, -apple-system,\n        BlinkMacSystemFont, Segoe UI, Roboto, Helvetica Neue, Noto Sans,\n        sans-serif;\n      --text-weight: 400;\n      --heading-weight: 600;\n      --heading1-size: calc(var(--heading-size) * 1.875);\n      --heading2-size: calc(var(--heading-size) * 1.5);\n      --heading3-size: calc(var(--heading-size) * 1.25);\n      --heading4-size: calc(var(--heading-size) * 1);\n      --heading5-size: calc(var(--heading-size) * 0.8125);\n      /* Scrollbars */\n      --scrollbar-width: 15px;\n      /* Navbar */\n      --navbar-height: 56px;\n      --navbar-shadow: none;\n      --navbar-button-border-radii: 50px;\n      --navbar-list-width-single-column: 320px;\n      --navbar-list-width: 620px;\n      /*Sidebar*/\n      --sidebar-width: 280px;\n      --sidebar-shadow: none;\n    }\n    html {\n      scroll-padding-top: 62px;\n    }\n    body {\n      font-family: var(--secondary-font);\n    }\n\n    span[class=\"highlighted-background bg-yellow\"] {\n  /*display: inline-block;*/\n  background-color: transparent;\n}\n\na[class=\"notion-link link\"] {\n  display: inline-block;\n  position: relative;\n}\n\na[class=\"notion-link link\"]:after {\n  position: absolute;\n  content: \" \";\n  top: 66%;\n  bottom: 0;\n  left: 0em;\n  right: -0.1em;\n  transition: top 800ms cubic-bezier(0, 0.8, 0.13, 1);\n  background-color: hsl(38, 152%, 80%);\n  z-index: -1;\n}\n\na[class=\"notion-link link\"]:hover:after {\n  top: 2%;\n}\n  "])</script><script>self.__next_f.push([1,"26:T5aa6,"])</script><script>self.__next_f.push([1,"\n    html.theme-light {\n      /* Gray */\n      --gray-h: 36;\n      --gray-s: 2%;\n      --gray-l: 46%;\n      --color-gray: hsl(var(--gray-h), var(--gray-s), var(--gray-l));\n      --color-text-gray: var(--color-gray);\n      --color-bg-gray: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        90%\n      );\n      --color-bg-gray-light: var(--color-bg-gray);\n      --color-pill-gray: hsl(\n        calc(var(--gray-h) + 14),\n        var(--gray-s),\n        88%\n      );\n      --color-pill-text-gray: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        calc(var(--gray-l) - 28%)\n      );\n      --color-bg-form-btn-gray: #A7A39A;\n\n      /* Brown */\n      --brown-h: 19;\n      --brown-s: 31%;\n      --brown-l: 47%;\n      --color-brown: hsl(var(--brown-h), var(--brown-s), var(--brown-l));\n      --color-text-brown: var(--color-brown);\n      --color-bg-brown: hsl(\n        var(--brown-h),\n        var(--brown-s),\n        90%\n      );\n      --color-bg-brown-light: var(--color-bg-brown);\n      --color-pill-brown: hsl(\n        var(--brown-h),\n        calc(var(--brown-s) + 10%),\n        89%\n      );\n      --color-pill-text-brown: hsl(\n        var(--brown-h),\n        calc(var(--brown-s) + 10%),\n        calc(var(--brown-l) - 28%)\n      );\n      --color-bg-form-btn-brown: #9A6851;\n\n      /* Orange */\n      --orange-h: 30;\n      --orange-s: 87%;\n      --orange-l: 45%;\n      --color-orange: hsl(\n        var(--orange-h),\n        var(--orange-s),\n        var(--orange-l)\n      );\n      --color-text-orange: var(--color-orange);\n      --color-bg-orange: hsl(\n        var(--orange-h),\n        var(--orange-s),\n        90%\n      );\n      --color-bg-orange-light: var(--color-bg-orange);\n      --color-pill-orange: hsl(\n        var(--orange-h),\n        calc(var(--orange-s) - 6%),\n        87%\n      );\n      --color-pill-text-orange: hsl(\n        calc(var(--orange-h) - 5),\n        calc(var(--orange-s) - 18%),\n        calc(var(--orange-l) - 28%)\n      );\n      --color-bg-form-btn-orange: #D9730D;\n\n      /* Yellow */\n      --yellow-h: 38;\n      --yellow-s: 62%;\n      --yellow-l: 49%;\n      --color-yellow: hsl(\n        var(--yellow-h),\n        var(--yellow-s),\n        var(--yellow-l)\n      );\n      --color-text-yellow: var(--color-yellow);\n      --color-bg-yellow: hsl(\n        var(--yellow-h),\n        calc(var(--yellow-s) + 90%),\n        90%\n      );\n      --color-bg-yellow-light: var(--color-bg-yellow);\n      --color-pill-yellow: hsl(\n        calc(var(--yellow-h) + 3),\n        calc(var(--yellow-s) + 24%),\n        90%\n      );\n      --color-pill-text-yellow: hsl(\n        calc(var(--yellow-h) - 11),\n        calc(var(--yellow-s) - 22%),\n        calc(var(--yellow-l) - 28%)\n      );\n      --color-bg-form-btn-yellow: #CA922F;\n\n      /* Green */\n      --green-h: 149;\n      --green-s: 31%;\n      --green-l: 39%;\n      --color-green: hsl(var(--green-h), var(--green-s), var(--green-l));\n      --color-text-green: var(--color-green);\n      --color-bg-green: hsl(\n        var(--green-h),\n        var(--green-s),\n        90%\n      );\n      --color-bg-green-light: var(--color-bg-green);\n      --color-pill-green: hsl(\n        calc(var(--green-h) - 28),\n        calc(var(--green-s) + 3%),\n        89%\n      );\n      --color-pill-text-green: hsl(\n        calc(var(--green-h) - 2),\n        var(--green-s),\n        calc(var(--green-l) - 22%)\n      );\n      --color-bg-form-btn-green: #448361;\n\n      /* Blue */\n      --blue-h: 202;\n      --blue-s: 53%;\n      --blue-l: 43%;\n      --color-blue: hsl(var(--blue-h), var(--blue-s), var(--blue-l));\n      --color-text-blue: var(--color-blue);\n      --color-bg-blue: hsl(\n        var(--blue-h),\n        var(--blue-s),\n        90%\n      );\n      --color-bg-blue-light: var(--color-bg-blue);\n      --color-pill-blue: hsl(\n        var(--blue-h),\n        calc(var(--blue-s) - 5%),\n        85%\n      );\n      --color-pill-text-blue: hsl(\n        calc(var(--blue-h) + 7),\n        calc(var(--blue-s) - 6%),\n        calc(var(--blue-l) - 22%)\n      );\n      --color-bg-form-btn-blue: #327DA9;\n\n      /* Purple */\n      --purple-h: 274;\n      --purple-s: 32%;\n      --purple-l: 54%;\n      --color-purple: hsl(\n        var(--purple-h),\n        var(--purple-s),\n        var(--purple-l)\n      );\n      --color-text-purple: var(--color-purple);\n      --color-bg-purple: hsl(\n        var(--purple-h),\n        var(--purple-s),\n        90%\n      );\n      --color-bg-purple-light: var(--color-bg-purple);\n      --color-pill-purple: hsl(\n        var(--purple-h),\n        calc(var(--purple-s) + 5%),\n        90%\n      );\n      --color-pill-text-purple: hsl(\n        calc(var(--purple-h) + 1),\n        calc(var(--purple-s) + 10%),\n        calc(var(--purple-l) - 31%)\n      );\n      --color-bg-form-btn-purple: #8F64AF;\n\n      /* Pink */\n      --pink-h: 328;\n      --pink-s: 48%;\n      --pink-l: 53%;\n      --color-pink: hsl(var(--pink-h), var(--pink-s), var(--pink-l));\n      --color-text-pink: var(--color-pink);\n      --color-bg-pink: hsl(\n        var(--pink-h),\n        var(--pink-s),\n        90%\n      );\n      --color-bg-pink-light: var(--color-bg-pink);\n      --color-pill-pink: hsl(\n        var(--pink-h),\n        var(--pink-s),\n        90%\n      );\n      --color-pill-text-pink: hsl(\n        calc(var(--pink-h) + 3),\n        calc(var(--pink-s) - 14%),\n        calc(var(--pink-l) - 31%)\n      );\n      --color-bg-form-btn-pink: #C24C8B;\n\n      /* Red */\n      --red-h: 2;\n      --red-s: 62%;\n      --red-l: 55%;\n      --color-red: hsl(var(--red-h), var(--red-s), var(--red-l));\n      --color-text-red: var(--color-red);\n      --color-bg-red: hsl(\n        var(--red-h),\n        var(--red-s),\n        90%\n      );\n      --color-bg-red-light: var(--color-bg-red);\n      --color-pill-red: hsl(\n        calc(var(--red-h) + 6),\n        calc(var(--red-s) + 42%),\n        90%\n      );\n      --color-pill-text-red: hsl(\n        calc(var(--red-h) + 0),\n        var(--red-s),\n        calc(var(--red-l) - 32%)\n      );\n      --color-bg-form-btn-red: #D44E49;\n\n      /*Default*/\n      --default-h: 45;\n      --default-s: 8%;\n      --default-l: 20%;\n      --color-default: hsl(\n        var(--default-h),\n        var(--default-s),\n        var(--default-l)\n      );\n      --color-pill-default: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        90%\n      );\n      --color-pill-text-default: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        calc(var(--gray-l) - 28%)\n      );\n      --color-bg-form-btn-default: #55534E;\n\n      /*Other*/\n      --color-text-default: #37352F;\n      --color-text-default-light: #7d7c78;\n      --color-bg-default: #ffffff;\n      --color-border-default: #E9E9E7;\n      --color-border-dark: var(--color-border-default);\n      --color-ui-hover-bg: #efefef;\n      --color-card-bg: #ffffff;\n      --color-card-bg-hover: #f9f9f8;\n      --color-calendar-weekend-bg: #f7f6f3;\n      --color-checkbox-bg: #2EAADC;\n      --color-code-bg: rgba(135,131,120,.15);\n\n      /*Scrollbar*/\n      --scrollbar-background-color: #FAFAFA;\n      --scrollbar-thumb-color: #C1C1C1;\n      --scrollbar-border-color: #E8E8E8;\n\n      /*Navbar*/\n      --navbar-text-color: #37352F;\n      --navbar-list-item-hover: rgba(130, 130, 130, 0.09);\n      --navbar-background-color-hover: #f0f0f0;\n      --navbar-background-color: #ffffff;\n      --navbar-button-text-color: #ffffff;\n      --navbar-button-background-color: #37352F;\n      --navbar-menu-background-color: var(--navbar-background-color);\n      --navbar-text-color-dark: #282620;\n\n      /*Footer*/\n      --footer-text-color: #37352F;\n      --footer-background-color: #ffffff;\n\n      /*Sidebar*/\n      --sidebar-text-color: #37352F;\n      --sidebar-cta-text-color: #37352F;\n      --sidebar-background-color: #ffffff;\n      --sidebar-cta-background-color: #ffffff;\n      --sidebar-border-color: #E9E9E7;\n      --sidebar-background-color-hover: #efefef;\n      --sidebar-text-color-dark: #282620;\n      --sidebar-cta-background-color-hover: #efefef;\n    }\n\n    \n  /**\n   * One Light theme for prism.js\n   * Based on Atom's One Light theme: https://github.com/atom/atom/tree/master/packages/one-light-syntax\n   */\n  \n  /**\n   * One Light colours (accurate as of commit eb064bf on 19 Feb 2021)\n   * From colors.less\n   * --mono-1: hsl(230, 8%, 24%);\n   * --mono-2: hsl(230, 6%, 44%);\n   * --mono-3: hsl(230, 4%, 64%)\n   * --hue-1: hsl(198, 99%, 37%);\n   * --hue-2: hsl(221, 87%, 60%);\n   * --hue-3: hsl(301, 63%, 40%);\n   * --hue-4: hsl(119, 34%, 47%);\n   * --hue-5: hsl(5, 74%, 59%);\n   * --hue-5-2: hsl(344, 84%, 43%);\n   * --hue-6: hsl(35, 99%, 36%);\n   * --hue-6-2: hsl(35, 99%, 40%);\n   * --syntax-fg: hsl(230, 8%, 24%);\n   * --syntax-bg: hsl(230, 1%, 98%);\n   * --syntax-gutter: hsl(230, 1%, 62%);\n   * --syntax-guide: hsla(230, 8%, 24%, 0.2);\n   * --syntax-accent: hsl(230, 100%, 66%);\n   * From syntax-variables.less\n   * --syntax-selection-color: hsl(230, 1%, 90%);\n   * --syntax-gutter-background-color-selected: hsl(230, 1%, 90%);\n   * --syntax-cursor-line: hsla(230, 8%, 24%, 0.05);\n   */\n  \n  html.theme-light code[class*=\"language-\"],\n  html.theme-light pre[class*=\"language-\"] {\n    color: hsl(230, 8%, 24%);\n    font-family: \"Fira Code\", \"Fira Mono\", Menlo, Consolas, \"DejaVu Sans Mono\", monospace;\n    direction: ltr;\n    text-align: left;\n    white-space: pre;\n    word-spacing: normal;\n    word-break: normal;\n    line-height: 1.5;\n    -moz-tab-size: 2;\n    -o-tab-size: 2;\n    tab-size: 2;\n    -webkit-hyphens: none;\n    -moz-hyphens: none;\n    -ms-hyphens: none;\n    hyphens: none;\n  }\n  \n  /* Selection */\n  html.theme-light code[class*=\"language-\"]::-moz-selection,\n  html.theme-light code[class*=\"language-\"] *::-moz-selection,\n  html.theme-light pre[class*=\"language-\"] *::-moz-selection {\n    background: hsl(230, 1%, 90%);\n    color: inherit;\n  }\n  \n  html.theme-light code[class*=\"language-\"]::selection,\n  html.theme-light code[class*=\"language-\"] *::selection,\n  html.theme-light pre[class*=\"language-\"] *::selection {\n    background: hsl(230, 1%, 90%);\n    color: inherit;\n  }\n  \n  /* Code blocks */\n  html.theme-light pre[class*=\"language-\"] {\n    padding: 1em;\n    margin: 0.5em 0;\n    overflow: auto;\n    border-radius: 0.3em;\n  }\n  \n  /* Inline code */\n  html.theme-light :not(pre) \u003e code[class*=\"language-\"] {\n    padding: 0.2em 0.3em;\n    border-radius: 0.3em;\n    white-space: normal;\n  }\n  \n  html.theme-light .token.comment,\n  html.theme-light .token.prolog,\n  html.theme-light .token.cdata {\n    color: hsl(230, 4%, 64%);\n  }\n  \n  html.theme-light .token.doctype,\n  html.theme-light .token.punctuation,\n  html.theme-light .token.entity {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .token.attr-name,\n  html.theme-light .token.class-name,\n  html.theme-light .token.boolean,\n  html.theme-light .token.constant,\n  html.theme-light .token.number,\n  html.theme-light .token.atrule {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  html.theme-light .token.keyword {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .token.property,\n  html.theme-light .token.tag,\n  html.theme-light .token.symbol,\n  html.theme-light .token.deleted,\n  html.theme-light .token.important {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .token.selector,\n  html.theme-light .token.string,\n  html.theme-light .token.char,\n  html.theme-light .token.builtin,\n  html.theme-light .token.inserted,\n  html.theme-light .token.regex,\n  html.theme-light .token.attr-value,\n  html.theme-light .token.attr-value \u003e .token.punctuation {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .token.variable,\n  html.theme-light .token.operator,\n  html.theme-light .token.function {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .token.url {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  /* HTML overrides */\n  html.theme-light .token.attr-value \u003e .token.punctuation.attr-equals,\n  html.theme-light .token.special-attr \u003e .token.attr-value \u003e .token.value.css {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  /* CSS overrides */\n  html.theme-light .language-css .token.selector {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .language-css .token.property {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-css .token.function,\n  html.theme-light .language-css .token.url \u003e .token.function {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  html.theme-light .language-css .token.url \u003e .token.string.url {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .language-css .token.important,\n  html.theme-light .language-css .token.atrule .token.rule {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  /* JS overrides */\n  html.theme-light .language-javascript .token.operator {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .language-javascript .token.template-string \u003e .token.interpolation \u003e .token.interpolation-punctuation.punctuation {\n    color: hsl(344, 84%, 43%);\n  }\n  \n  /* JSON overrides */\n  html.theme-light .language-json .token.operator {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-json .token.null.keyword {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  /* MD overrides */\n  html.theme-light .language-markdown .token.url,\n  html.theme-light .language-markdown .token.url \u003e .token.operator,\n  html.theme-light .language-markdown .token.url-reference.url \u003e .token.string {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-markdown .token.url \u003e .token.content {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .language-markdown .token.url \u003e .token.url,\n  html.theme-light .language-markdown .token.url-reference.url {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  html.theme-light .language-markdown .token.blockquote.punctuation,\n  html.theme-light .language-markdown .token.hr.punctuation {\n    color: hsl(230, 4%, 64%);\n    font-style: italic;\n  }\n  \n  html.theme-light .language-markdown .token.code-snippet {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .language-markdown .token.bold .token.content {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  html.theme-light .language-markdown .token.italic .token.content {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .language-markdown .token.strike .token.content,\n  html.theme-light .language-markdown .token.strike .token.punctuation,\n  html.theme-light .language-markdown .token.list.punctuation,\n  html.theme-light .language-markdown .token.title.important \u003e .token.punctuation {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  /* General */\n  html.theme-light .token.bold {\n    font-weight: bold;\n  }\n  \n  html.theme-light .token.comment,\n  html.theme-light .token.italic {\n    font-style: italic;\n  }\n  \n  html.theme-light .token.entity {\n    cursor: help;\n  }\n  \n  html.theme-light .token.namespace {\n    opacity: 0.8;\n  }\n  \n  /* Plugin overrides */\n  /* Selectors should have higher specificity than those in the plugins' default stylesheets */\n  \n  /* Show Invisibles plugin overrides */\n  html.theme-light .token.token.tab:not(:empty):before,\n  html.theme-light .token.token.cr:before,\n  html.theme-light .token.token.lf:before,\n  html.theme-light .token.token.space:before {\n    color: hsla(230, 8%, 24%, 0.2);\n  }\n  \n  /* Toolbar plugin overrides */\n  /* Space out all buttons and move them away from the right edge of the code block */\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item {\n    margin-right: 0.4em;\n  }\n  \n  /* Styling the buttons */\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span {\n    background: hsl(230, 1%, 90%);\n    color: hsl(230, 6%, 44%);\n    padding: 0.1em 0.4em;\n    border-radius: 0.3em;\n  }\n  \n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:focus,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:focus,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:focus {\n    background: hsl(230, 1%, 78%); /* custom: darken(--syntax-bg, 20%) */\n    color: hsl(230, 8%, 24%);\n  }\n  \n  /* Line Highlight plugin overrides */\n  /* The highlighted line itself */\n  html.theme-light .line-highlight.line-highlight {\n    background: hsla(230, 8%, 24%, 0.05);\n  }\n  \n  /* Default line numbers in Line Highlight plugin */\n  html.theme-light .line-highlight.line-highlight:before,\n  html.theme-light .line-highlight.line-highlight[data-end]:after {\n    background: hsl(230, 1%, 90%);\n    color: hsl(230, 8%, 24%);\n    padding: 0.1em 0.6em;\n    border-radius: 0.3em;\n    box-shadow: 0 2px 0 0 rgba(0, 0, 0, 0.2); /* same as Toolbar plugin default */\n  }\n  \n  /* Hovering over a linkable line number (in the gutter area) */\n  /* Requires Line Numbers plugin as well */\n  html.theme-light pre[id].linkable-line-numbers.linkable-line-numbers span.line-numbers-rows \u003e span:hover:before {\n    background-color: hsla(230, 8%, 24%, 0.05);\n  }\n  \n  /* Line Numbers and Command Line plugins overrides */\n  /* Line separating gutter from coding area */\n  html.theme-light .line-numbers.line-numbers .line-numbers-rows,\n  html.theme-light .command-line .command-line-prompt {\n    border-right-color: hsla(230, 8%, 24%, 0.2);\n  }\n  \n  /* Stuff in the gutter */\n  html.theme-light .line-numbers .line-numbers-rows \u003e span:before,\n  html.theme-light .command-line .command-line-prompt \u003e span:before {\n    color: hsl(230, 1%, 62%);\n  }\n  \n  /* Match Braces plugin overrides */\n  /* Note: Outline colour is inherited from the braces */\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-1,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-5,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-9 {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-2,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-6,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-10 {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-3,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-7,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-11 {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-4,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-8,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-12 {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  /* Diff Highlight plugin overrides */\n  /* Taken from https://github.com/atom/github/blob/master/styles/variables.less */\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix),\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) {\n    background-color: hsla(353, 100%, 66%, 0.15);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::-moz-selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix),\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) {\n    background-color: hsla(137, 100%, 55%, 0.15);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::-moz-selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n  \n  /* Previewers plugin overrides */\n  /* Based on https://github.com/atom-community/atom-ide-datatip/blob/master/styles/atom-ide-datatips.less and https://github.com/atom/atom/blob/master/packages/one-light-ui */\n  /* Border around popup */\n  html.theme-light .prism-previewer.prism-previewer:before,\n  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div {\n    border-color: hsl(0, 0, 95%);\n  }\n  \n  /* Angle and time should remain as circles and are hence not included */\n  html.theme-light .prism-previewer-color.prism-previewer-color:before,\n  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing:before {\n    border-radius: 0.3em;\n  }\n  \n  /* Triangles pointing to the code */\n  html.theme-light .prism-previewer.prism-previewer:after {\n    border-top-color: hsl(0, 0, 95%);\n  }\n  \n  html.theme-light .prism-previewer-flipped.prism-previewer-flipped.after {\n    border-bottom-color: hsl(0, 0, 95%);\n  }\n  \n  /* Background colour within the popup */\n  html.theme-light .prism-previewer-angle.prism-previewer-angle:before,\n  html.theme-light .prism-previewer-time.prism-previewer-time:before,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing {\n    background: hsl(0, 0%, 100%);\n  }\n  \n  /* For angle, this is the positive area (eg. 90deg will display one quadrant in this colour) */\n  /* For time, this is the alternate colour */\n  html.theme-light .prism-previewer-angle.prism-previewer-angle circle,\n  html.theme-light .prism-previewer-time.prism-previewer-time circle {\n    stroke: hsl(230, 8%, 24%);\n    stroke-opacity: 1;\n  }\n  \n  /* Stroke colours of the handle, direction point, and vector itself */\n  html.theme-light .prism-previewer-easing.prism-previewer-easing circle,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing path,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing line {\n    stroke: hsl(230, 8%, 24%);\n  }\n  \n  /* Fill colour of the handle */\n  html.theme-light .prism-previewer-easing.prism-previewer-easing circle {\n    fill: transparent;\n  }\n  \n  "])</script><script>self.__next_f.push([1,"27:T5f9c,"])</script><script>self.__next_f.push([1,"\n    html.theme-dark {\n      /* Gray */\n      --gray-h: 0;\n      --gray-s: 0%;\n      --gray-l: 61%;\n      --color-gray: hsl(var(--gray-h), var(--gray-s), var(--gray-l));\n      --color-text-gray: var(--color-gray);\n      --color-bg-gray: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        14.5%\n      );\n      --color-bg-gray-light: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        14.5%\n      );\n      --color-pill-gray: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        35%\n      );\n      --color-pill-text-gray: hsl(\n        var(--gray-h),\n        var(--gray-s),\n        calc(var(--gray-l) + 26%)\n      );\n      --color-bg-form-btn-gray: #A7A39A;\n\n      /* Brown */\n      --brown-h: 18;\n      --brown-s: 35%;\n      --brown-l: 58%;\n      --color-brown: hsl(var(--brown-h), var(--brown-s), var(--brown-l));\n      --color-text-brown: var(--color-brown);\n      --color-bg-brown: hsl(\n        var(--brown-h),\n        calc(var(--brown-s) - 7%),\n        22%\n      );\n      --color-bg-brown-light: hsl(\n        calc(var(--brown-h) - 2),\n        calc(var(--brown-s) - 22%),\n        16%\n      );\n      --color-pill-brown: hsl(\n        var(--brown-h),\n        calc(var(--brown-s) + 2%),\n        27%\n      );\n      --color-pill-text-brown: hsl(\n        var(--brown-h),\n        calc(var(--brown-s) - 21%),\n        calc(var(--brown-l) + 33%)\n      );\n      --color-bg-form-btn-brown: #9A6851;\n\n      /* Orange */\n      --orange-h: 25;\n      --orange-s: 54%;\n      --orange-l: 53%;\n      --color-orange: hsl(\n        var(--orange-h),\n        var(--orange-s),\n        var(--orange-l)\n      );\n      --color-text-orange: var(--color-orange);\n      --color-bg-orange: hsl(\n        var(--orange-h),\n        calc(var(--orange-s) - 10%),\n        25%\n      );\n      --color-bg-orange-light: hsl(\n        var(--orange-h),\n        calc(var(--orange-s) - 30%),\n        17%\n      );\n      --color-pill-orange: hsl(\n        var(--orange-h),\n        calc(var(--orange-s) + 9%),\n        32%\n      );\n      --color-pill-text-orange: hsl(\n        var(--orange-h),\n        calc(var(--orange-s) - 23%),\n        calc(var(--orange-l) + 33%)\n      );\n      --color-bg-form-btn-orange: #D9730D;\n\n      /* Yellow */\n      --yellow-h: 38;\n      --yellow-s: 54%;\n      --yellow-l: 54%;\n      --color-yellow: hsl(\n        var(--yellow-h),\n        var(--yellow-s),\n        var(--yellow-l)\n      );\n      --color-text-yellow: var(--color-yellow);\n      --color-bg-yellow: hsl(\n        var(--yellow-h),\n        calc(var(--yellow-s) - 18%),\n        25%\n      );\n      --color-bg-yellow-light: hsl(\n        var(--yellow-h),\n        calc(var(--yellow-s) - 24%),\n        17%\n      );\n      --color-pill-yellow: hsl(\n        calc(var(--yellow-h) - 1),\n        calc(var(--yellow-s) + 2%),\n        35%\n      );\n      --color-pill-text-yellow: hsl(\n        var(--yellow-h),\n        calc(var(--yellow-s) - 28%),\n        calc(var(--yellow-l) + 33%)\n      );\n      --color-bg-form-btn-yellow: #CA922F;\n\n      /* Green */\n      --green-h: 146;\n      --green-s: 32%;\n      --green-l: 47%;\n      --color-green: hsl(var(--green-h), var(--green-s), var(--green-l));\n      --color-text-green: var(--color-green);\n      --color-bg-green: hsl(\n        calc(var(--green-h) + 3),\n        calc(var(--green-s) - 6%),\n        19%\n      );\n      --color-bg-green-light: hsl(\n        var(--green-h),\n        calc(var(--green-s) - 20%),\n        15%\n      );\n      --color-pill-green: hsl(\n        calc(var(--green-h) + 1),\n        calc(var(--green-s) + 3%),\n        26%\n      );\n      --color-pill-text-green: hsl(\n        var(--green-h),\n        calc(var(--green-s) - 20%),\n        calc(var(--green-l) + 38%)\n      );\n      --color-bg-form-btn-green: #448361;\n\n      /* Blue */\n      --blue-h: 217;\n      --blue-s: 50%;\n      --blue-l: 58%;\n      --color-blue: hsl(var(--blue-h), var(--blue-s), var(--blue-l));\n      --color-text-blue: var(--color-blue);\n      --color-bg-blue: hsl(\n        calc(var(--blue-h) - 16),\n        calc(var(--blue-s) + 9%),\n        19%\n      );\n      --color-bg-blue-light: hsl(\n        calc(var(--blue-h) + 16),\n        calc(var(--blue-s) - 27%),\n        15%\n      );\n      --color-pill-blue: hsl(\n        calc(var(--blue-h) - 3),\n        calc(var(--blue-s) - 6%),\n        29%\n      );\n      --color-pill-text-blue: hsl(\n        calc(var(--blue-h) - 5),\n        calc(var(--blue-s) - 32%),\n        calc(var(--blue-l) + 28%)\n      );\n      --color-bg-form-btn-blue: #327DA9;\n\n      /* Purple */\n      --purple-h: 270;\n      --purple-s: 55%;\n      --purple-l: 62%;\n      --color-purple: hsl(\n        var(--purple-h),\n        var(--purple-s),\n        var(--purple-l)\n      );\n      --color-text-purple: var(--color-purple);\n      --color-bg-purple: hsl(\n        calc(var(--purple-h) + 2),\n        calc(var(--purple-s) - 31%),\n        23%\n      );\n      --color-bg-purple-light: hsl(\n        calc(var(--purple-h) + 5),\n        calc(var(--purple-s) - 40%),\n        17%\n      );\n      --color-pill-purple: hsl(\n        var(--purple-h),\n        calc(var(--purple-s) - 19%),\n        29%\n      );\n      --color-pill-text-purple: hsl(\n        calc(var(--purple-h) - 3),\n        calc(var(--purple-s) - 40%),\n        calc(var(--purple-l) + 24%)\n      );\n      --color-bg-form-btn-purple: #8F64AF;\n\n      /* Pink */\n      --pink-h: 329;\n      --pink-s: 57%;\n      --pink-l: 58%;\n      --color-pink: hsl(var(--pink-h), var(--pink-s), var(--pink-l));\n      --color-text-pink: var(--color-pink);\n      --color-bg-pink: hsl(\n        calc(var(--pink-h) + 3),\n        calc(var(--pink-s) - 29%),\n        24%\n      );\n      --color-bg-pink-light: hsl(\n        calc(var(--pink-h) + 5),\n        calc(var(--pink-s) - 40%),\n        16%\n      );\n      --color-pill-pink: hsl(\n        calc(var(--pink-h) - 2),\n        calc(var(--pink-s) - 21%),\n        30%\n      );\n      --color-pill-text-pink: hsl(\n        calc(var(--pink-h) + 4),\n        calc(var(--pink-s) - 41%),\n        calc(var(--pink-l) + 28%)\n      );\n      --color-bg-form-btn-pink: #C24C8B;\n\n      /* Red */\n      --red-h: 1;\n      --red-s: 69%;\n      --red-l: 60%;\n      --color-red: hsl(var(--red-h), var(--red-s), var(--red-l));\n      --color-text-red: var(--color-red);\n      --color-bg-red: hsl(\n        calc(var(--red-h) + 5),\n        calc(var(--red-s) - 37%),\n        24%\n      );\n      --color-bg-red-light: hsl(\n        calc(var(--red-h) + 5),\n        calc(var(--red-s) - 56%),\n        17%\n      );\n      --color-pill-red: hsl(\n        calc(var(--red-h) + 5),\n        calc(var(--red-s) - 30%),\n        31%\n      );\n      --color-pill-text-red: hsl(\n        calc(var(--red-h) + 7),\n        calc(var(--red-s) - 50%),\n        calc(var(--red-l) + 24%)\n      );\n      --color-bg-form-btn-red: #D44E49;\n\n      /*Default*/\n      --default-h: 45;\n      --default-s: 8%;\n      --default-l: 20%;\n      --color-default: hsl(\n        var(--default-h),\n        var(--default-s),\n        var(--default-l)\n      );\n      --color-pill-default: hsl(\n        var(--default-h),\n        var(--default-s),\n        -19%\n      );\n      --color-pill-text-default: hsl(\n        var(--default-h),\n        var(--default-s),\n        calc(var(--default-l) + 23%)\n      );\n      --color-bg-form-btn-default: #55534E;\n\n      /*Other*/\n      --color-text-default: #e1e1e1;\n      --color-text-default-light: #9b9b9b;\n      --color-bg-default: #191919;\n      --color-border-default: #373737;\n      --color-border-dark: var(--color-border-default);\n      --color-ui-hover-bg: #262626;\n      --color-card-bg: #262626;\n      --color-card-bg-hover: #2f2f2f;\n      --color-calendar-weekend-bg: #202020;\n      --color-checkbox-bg: #2EAADC;\n      --color-code-bg: rgba(135,131,120,.15);\n\n      /*Scrollbar*/\n      --scrollbar-background-color: #FAFAFA;\n      --scrollbar-thumb-color: #C1C1C1;\n      --scrollbar-border-color: #E8E8E8;\n\n      /*Navbar*/\n      --navbar-text-color: #e1e1e1;\n      --navbar-list-item-hover: rgba(255,255,255,.2);\n      --navbar-background-color: #191919;\n      --navbar-background-color-hover: #282828;\n      --navbar-button-text-color: #191919;\n      --navbar-button-background-color: #e1e1e1;\n      --navbar-menu-background-color: var(--navbar-background-color);\n      --navbar-text-color-dark: #d2d2d2;\n\n      /*Footer*/\n      --footer-text-color: #e1e1e1;\n      --footer-background-color: #191919;\n\n      /*Sidebar*/\n      --sidebar-text-color: #e1e1e1;\n      --sidebar-cta-text-color: #e1e1e1;\n      --sidebar-background-color: #191919;\n      --sidebar-cta-background-color: #191919;\n      --sidebar-border-color: #373737;\n      --sidebar-background-color-hover: #292929;\n      --sidebar-text-color-dark: #d2d2d2;\n      --sidebar-cta-background-color-hover: #292929;\n    }\n\n    /* Calendar for dark mode */\n    html.theme-dark .notion-collection-calendar__row {\n      box-shadow: var(--color-border-default) -1px 0px 0px;\n    }\n\n    html.theme-dark .notion-collection-calendar__week-days {\n      box-shadow: var(--color-border-default) 0px 1px 0px;\n    }\n\n    html.theme-dark .notion-collection-calendar__date {\n      border-right: 1px solid var(--color-border-default);\n      border-bottom: 1px solid var(--color-border-default);\n    }\n\n    \n  /**\n   * One Dark theme for prism.js\n   * Based on Atom's One Dark theme: https://github.com/atom/atom/tree/master/packages/one-dark-syntax\n   */\n\n  /**\n   * One Dark colours (accurate as of commit 8ae45ca on 6 Sep 2018)\n   * From colors.less\n   * --mono-1: hsl(220, 14%, 71%);\n   * --mono-2: hsl(220, 9%, 55%);\n   * --mono-3: hsl(220, 10%, 40%);\n   * --hue-1: hsl(187, 47%, 55%);\n   * --hue-2: hsl(207, 82%, 66%);\n   * --hue-3: hsl(286, 60%, 67%);\n   * --hue-4: hsl(95, 38%, 62%);\n   * --hue-5: hsl(355, 65%, 65%);\n   * --hue-5-2: hsl(5, 48%, 51%);\n   * --hue-6: hsl(29, 54%, 61%);\n   * --hue-6-2: hsl(39, 67%, 69%);\n   * --syntax-fg: hsl(220, 14%, 71%);\n   * --syntax-bg: hsl(220, 13%, 18%);\n   * --syntax-gutter: hsl(220, 14%, 45%);\n   * --syntax-guide: hsla(220, 14%, 71%, 0.15);\n   * --syntax-accent: hsl(220, 100%, 66%);\n   * From syntax-variables.less\n   * --syntax-selection-color: hsl(220, 13%, 28%);\n   * --syntax-gutter-background-color-selected: hsl(220, 13%, 26%);\n   * --syntax-cursor-line: hsla(220, 100%, 80%, 0.04);\n   */\n\n  html.theme-dark code[class*=\"language-\"],\n  html.theme-dark pre[class*=\"language-\"] {\n    background: hsl(220, 13%, 18%);\n    color: hsl(220, 14%, 71%);\n    text-shadow: 0 1px rgba(0, 0, 0, 0.3);\n    font-family: \"Fira Code\", \"Fira Mono\", Menlo, Consolas, \"DejaVu Sans Mono\", monospace;\n    direction: ltr;\n    text-align: left;\n    white-space: pre;\n    word-spacing: normal;\n    word-break: normal;\n    line-height: 1.5;\n    -moz-tab-size: 2;\n    -o-tab-size: 2;\n    tab-size: 2;\n    -webkit-hyphens: none;\n    -moz-hyphens: none;\n    -ms-hyphens: none;\n    hyphens: none;\n  }\n\n  /* Selection */\n  html.theme-dark code[class*=\"language-\"]::-moz-selection,\n  html.theme-dark code[class*=\"language-\"] *::-moz-selection,\n  html.theme-dark pre[class*=\"language-\"] *::-moz-selection {\n    background: hsl(220, 13%, 28%);\n    color: inherit;\n    text-shadow: none;\n  }\n\n  html.theme-dark code[class*=\"language-\"]::selection,\n  html.theme-dark code[class*=\"language-\"] *::selection,\n  html.theme-dark pre[class*=\"language-\"] *::selection {\n    background: hsl(220, 13%, 28%);\n    color: inherit;\n    text-shadow: none;\n  }\n\n  /* Code blocks */\n  html.theme-dark pre[class*=\"language-\"] {\n    padding: 1em;\n    margin: 0.5em 0;\n    overflow: auto;\n    border-radius: 0.3em;\n  }\n\n  /* Inline code */\n  html.theme-dark :not(pre) \u003e code[class*=\"language-\"] {\n    padding: 0.2em 0.3em;\n    border-radius: 0.3em;\n    white-space: normal;\n  }\n\n  /* Print */\n  @media print {\n    html.theme-dark code[class*=\"language-\"],\n    html.theme-dark pre[class*=\"language-\"] {\n      text-shadow: none;\n    }\n  }\n\n  html.theme-dark .token.comment,\n  html.theme-dark .token.prolog,\n  html.theme-dark .token.cdata {\n    color: hsl(220, 10%, 40%);\n  }\n\n  html.theme-dark .token.doctype,\n  html.theme-dark .token.punctuation,\n  html.theme-dark .token.entity {\n    color: hsl(220, 14%, 71%);\n  }\n\n  html.theme-dark .token.attr-name,\n  html.theme-dark .token.class-name,\n  html.theme-dark .token.boolean,\n  html.theme-dark .token.constant,\n  html.theme-dark .token.number,\n  html.theme-dark .token.atrule {\n    color: hsl(29, 54%, 61%);\n  }\n\n  html.theme-dark .token.keyword {\n    color: hsl(286, 60%, 67%);\n  }\n\n  html.theme-dark .token.property,\n  html.theme-dark .token.tag,\n  html.theme-dark .token.symbol,\n  html.theme-dark .token.deleted,\n  html.theme-dark .token.important {\n    color: hsl(355, 65%, 65%);\n  }\n\n  html.theme-dark .token.selector,\n  html.theme-dark .token.string,\n  html.theme-dark .token.char,\n  html.theme-dark .token.builtin,\n  html.theme-dark .token.inserted,\n  html.theme-dark .token.regex,\n  html.theme-dark .token.attr-value,\n  html.theme-dark .token.attr-value \u003e .token.punctuation {\n    color: hsl(95, 38%, 62%);\n  }\n\n  html.theme-dark .token.variable,\n  html.theme-dark .token.operator,\n  html.theme-dark .token.function {\n    color: hsl(207, 82%, 66%);\n  }\n\n  html.theme-dark .token.url {\n    color: hsl(187, 47%, 55%);\n  }\n\n  /* HTML overrides */\n  html.theme-dark .token.attr-value \u003e .token.punctuation.attr-equals,\n  html.theme-dark .token.special-attr \u003e .token.attr-value \u003e .token.value.css {\n    color: hsl(220, 14%, 71%);\n  }\n\n  /* CSS overrides */\n  html.theme-dark .language-css .token.selector {\n    color: hsl(355, 65%, 65%);\n  }\n\n  html.theme-dark .language-css .token.property {\n    color: hsl(220, 14%, 71%);\n  }\n\n  html.theme-dark .language-css .token.function,\n  html.theme-dark .language-css .token.url \u003e .token.function {\n    color: hsl(187, 47%, 55%);\n  }\n\n  html.theme-dark .language-css .token.url \u003e .token.string.url {\n    color: hsl(95, 38%, 62%);\n  }\n\n  html.theme-dark .language-css .token.important,\n  html.theme-dark .language-css .token.atrule .token.rule {\n    color: hsl(286, 60%, 67%);\n  }\n\n  /* JS overrides */\n  html.theme-dark .language-javascript .token.operator {\n    color: hsl(286, 60%, 67%);\n  }\n\n  html.theme-dark .language-javascript .token.template-string \u003e .token.interpolation \u003e .token.interpolation-punctuation.punctuation {\n    color: hsl(5, 48%, 51%);\n  }\n\n  /* JSON overrides */\n  html.theme-dark .language-json .token.operator {\n    color: hsl(220, 14%, 71%);\n  }\n\n  html.theme-dark .language-json .token.null.keyword {\n    color: hsl(29, 54%, 61%);\n  }\n\n  /* MD overrides */\n  html.theme-dark .language-markdown .token.url,\n  html.theme-dark .language-markdown .token.url \u003e .token.operator,\n  html.theme-dark .language-markdown .token.url-reference.url \u003e .token.string {\n    color: hsl(220, 14%, 71%);\n  }\n\n  html.theme-dark .language-markdown .token.url \u003e .token.content {\n    color: hsl(207, 82%, 66%);\n  }\n\n  html.theme-dark .language-markdown .token.url \u003e .token.url,\n  html.theme-dark .language-markdown .token.url-reference.url {\n    color: hsl(187, 47%, 55%);\n  }\n\n  html.theme-dark .language-markdown .token.blockquote.punctuation,\n  html.theme-dark .language-markdown .token.hr.punctuation {\n    color: hsl(220, 10%, 40%);\n    font-style: italic;\n  }\n\n  html.theme-dark .language-markdown .token.code-snippet {\n    color: hsl(95, 38%, 62%);\n  }\n\n  html.theme-dark .language-markdown .token.bold .token.content {\n    color: hsl(29, 54%, 61%);\n  }\n\n  html.theme-dark .language-markdown .token.italic .token.content {\n    color: hsl(286, 60%, 67%);\n  }\n\n  html.theme-dark .language-markdown .token.strike .token.content,\n  html.theme-dark .language-markdown .token.strike .token.punctuation,\n  html.theme-dark .language-markdown .token.list.punctuation,\n  html.theme-dark .language-markdown .token.title.important \u003e .token.punctuation {\n    color: hsl(355, 65%, 65%);\n  }\n\n  /* General */\n  html.theme-dark .token.bold {\n    font-weight: bold;\n  }\n\n  html.theme-dark .token.comment,\n  html.theme-dark .token.italic {\n    font-style: italic;\n  }\n\n  html.theme-dark .token.entity {\n    cursor: help;\n  }\n\n  html.theme-dark .token.namespace {\n    opacity: 0.8;\n  }\n\n  /* Plugin overrides */\n  /* Selectors should have higher specificity than those in the plugins' default stylesheets */\n\n  /* Show Invisibles plugin overrides */\n  html.theme-dark .token.token.tab:not(:empty):before,\n  html.theme-dark .token.token.cr:before,\n  html.theme-dark .token.token.lf:before,\n  html.theme-dark .token.token.space:before {\n    color: hsla(220, 14%, 71%, 0.15);\n    text-shadow: none;\n  }\n\n  /* Toolbar plugin overrides */\n  /* Space out all buttons and move them away from the right edge of the code block */\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item {\n    margin-right: 0.4em;\n  }\n\n  /* Styling the buttons */\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span {\n    background: hsl(220, 13%, 26%);\n    color: hsl(220, 9%, 55%);\n    padding: 0.1em 0.4em;\n    border-radius: 0.3em;\n  }\n\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:hover,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:focus,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:hover,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:focus,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:hover,\n  html.theme-dark div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:focus {\n    background: hsl(220, 13%, 28%);\n    color: hsl(220, 14%, 71%);\n  }\n\n  /* Line Highlight plugin overrides */\n  /* The highlighted line itself */\n  html.theme-dark .line-highlight.line-highlight {\n    background: hsla(220, 100%, 80%, 0.04);\n  }\n\n  /* Default line numbers in Line Highlight plugin */\n  html.theme-dark .line-highlight.line-highlight:before,\n  html.theme-dark .line-highlight.line-highlight[data-end]:after {\n    background: hsl(220, 13%, 26%);\n    color: hsl(220, 14%, 71%);\n    padding: 0.1em 0.6em;\n    border-radius: 0.3em;\n    box-shadow: 0 2px 0 0 rgba(0, 0, 0, 0.2); /* same as Toolbar plugin default */\n  }\n\n  /* Hovering over a linkable line number (in the gutter area) */\n  /* Requires Line Numbers plugin as well */\n  html.theme-dark pre[id].linkable-line-numbers.linkable-line-numbers span.line-numbers-rows \u003e span:hover:before {\n    background-color: hsla(220, 100%, 80%, 0.04);\n  }\n\n  /* Line Numbers and Command Line plugins overrides */\n  /* Line separating gutter from coding area */\n  html.theme-dark .line-numbers.line-numbers .line-numbers-rows,\n  html.theme-dark .command-line .command-line-prompt {\n    border-right-color: hsla(220, 14%, 71%, 0.15);\n  }\n\n  /* Stuff in the gutter */\n  html.theme-dark .line-numbers .line-numbers-rows \u003e span:before,\n  html.theme-dark .command-line .command-line-prompt \u003e span:before {\n    color: hsl(220, 14%, 45%);\n  }\n\n  /* Match Braces plugin overrides */\n  /* Note: Outline colour is inherited from the braces */\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-1,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-5,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-9 {\n    color: hsl(355, 65%, 65%);\n  }\n\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-2,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-6,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-10 {\n    color: hsl(95, 38%, 62%);\n  }\n\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-3,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-7,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-11 {\n    color: hsl(207, 82%, 66%);\n  }\n\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-4,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-8,\n  html.theme-dark .rainbow-braces .token.token.punctuation.brace-level-12 {\n    color: hsl(286, 60%, 67%);\n  }\n\n  /* Diff Highlight plugin overrides */\n  /* Taken from https://github.com/atom/github/blob/master/styles/variables.less */\n  html.theme-dark pre.diff-highlight \u003e code .token.token.deleted:not(.prefix),\n  html.theme-dark pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) {\n    background-color: hsla(353, 100%, 66%, 0.15);\n  }\n\n  html.theme-dark pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-dark pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::-moz-selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::-moz-selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n\n  html.theme-dark pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::selection,\n  html.theme-dark pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n\n  html.theme-dark pre.diff-highlight \u003e code .token.token.inserted:not(.prefix),\n  html.theme-dark pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) {\n    background-color: hsla(137, 100%, 55%, 0.15);\n  }\n\n  html.theme-dark pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-dark pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::-moz-selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::-moz-selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n\n  html.theme-dark pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::selection,\n  html.theme-dark pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::selection,\n  html.theme-dark pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n\n  /* Previewers plugin overrides */\n  /* Based on https://github.com/atom-community/atom-ide-datatip/blob/master/styles/atom-ide-datatips.less and https://github.com/atom/atom/blob/master/packages/one-dark-ui */\n  /* Border around popup */\n  html.theme-dark .prism-previewer.prism-previewer:before,\n  html.theme-dark .prism-previewer-gradient.prism-previewer-gradient div {\n    border-color: hsl(224, 13%, 17%);\n  }\n\n  /* Angle and time should remain as circles and are hence not included */\n  html.theme-dark .prism-previewer-color.prism-previewer-color:before,\n  html.theme-dark .prism-previewer-gradient.prism-previewer-gradient div,\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing:before {\n    border-radius: 0.3em;\n  }\n\n  /* Triangles pointing to the code */\n  html.theme-dark .prism-previewer.prism-previewer:after {\n    border-top-color: hsl(224, 13%, 17%);\n  }\n\n  html.theme-dark .prism-previewer-flipped.prism-previewer-flipped.after {\n    border-bottom-color: hsl(224, 13%, 17%);\n  }\n\n  /* Background colour within the popup */\n  html.theme-dark .prism-previewer-angle.prism-previewer-angle:before,\n  html.theme-dark .prism-previewer-time.prism-previewer-time:before,\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing {\n    background: hsl(219, 13%, 22%);\n  }\n\n  /* For angle, this is the positive area (eg. 90deg will display one quadrant in this colour) */\n  /* For time, this is the alternate colour */\n  html.theme-dark .prism-previewer-angle.prism-previewer-angle circle,\n  html.theme-dark .prism-previewer-time.prism-previewer-time circle {\n    stroke: hsl(220, 14%, 71%);\n    stroke-opacity: 1;\n  }\n\n  /* Stroke colours of the handle, direction point, and vector itself */\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing circle,\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing path,\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing line {\n    stroke: hsl(220, 14%, 71%);\n  }\n\n  /* Fill colour of the handle */\n  html.theme-dark .prism-previewer-easing.prism-previewer-easing circle {\n    fill: transparent;\n  }\n  \n  "])</script><script>self.__next_f.push([1,"28:T46e0,"])</script><script>self.__next_f.push([1,"\n    :root {\n      --color-text-default: rgb(55, 53, 47);\n      --color-text-default-light: rgba(55, 53, 47, 0.6);\n      --color-text-gray: rgb(155, 154, 151);\n      --color-text-brown: rgb(100, 71, 58);\n      --color-text-orange: rgb(217, 115, 13);\n      --color-text-yellow: rgb(223, 171, 1);\n      --color-text-green: rgb(15, 123, 108);\n      --color-text-blue: rgb(11, 110, 153);\n      --color-text-purple: rgb(105, 64, 165);\n      --color-text-pink: rgb(173, 26, 114);\n      --color-text-red: rgb(224, 62, 62);\n\n      --color-bg-default: rgb(255, 255, 255);\n      --color-bg-gray: rgb(235, 236, 237);\n      --color-bg-brown: rgb(233, 229, 227);\n      --color-bg-orange: rgb(250, 235, 221);\n      --color-bg-yellow: rgb(251, 243, 219);\n      --color-bg-green: rgb(221, 237, 234);\n      --color-bg-blue: rgb(221, 235, 241);\n      --color-bg-purple: rgb(234, 228, 242);\n      --color-bg-pink: rgb(244, 223, 235);\n      --color-bg-red: rgb(251, 228, 228);\n\n      --color-bg-gray-light: rgba(235, 236, 237, 0.3);\n      --color-bg-brown-light: rgba(233, 229, 227, 0.3);\n      --color-bg-orange-light: rgba(250, 235, 221, 0.3);\n      --color-bg-yellow-light: rgba(251, 243, 219, 0.3);\n      --color-bg-green-light: rgba(221, 237, 234, 0.3);\n      --color-bg-blue-light: rgba(221, 235, 241, 0.3);\n      --color-bg-purple-light: rgba(234, 228, 242, 0.3);\n      --color-bg-pink-light: rgba(244, 223, 235, 0.3);\n      --color-bg-red-light: rgba(251, 228, 228, 0.3);\n\n      --color-pill-default: rgba(206, 205, 202, 0.5);\n      --color-pill-gray: rgba(155, 154, 151, 0.4);\n      --color-pill-brown: rgba(140, 46, 0, 0.2);\n      --color-pill-orange: rgba(245, 93, 0, 0.2);\n      --color-pill-yellow: rgba(233, 168, 0, 0.2);\n      --color-pill-green: rgba(0, 135, 107, 0.2);\n      --color-pill-blue: rgba(0, 120, 223, 0.2);\n      --color-pill-purple: rgba(103, 36, 222, 0.2);\n      --color-pill-pink: rgba(221, 0, 129, 0.2);\n      --color-pill-red: rgba(255, 0, 26, 0.2);\n\n      --color-ui-hover-bg: rgba(239, 239, 239);\n      --color-card-bg: rgb(255, 255, 255);\n      --color-checkbox-bg: rgb(46, 170, 220);;\n      --color-border-default: rgba(235, 236, 237, 0.8);\n      --color-border-dark: rgba(55, 53, 47, 0.16);\n      --color-code-bg: rgba(239, 239, 239);\n\n      /*Scrollbar*/\n      --scrollbar-background-color: #fafafa;\n      --scrollbar-thumb-color: #c1c1c1;\n      --scrollbar-border-color: #e8e8e8;\n\n      --navbar-text-color: var(--color-text-default);\n      --navbar-background-color: var(--color-bg-default);\n      --navbar-button-text-color: var(--color-text-default);\n      --navbar-button-background-color: var(--color-bg-default);\n      --navbar-menu-background-color: var(--navbar-background-color);\n\n      --footer-text-color: var(--color-text-default);\n      --footer-background-color: var(--color-bg-default);\n      --color-calendar-weekend-bg: #f7f6f3;\n    }\n\n    \n  /**\n   * One Light theme for prism.js\n   * Based on Atom's One Light theme: https://github.com/atom/atom/tree/master/packages/one-light-syntax\n   */\n  \n  /**\n   * One Light colours (accurate as of commit eb064bf on 19 Feb 2021)\n   * From colors.less\n   * --mono-1: hsl(230, 8%, 24%);\n   * --mono-2: hsl(230, 6%, 44%);\n   * --mono-3: hsl(230, 4%, 64%)\n   * --hue-1: hsl(198, 99%, 37%);\n   * --hue-2: hsl(221, 87%, 60%);\n   * --hue-3: hsl(301, 63%, 40%);\n   * --hue-4: hsl(119, 34%, 47%);\n   * --hue-5: hsl(5, 74%, 59%);\n   * --hue-5-2: hsl(344, 84%, 43%);\n   * --hue-6: hsl(35, 99%, 36%);\n   * --hue-6-2: hsl(35, 99%, 40%);\n   * --syntax-fg: hsl(230, 8%, 24%);\n   * --syntax-bg: hsl(230, 1%, 98%);\n   * --syntax-gutter: hsl(230, 1%, 62%);\n   * --syntax-guide: hsla(230, 8%, 24%, 0.2);\n   * --syntax-accent: hsl(230, 100%, 66%);\n   * From syntax-variables.less\n   * --syntax-selection-color: hsl(230, 1%, 90%);\n   * --syntax-gutter-background-color-selected: hsl(230, 1%, 90%);\n   * --syntax-cursor-line: hsla(230, 8%, 24%, 0.05);\n   */\n  \n  html.theme-light code[class*=\"language-\"],\n  html.theme-light pre[class*=\"language-\"] {\n    color: hsl(230, 8%, 24%);\n    font-family: \"Fira Code\", \"Fira Mono\", Menlo, Consolas, \"DejaVu Sans Mono\", monospace;\n    direction: ltr;\n    text-align: left;\n    white-space: pre;\n    word-spacing: normal;\n    word-break: normal;\n    line-height: 1.5;\n    -moz-tab-size: 2;\n    -o-tab-size: 2;\n    tab-size: 2;\n    -webkit-hyphens: none;\n    -moz-hyphens: none;\n    -ms-hyphens: none;\n    hyphens: none;\n  }\n  \n  /* Selection */\n  html.theme-light code[class*=\"language-\"]::-moz-selection,\n  html.theme-light code[class*=\"language-\"] *::-moz-selection,\n  html.theme-light pre[class*=\"language-\"] *::-moz-selection {\n    background: hsl(230, 1%, 90%);\n    color: inherit;\n  }\n  \n  html.theme-light code[class*=\"language-\"]::selection,\n  html.theme-light code[class*=\"language-\"] *::selection,\n  html.theme-light pre[class*=\"language-\"] *::selection {\n    background: hsl(230, 1%, 90%);\n    color: inherit;\n  }\n  \n  /* Code blocks */\n  html.theme-light pre[class*=\"language-\"] {\n    padding: 1em;\n    margin: 0.5em 0;\n    overflow: auto;\n    border-radius: 0.3em;\n  }\n  \n  /* Inline code */\n  html.theme-light :not(pre) \u003e code[class*=\"language-\"] {\n    padding: 0.2em 0.3em;\n    border-radius: 0.3em;\n    white-space: normal;\n  }\n  \n  html.theme-light .token.comment,\n  html.theme-light .token.prolog,\n  html.theme-light .token.cdata {\n    color: hsl(230, 4%, 64%);\n  }\n  \n  html.theme-light .token.doctype,\n  html.theme-light .token.punctuation,\n  html.theme-light .token.entity {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .token.attr-name,\n  html.theme-light .token.class-name,\n  html.theme-light .token.boolean,\n  html.theme-light .token.constant,\n  html.theme-light .token.number,\n  html.theme-light .token.atrule {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  html.theme-light .token.keyword {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .token.property,\n  html.theme-light .token.tag,\n  html.theme-light .token.symbol,\n  html.theme-light .token.deleted,\n  html.theme-light .token.important {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .token.selector,\n  html.theme-light .token.string,\n  html.theme-light .token.char,\n  html.theme-light .token.builtin,\n  html.theme-light .token.inserted,\n  html.theme-light .token.regex,\n  html.theme-light .token.attr-value,\n  html.theme-light .token.attr-value \u003e .token.punctuation {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .token.variable,\n  html.theme-light .token.operator,\n  html.theme-light .token.function {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .token.url {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  /* HTML overrides */\n  html.theme-light .token.attr-value \u003e .token.punctuation.attr-equals,\n  html.theme-light .token.special-attr \u003e .token.attr-value \u003e .token.value.css {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  /* CSS overrides */\n  html.theme-light .language-css .token.selector {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .language-css .token.property {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-css .token.function,\n  html.theme-light .language-css .token.url \u003e .token.function {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  html.theme-light .language-css .token.url \u003e .token.string.url {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .language-css .token.important,\n  html.theme-light .language-css .token.atrule .token.rule {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  /* JS overrides */\n  html.theme-light .language-javascript .token.operator {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .language-javascript .token.template-string \u003e .token.interpolation \u003e .token.interpolation-punctuation.punctuation {\n    color: hsl(344, 84%, 43%);\n  }\n  \n  /* JSON overrides */\n  html.theme-light .language-json .token.operator {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-json .token.null.keyword {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  /* MD overrides */\n  html.theme-light .language-markdown .token.url,\n  html.theme-light .language-markdown .token.url \u003e .token.operator,\n  html.theme-light .language-markdown .token.url-reference.url \u003e .token.string {\n    color: hsl(230, 8%, 24%);\n  }\n  \n  html.theme-light .language-markdown .token.url \u003e .token.content {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .language-markdown .token.url \u003e .token.url,\n  html.theme-light .language-markdown .token.url-reference.url {\n    color: hsl(198, 99%, 37%);\n  }\n  \n  html.theme-light .language-markdown .token.blockquote.punctuation,\n  html.theme-light .language-markdown .token.hr.punctuation {\n    color: hsl(230, 4%, 64%);\n    font-style: italic;\n  }\n  \n  html.theme-light .language-markdown .token.code-snippet {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .language-markdown .token.bold .token.content {\n    color: hsl(35, 99%, 36%);\n  }\n  \n  html.theme-light .language-markdown .token.italic .token.content {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  html.theme-light .language-markdown .token.strike .token.content,\n  html.theme-light .language-markdown .token.strike .token.punctuation,\n  html.theme-light .language-markdown .token.list.punctuation,\n  html.theme-light .language-markdown .token.title.important \u003e .token.punctuation {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  /* General */\n  html.theme-light .token.bold {\n    font-weight: bold;\n  }\n  \n  html.theme-light .token.comment,\n  html.theme-light .token.italic {\n    font-style: italic;\n  }\n  \n  html.theme-light .token.entity {\n    cursor: help;\n  }\n  \n  html.theme-light .token.namespace {\n    opacity: 0.8;\n  }\n  \n  /* Plugin overrides */\n  /* Selectors should have higher specificity than those in the plugins' default stylesheets */\n  \n  /* Show Invisibles plugin overrides */\n  html.theme-light .token.token.tab:not(:empty):before,\n  html.theme-light .token.token.cr:before,\n  html.theme-light .token.token.lf:before,\n  html.theme-light .token.token.space:before {\n    color: hsla(230, 8%, 24%, 0.2);\n  }\n  \n  /* Toolbar plugin overrides */\n  /* Space out all buttons and move them away from the right edge of the code block */\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item {\n    margin-right: 0.4em;\n  }\n  \n  /* Styling the buttons */\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span {\n    background: hsl(230, 1%, 90%);\n    color: hsl(230, 6%, 44%);\n    padding: 0.1em 0.4em;\n    border-radius: 0.3em;\n  }\n  \n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e button:focus,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e a:focus,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:hover,\n  html.theme-light div.code-toolbar \u003e .toolbar.toolbar \u003e .toolbar-item \u003e span:focus {\n    background: hsl(230, 1%, 78%); /* custom: darken(--syntax-bg, 20%) */\n    color: hsl(230, 8%, 24%);\n  }\n  \n  /* Line Highlight plugin overrides */\n  /* The highlighted line itself */\n  html.theme-light .line-highlight.line-highlight {\n    background: hsla(230, 8%, 24%, 0.05);\n  }\n  \n  /* Default line numbers in Line Highlight plugin */\n  html.theme-light .line-highlight.line-highlight:before,\n  html.theme-light .line-highlight.line-highlight[data-end]:after {\n    background: hsl(230, 1%, 90%);\n    color: hsl(230, 8%, 24%);\n    padding: 0.1em 0.6em;\n    border-radius: 0.3em;\n    box-shadow: 0 2px 0 0 rgba(0, 0, 0, 0.2); /* same as Toolbar plugin default */\n  }\n  \n  /* Hovering over a linkable line number (in the gutter area) */\n  /* Requires Line Numbers plugin as well */\n  html.theme-light pre[id].linkable-line-numbers.linkable-line-numbers span.line-numbers-rows \u003e span:hover:before {\n    background-color: hsla(230, 8%, 24%, 0.05);\n  }\n  \n  /* Line Numbers and Command Line plugins overrides */\n  /* Line separating gutter from coding area */\n  html.theme-light .line-numbers.line-numbers .line-numbers-rows,\n  html.theme-light .command-line .command-line-prompt {\n    border-right-color: hsla(230, 8%, 24%, 0.2);\n  }\n  \n  /* Stuff in the gutter */\n  html.theme-light .line-numbers .line-numbers-rows \u003e span:before,\n  html.theme-light .command-line .command-line-prompt \u003e span:before {\n    color: hsl(230, 1%, 62%);\n  }\n  \n  /* Match Braces plugin overrides */\n  /* Note: Outline colour is inherited from the braces */\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-1,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-5,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-9 {\n    color: hsl(5, 74%, 59%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-2,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-6,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-10 {\n    color: hsl(119, 34%, 47%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-3,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-7,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-11 {\n    color: hsl(221, 87%, 60%);\n  }\n  \n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-4,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-8,\n  html.theme-light .rainbow-braces .token.token.punctuation.brace-level-12 {\n    color: hsl(301, 63%, 40%);\n  }\n  \n  /* Diff Highlight plugin overrides */\n  /* Taken from https://github.com/atom/github/blob/master/styles/variables.less */\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix),\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) {\n    background-color: hsla(353, 100%, 66%, 0.15);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::-moz-selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix)::selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.deleted:not(.prefix) *::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix)::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.deleted:not(.prefix) *::selection {\n    background-color: hsla(353, 95%, 66%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix),\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) {\n    background-color: hsla(137, 100%, 55%, 0.15);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::-moz-selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::-moz-selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n  \n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix)::selection,\n  html.theme-light pre.diff-highlight \u003e code .token.token.inserted:not(.prefix) *::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix)::selection,\n  html.theme-light pre \u003e code.diff-highlight .token.token.inserted:not(.prefix) *::selection {\n    background-color: hsla(135, 73%, 55%, 0.25);\n  }\n  \n  /* Previewers plugin overrides */\n  /* Based on https://github.com/atom-community/atom-ide-datatip/blob/master/styles/atom-ide-datatips.less and https://github.com/atom/atom/blob/master/packages/one-light-ui */\n  /* Border around popup */\n  html.theme-light .prism-previewer.prism-previewer:before,\n  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div {\n    border-color: hsl(0, 0, 95%);\n  }\n  \n  /* Angle and time should remain as circles and are hence not included */\n  html.theme-light .prism-previewer-color.prism-previewer-color:before,\n  html.theme-light .prism-previewer-gradient.prism-previewer-gradient div,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing:before {\n    border-radius: 0.3em;\n  }\n  \n  /* Triangles pointing to the code */\n  html.theme-light .prism-previewer.prism-previewer:after {\n    border-top-color: hsl(0, 0, 95%);\n  }\n  \n  html.theme-light .prism-previewer-flipped.prism-previewer-flipped.after {\n    border-bottom-color: hsl(0, 0, 95%);\n  }\n  \n  /* Background colour within the popup */\n  html.theme-light .prism-previewer-angle.prism-previewer-angle:before,\n  html.theme-light .prism-previewer-time.prism-previewer-time:before,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing {\n    background: hsl(0, 0%, 100%);\n  }\n  \n  /* For angle, this is the positive area (eg. 90deg will display one quadrant in this colour) */\n  /* For time, this is the alternate colour */\n  html.theme-light .prism-previewer-angle.prism-previewer-angle circle,\n  html.theme-light .prism-previewer-time.prism-previewer-time circle {\n    stroke: hsl(230, 8%, 24%);\n    stroke-opacity: 1;\n  }\n  \n  /* Stroke colours of the handle, direction point, and vector itself */\n  html.theme-light .prism-previewer-easing.prism-previewer-easing circle,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing path,\n  html.theme-light .prism-previewer-easing.prism-previewer-easing line {\n    stroke: hsl(230, 8%, 24%);\n  }\n  \n  /* Fill colour of the handle */\n  html.theme-light .prism-previewer-easing.prism-previewer-easing circle {\n    fill: transparent;\n  }\n  \n  "])</script><script>self.__next_f.push([1,"1d:[\"$\",\"$L24\",null,{\"settings\":\"$1a:props:children:0:props:settings\",\"isPreview\":false,\"global\":\"$25\",\"light\":\"$26\",\"dark\":\"$27\",\"legacy\":\"$28\",\"children\":[\"$L29\",null]}]\n1e:[\"$\",\"$8\",null,{\"fallback\":null,\"children\":[[\"$\",\"$L2a\",null,{}],[\"$\",\"$L2b\",null,{}],[\"$\",\"$L2c\",null,{}]]}]\n1f:[\"$\",\"$L2d\",null,{\"sampleRate\":0.3}]\n"])</script><script>self.__next_f.push([1,"21:[\"$\",\"main\",null,{\"id\":\"page-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"className\":\"super-content page__blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models parent-page__blog-list\",\"children\":[null,[\"$\",\"$L2e\",null,{\"navbar\":\"$6:props:settings:navbar\",\"head\":{\"url\":\"https://jinkunchen.com/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\",\"description\":\"Why the order of retrieved information can quietly change how AI reasons step by step\",\"root\":{\"id\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"uri\":\"/\",\"title\":\"Hi there!\",\"icon\":null},\"image\":null,\"canonical\":null,\"author\":\"Jinkun Chen\",\"favicon\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\",\"breadcrumbs\":[{\"id\":\"blog\",\"uri\":\"/blog\",\"title\":\"Blog\",\"icon\":null},{\"id\":\"blog-list\",\"uri\":\"/blog/list\",\"title\":\"List\",\"icon\":null},{\"id\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"uri\":\"/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\",\"icon\":null}]},\"settings\":\"$6:props:settings\"}],null,[\"$\",\"div\",null,{\"className\":\"notion-header page\",\"children\":[[\"$\",\"div\",null,{\"className\":\"notion-header__cover no-cover no-icon\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"notion-header__content max-width no-cover no-icon\",\"children\":[[\"$\",\"div\",null,{\"className\":\"notion-header__title-wrapper\",\"children\":[null,[\"$\",\"h1\",null,{\"className\":\"notion-header__title\",\"children\":\"The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models\"}]]}],null]}]]}],[[\"$\",\"$L2f\",\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",{\"id\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"children\":[[\"$\",\"p\",\"37f7210c96014e2a9f56aecbc694e2c5,37f7210c96014e2a9f56aecbc694e2c5\",{\"id\":\"block-37f7210c96014e2a9f56aecbc694e2c5\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"0-Why the order of retrieved information can quietly change how AI reasons step by step\",{\"children\":\"Why the order of retrieved information can quietly change how AI reasons step by step\"}]}]],\"$undefined\"]}],[\"$\",\"p\",\"6c8403d936fb490e8e24f1bf3563e2eb,6c8403d936fb490e8e24f1bf3563e2eb\",{\"id\":\"block-6c8403d936fb490e8e24f1bf3563e2eb\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-You give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.\",{\"children\":\"You give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.\"}]],\"$undefined\"]}],[\"$\",\"p\",\"2cbe5c0189b147baaaf2eebd568f3e11,2cbe5c0189b147baaaf2eebd568f3e11\",{\"id\":\"block-2cbe5c0189b147baaaf2eebd568f3e11\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-This behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages (\\\"chunks\\\") shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, \\\"multi-step inference\\\" refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.\",{\"children\":\"This behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages (\\\"chunks\\\") shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, \\\"multi-step inference\\\" refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.\"}]],\"$undefined\"]}],\"$L30\",\"$L31\",\"$L32\",\"$L33\",\"$L34\",\"$L35\",\"$L36\",\"$L37\",\"$L38\",\"$L39\",\"$L3a\",\"$L3b\",\"$L3c\",\"$L3d\",\"$L3e\",\"$L3f\",\"$L40\",\"$L41\",\"$L42\",\"$L43\",\"$L44\",\"$L45\",\"$L46\",\"$L47\",\"$L48\",\"$L49\",\"$L4a\",\"$L4b\",\"$L4c\",\"$L4d\",\"$L4e\",\"$L4f\",\"$L50\",\"$L51\",\"$L52\",\"$L53\",\"$L54\",\"$L55\",\"$L56\",\"$L57\",\"$L58\",\"$L59\",\"$L5a\",\"$L5b\",\"$L5c\",\"$L5d\",\"$L5e\",\"$L5f\",\"$L60\",\"$L61\",\"$L62\",\"$L63\",\"$L64\",\"$L65\",\"$L66\",\"$L67\",\"$L68\",\"$L69\",\"$L6a\",\"$L6b\",\"$L6c\",\"$L6d\",\"$L6e\",\"$L6f\",\"$L70\",\"$L71\",\"$L72\",\"$L73\",\"$L74\",\"$L75\",\"$L76\",\"$L77\",\"$L78\",\"$L79\",\"$L7a\",\"$L7b\",\"$L7c\",\"$L7d\",\"$L7e\",\"$L7f\",\"$L80\",\"$L81\",\"$L82\",\"$L83\",\"$L84\",\"$L85\",\"$L86\",\"$L87\",\"$L88\",\"$L89\",\"$L8a\",\"$L8b\",\"$L8c\",\"$L8d\",\"$L8e\",\"$L8f\",\"$L90\",\"$L91\",\"$L92\"],\"hasContent\":true,\"parentId\":\"21040d70fdf580fb9cbb000b32f903d7\",\"title\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:title\",\"updatedAt\":1770351032755,\"type\":\"page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"createdTime\":1750303131238,\"lastEditedTime\":1770351032755,\"createdBy\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:createdBy\",\"lastEditedBy\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:lastEditedBy\",\"layout\":null,\"propertySort\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:propertySort\",\"uri\":\"/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"fullWidth\":false,\"smallText\":false,\"noCover\":true,\"superProperties\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:superProperties\",\"propertyValues\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:propertyValues\",\"blockId\":\"21740d70-fdf5-80f2-81fb-e08a7014ce1d\",\"isRoot\":true,\"ctx\":{\"head\":\"$21:props:children:1:props:head\",\"settings\":{\"siteId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb\",\"userId\":\"ef6c16b7-79e4-44d2-99bf-fe8a479479b5\",\"domainName\":\"jinkunchen.com\",\"name\":\"Jinkun Chen\",\"active\":true,\"free\":false,\"tier\":\"personal\",\"favicon\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\",\"fontFamily\":\"Inter\",\"legacyTheme\":null,\"language\":\"en\",\"languages\":\"$6:props:settings:languages\",\"notionPage\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"indexPageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"pageProperties\":true,\"pageSyncing\":true,\"viewSwitcher\":true,\"layoutSidePanel\":true,\"siteSearch\":true,\"advancedSearch\":false,\"themeToggle\":false,\"mondayFirst\":false,\"loadLimits\":true,\"hidePersonInfo\":false,\"showPropertyIcons\":true,\"filterRedirectsFromSearch\":false,\"googleAnalyticsId\":\"G-QJVQ934892\",\"redirectSubdomain\":null,\"noIndex\":false,\"viewsExceeded\":false,\"cacheTTL\":0,\"manualSync\":false,\"syncStatus\":\"unsynced\",\"publishedAt\":null,\"custom404Enabled\":false,\"navbar\":\"$6:props:settings:navbar\",\"footer\":\"$6:props:settings:footer\",\"sidebar\":\"$6:props:settings:sidebar\",\"theme\":\"$6:props:settings:theme\",\"code\":\"$6:props:settings:code\",\"files\":\"$6:props:settings:files\",\"hasPassword\":false,\"hasFeed\":false},\"records\":\"$6:props:records\",\"pageId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\"}}]]]}]\n"])</script><script>self.__next_f.push([1,"22:[\"$\",\"$L93\",null,{\"id\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"children\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:children\",\"hasContent\":true,\"parentId\":\"21040d70fdf580fb9cbb000b32f903d7\",\"title\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:title\",\"updatedAt\":1770351032755,\"type\":\"page\",\"spaceId\":\"f5f1d2cc-b4c1-464e-8646-8e4ce4f03841\",\"createdTime\":1750303131238,\"lastEditedTime\":1770351032755,\"createdBy\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:createdBy\",\"lastEditedBy\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:lastEditedBy\",\"layout\":null,\"propertySort\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:propertySort\",\"uri\":\"/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"fullWidth\":false,\"smallText\":false,\"noCover\":true,\"superProperties\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:superProperties\",\"propertyValues\":\"$6:props:records:block:blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models:propertyValues\",\"blockId\":\"21740d70-fdf5-80f2-81fb-e08a7014ce1d\",\"ctx\":\"$21:props:children:4:0:props:ctx\"}]\n"])</script><script>self.__next_f.push([1,"94:I[238581,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"SuperNavbar\"]\n95:I[381533,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/398f78cbac628ac1.js\"],\"default\"]\n96:I[7549,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\"],\"FooterLogo\"]\n97:I[537826,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"Link\"]\n98:I[652157,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"PreloadChunks\"]\n171:I[240823,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"Toggle\"]\n"])</script><script>self.__next_f.push([1,"29:[\"$\",\"div\",null,{\"className\":\"super-root\",\"children\":[null,[[\"$\",\"$L94\",null,{\"siteId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb\",\"userId\":\"ef6c16b7-79e4-44d2-99bf-fe8a479479b5\",\"domainName\":\"jinkunchen.com\",\"name\":\"Jinkun Chen\",\"active\":true,\"free\":false,\"tier\":\"personal\",\"favicon\":\"https://assets.super.so/e331c927-5859-4092-b1ca-16eddc17b1bb/uploads/favicon/0a4dbc1f-44e7-4d55-9fed-32d043755a78.png\",\"fontFamily\":\"Inter\",\"legacyTheme\":null,\"language\":\"en\",\"languages\":\"$1a:props:children:0:props:settings:languages\",\"notionPage\":\"8d6dfeef4c7f4d678b4899d2198877cb\",\"indexPageId\":\"e331c927-5859-4092-b1ca-16eddc17b1bb:::339f4a8f-e24b-4a21-86cd-5b0a2bae2c18\",\"pageProperties\":true,\"pageSyncing\":true,\"viewSwitcher\":true,\"layoutSidePanel\":true,\"siteSearch\":true,\"advancedSearch\":false,\"themeToggle\":false,\"mondayFirst\":false,\"loadLimits\":true,\"hidePersonInfo\":false,\"showPropertyIcons\":true,\"filterRedirectsFromSearch\":false,\"googleAnalyticsId\":\"G-QJVQ934892\",\"redirectSubdomain\":null,\"noIndex\":false,\"viewsExceeded\":false,\"cacheTTL\":0,\"manualSync\":false,\"syncStatus\":\"unsynced\",\"publishedAt\":null,\"custom404Enabled\":false,\"navbar\":\"$1a:props:children:0:props:settings:navbar\",\"footer\":\"$1a:props:children:0:props:settings:footer\",\"sidebar\":\"$1a:props:children:0:props:settings:sidebar\",\"theme\":\"$1a:props:children:0:props:settings:theme\",\"code\":\"$1a:props:children:0:props:settings:code\",\"files\":\"$1a:props:children:0:props:settings:files\"}],[\"$\",\"div\",null,{\"className\":\"super-content-wrapper\",\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$95\",\"errorStyles\":[],\"errorScripts\":[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/398f78cbac628ac1.js\",\"async\":true}]],\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],[\"$\",\"footer\",null,{\"className\":\"super-footer stack no-links no-footnote\",\"children\":[\"$\",\"div\",null,{\"className\":\"super-footer__content\",\"children\":[[\"$\",\"$L96\",null,{\"logo\":\"$1a:props:children:0:props:settings:footer:logo\"}],null,null,null,[\"$\",\"div\",null,{\"className\":\"super-footer__icons\",\"children\":[[\"$\",\"$L97\",\"https://github.com/Jinnkunn\",{\"uri\":\"https://github.com/Jinnkunn\",\"children\":[\"$\",\"$1\",\"github\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":[540566]}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"width\":24,\"height\":24,\"children\":[[\"$\",\"title\",null,{\"children\":\"GitHub\"}],[\"$\",\"path\",null,{\"d\":\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"}]]}]]}]}],[\"$\",\"$L97\",\"https://www.linkedin.com/in/jinkun-chen/\",{\"uri\":\"https://www.linkedin.com/in/jinkun-chen/\",\"children\":[\"$\",\"$1\",\"linkedin\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":[301034]}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"width\":24,\"height\":24,\"children\":[[\"$\",\"title\",null,{\"children\":\"LinkedIn\"}],[\"$\",\"path\",null,{\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]]}]]}]}],[\"$\",\"$L97\",\"https://twitter.com/_jinnkunn\",{\"uri\":\"https://twitter.com/_jinnkunn\",\"children\":[\"$\",\"$1\",\"twitter\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":[23328]}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 16 16\",\"width\":24,\"height\":24,\"children\":[[\"$\",\"title\",null,{\"children\":\"X\"}],[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"clipRule\":\"evenodd\",\"d\":\"M0.5 0.5H5.75L9.48421 5.71053L14 0.5H16L10.3895 6.97368L16.5 15.5H11.25L7.51579 10.2895L3 15.5H1L6.61053 9.02632L0.5 0.5ZM12.0204 14L3.42043 2H4.97957L13.5796 14H12.0204Z\"}]]}]]}]}],\"$L99\"]}]]}]}]]]}]\n"])</script><script>self.__next_f.push([1,"30:[\"$\",\"p\",\"4a8efe9c8c844c6099a647fae633e65a,4a8efe9c8c844c6099a647fae633e65a\",{\"id\":\"block-4a8efe9c8c844c6099a647fae633e65a\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-A key finding is that preserving the original document structure, as seen in Document's Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM's sequential processing.\",{\"children\":\"A key finding is that preserving the original document structure, as seen in Document's Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM's sequential processing.\"}]],\"$undefined\"]}]\n31:[\"$\",\"p\",\"d95145e3a15c47179bb2c31dae95d2ef,d95145e3a15c47179bb2c31dae95d2ef\",{\"id\":\"block-d95145e3a15c47179bb2c31dae95d2ef\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-For users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.\",{\"children\":\"For users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"32:[\"$\",\"p\",\"975a6c020a53497d81afe6ed91c2ac38,975a6c020a53497d81afe6ed91c2ac38\",{\"id\":\"block-975a6c020a53497d81afe6ed91c2ac38\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The analysis further reveals that LLMs exhibit a \\\"cognitive linearity\\\", performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.\",{\"children\":\"The analysis further reveals that LLMs exhibit a \\\"cognitive linearity\\\", performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"33:[\"$\",\"p\",\"15da42c0920c4fe5a384039880acbf25,15da42c0920c4fe5a384039880acbf25\",{\"id\":\"block-15da42c0920c4fe5a384039880acbf25\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Beyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.\",{\"children\":\"Beyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.\"}]],\"$undefined\"]}]\n34:[\"$\",\"div\",\"21740d70fdf5809e9b08e50fd38ec2c9,21740d70fdf5809e9b08e50fd38ec2c9\",{\"id\":\"block-21740d70fdf5809e9b08e50fd38ec2c9\",\"className\":\"notion-text\"}]\n35:[\"$\",\"div\",\"21740d70fdf5807cb076cbe47bde3bc5,21740d70fdf5807cb076cbe47bde3bc5\",{\"id\":\"block-21740d70fdf5807cb076cbe47bde3bc5\",\"className\":\"notion-divider\"}]\n"])</script><script>self.__next_f.push([1,"36:[\"$\",\"ul\",\"21740d70fdf5803ba492f8f7a0f1124b,21740d70fdf5803ba492f8f7a0f1124b\",{\"id\":\"block-21740d70fdf5803ba492f8f7a0f1124b\",\"className\":\"notion-table-of-contents color-gray\",\"children\":[[\"$\",\"li\",\"21740d70fdf580789318c4b68c9dc180\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580789318c4b68c9dc180\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\",{\"children\":\"1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf580998476c769158dc800\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580998476c769158dc800\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\",{\"children\":\"1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf5804ea1b3c814f4776242\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5804ea1b3c814f4776242\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-1.2 Understanding Retrieval-Augmented Generation (RAG)\",{\"children\":\"1.2 Understanding Retrieval-Augmented Generation (RAG)\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf580b78f06c6a97022bc5c\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580b78f06c6a97022bc5c\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-1.3 The Essence of Multi-Step Inference in LLMs\",{\"children\":\"1.3 The Essence of Multi-Step Inference in LLMs\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf58034b829fad21730876f\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf58034b829fad21730876f\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-1.4 Purpose and Scope of the Report\",{\"children\":\"1.4 Purpose and Scope of the Report\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf5803d9807e94a97e877c9\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5803d9807e94a97e877c9\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-2. Chunking and Retrieval in RAG Architectures\",{\"children\":\"2. Chunking and Retrieval in RAG Architectures\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf580e68ebfe97e77da4cf0\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580e68ebfe97e77da4cf0\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-2.1 Principles of Document Chunking for RAG\",{\"children\":\"2.1 Principles of Document Chunking for RAG\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf5807b8ea1e3f0ef8cce21\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5807b8ea1e3f0ef8cce21\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-2.2 Overview of Retrieval Mechanisms in RAG\",{\"children\":\"2.2 Overview of Retrieval Mechanisms in RAG\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf58015b472e155c66d727f\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf58015b472e155c66d727f\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"48px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-3. The Critical Role of Chunk Retrieval Sequence\",{\"children\":\"3. The Critical Role of Chunk Retrieval Sequence\"}]],\"$undefined\"]}]}]}],[\"$\",\"li\",\"21740d70fdf580c4a559c3eff45d8257\",{\"className\":\"notion-table-of-contents__item\",\"children\":\"$L9a\"}],\"$L9b\",\"$L9c\",\"$L9d\",\"$L9e\",\"$L9f\",\"$La0\",\"$La1\",\"$La2\",\"$La3\",\"$La4\",\"$La5\",\"$La6\",\"$La7\",\"$La8\",\"$La9\"]}]\n"])</script><script>self.__next_f.push([1,"37:[\"$\",\"div\",\"21740d70fdf5804496dffff09e93c171,21740d70fdf5804496dffff09e93c171\",{\"id\":\"block-21740d70fdf5804496dffff09e93c171\",\"className\":\"notion-divider\"}]\n38:[\"$\",\"div\",\"21740d70fdf58019b4e5f82038300e4e,21740d70fdf58019b4e5f82038300e4e\",{\"id\":\"block-21740d70fdf58019b4e5f82038300e4e\",\"className\":\"notion-text\"}]\n39:[\"$\",\"$1\",\"21740d70fdf580789318c4b68c9dc180,21740d70fdf580789318c4b68c9dc180\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580789318c4b68c9dc180\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf580789318c4b68c9dc180\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\",{\"children\":\"1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"3a:[\"$\",\"p\",\"21740d70fdf580b59114ebb95e594cb4,21740d70fdf580b59114ebb95e594cb4\",{\"id\":\"block-21740d70fdf580b59114ebb95e594cb4\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.\",{\"children\":\"The landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"3b:[\"$\",\"$1\",\"21740d70fdf580998476c769158dc800,21740d70fdf580998476c769158dc800\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580998476c769158dc800\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580998476c769158dc800\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\",{\"children\":\"1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"3c:[\"$\",\"p\",\"21740d70fdf580ae8f73f007b0a8ec92,21740d70fdf580ae8f73f007b0a8ec92\",{\"id\":\"block-21740d70fdf580ae8f73f007b0a8ec92\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Large Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.\",{\"children\":\"Large Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":[109771]}],\"$Laa\"]}],[\"$\",\"$1\",\"2- These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.\",{\"children\":\" These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lab\"]}],[\"$\",\"$1\",\"4- LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.\",{\"children\":\" LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lac\"]}],[\"$\",\"$1\",\"6- Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.\",{\"children\":\" Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lad\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"3d:[\"$\",\"p\",\"21740d70fdf5807ba813e037c8c3158e,21740d70fdf5807ba813e037c8c3158e\",{\"id\":\"block-21740d70fdf5807ba813e037c8c3158e\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Despite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.\",{\"children\":\"Despite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lae\"]}],[\"$\",\"$1\",\"2- This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as \\\"hallucinations\\\", where the model generates factually incorrect or nonsensical information not present in its training data.\",{\"children\":\" This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as \\\"hallucinations\\\", where the model generates factually incorrect or nonsensical information not present in its training data.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Laf\"]}],[\"$\",\"$1\",\"4- Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.\",{\"children\":\" Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb0\"]}],[\"$\",\"$1\",\"6- These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.\",{\"children\":\" These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"div\",\"21740d70fdf580e8a51ad82068f715f8,21740d70fdf580e8a51ad82068f715f8\",{\"id\":\"block-21740d70fdf580e8a51ad82068f715f8\",\"className\":\"notion-text\"}]\n3f:[\"$\",\"$1\",\"21740d70fdf5804ea1b3c814f4776242,21740d70fdf5804ea1b3c814f4776242\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5804ea1b3c814f4776242\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf5804ea1b3c814f4776242\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-1.2 Understanding Retrieval-Augmented Generation (RAG)\",{\"children\":\"1.2 Understanding Retrieval-Augmented Generation (RAG)\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"p\",\"21740d70fdf5804ea49fd96ef424e931,21740d70fdf5804ea49fd96ef424e931\",{\"id\":\"block-21740d70fdf5804ea49fd96ef424e931\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-\\nRetrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.\",{\"children\":\"\\nRetrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb1\"]}],[\"$\",\"$1\",\"2- This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.\",{\"children\":\" This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb2\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"p\",\"21740d70fdf58009ba9df8dfcba9a012,21740d70fdf58009ba9df8dfcba9a012\",{\"id\":\"block-21740d70fdf58009ba9df8dfcba9a012\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The core process of RAG typically involves two main stages. First, \",{\"children\":\"The core process of RAG typically involves two main stages. First, \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-Retrieval and Pre-processing\",{\"children\":\"Retrieval and Pre-processing\"}]}],[\"$\",\"$1\",\"2- occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.\",{\"children\":\" occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb3\"]}],[\"$\",\"$1\",\"4- The second stage is \",{\"children\":\" The second stage is \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-Grounded Generation\",{\"children\":\"Grounded Generation\"}]}],[\"$\",\"$1\",\"6-, where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM's context. This integration significantly enhances the LLM's understanding of the topic, allowing it to produce more precise, informative, and engaging responses.\",{\"children\":\", where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM's context. This integration significantly enhances the LLM's understanding of the topic, allowing it to produce more precise, informative, and engaging responses.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb4\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"42:[\"$\",\"p\",\"21740d70fdf580849758fee1679a61da,21740d70fdf580849758fee1679a61da\",{\"id\":\"block-21740d70fdf580849758fee1679a61da\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-RAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.\",{\"children\":\"RAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb5\"]}],[\"$\",\"$1\",\"2- This factual grounding is crucial for mitigating \\\"gen AI hallucinations\\\" by supplying verifiable facts as part of the input prompt.\",{\"children\":\" This factual grounding is crucial for mitigating \\\"gen AI hallucinations\\\" by supplying verifiable facts as part of the input prompt.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb6\"]}],[\"$\",\"$1\",\"4- The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM's outputs.\",{\"children\":\" The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM's outputs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb7\"]}],[\"$\",\"$1\",\"6- This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.\",{\"children\":\" This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"$1\",\"21740d70fdf580b78f06c6a97022bc5c,21740d70fdf580b78f06c6a97022bc5c\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580b78f06c6a97022bc5c\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580b78f06c6a97022bc5c\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-1.3 The Essence of Multi-Step Inference in LLMs\",{\"children\":\"1.3 The Essence of Multi-Step Inference in LLMs\"}]}]],\"$undefined\"]}]]}]\n44:[\"$\",\"p\",\"21740d70fdf580e887d9cfc1af9f673c,21740d70fdf580e887d9cfc1af9f673c\",{\"id\":\"block-21740d70fdf580e887d9cfc1af9f673c\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Multi-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM's capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.\",{\"children\":\"Multi-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM's capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb8\"]}],[\"$\",\"$1\",\"2- This capability extends beyond merely following a single instruction or performing a singular task.\",{\"children\":\" This capability extends beyond merely following a single instruction or performing a singular task.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lb9\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"p\",\"21740d70fdf580b3a0d7c0300ffd118a,21740d70fdf580b3a0d7c0300ffd118a\",{\"id\":\"block-21740d70fdf580b3a0d7c0300ffd118a\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.\",{\"children\":\"The ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lba\"]}],[\"$\",\"$1\",\"2- It is widely recognized as a key indicator of advanced intelligence in AI systems.\",{\"children\":\" It is widely recognized as a key indicator of advanced intelligence in AI systems.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lbb\"]}],[\"$\",\"$1\",\"4- However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.\",{\"children\":\" However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lbc\"]}],[\"$\",\"$1\",\"6- While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.\",{\"children\":\" While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lbd\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"p\",\"21740d70fdf580d1b729df508408cc3d,21740d70fdf580d1b729df508408cc3d\",{\"id\":\"block-21740d70fdf580d1b729df508408cc3d\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-To address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\",{\"children\":\"To address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lbe\"]}],[\"$\",\"$1\",\"2- Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.\",{\"children\":\" Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lbf\"]}],[\"$\",\"$1\",\"4- The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. \",{\"children\":\" The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. \"}],[\"$\",\"$1\",\"5-This shift underscores that future advancements in LLMs and RAG systems must prioritize not just \",{\"children\":\"This shift underscores that future advancements in LLMs and RAG systems must prioritize not just \"}],[\"$\",\"em\",\"2-i\",{\"children\":[\"$\",\"strong\",\"1-b\",{\"children\":[\"$\",\"u\",\"0-_\",{\"children\":[\"$\",\"$1\",\"6-what\",{\"children\":\"what\"}]}]}]}],[\"$\",\"$1\",\"7- information is retrieved, but \",{\"children\":\" information is retrieved, but \"}],[\"$\",\"em\",\"2-i\",{\"children\":[\"$\",\"strong\",\"1-b\",{\"children\":[\"$\",\"u\",\"0-_\",{\"children\":[\"$\",\"$1\",\"8-how\",{\"children\":\"how\"}]}]}]}],[\"$\",\"$1\",\"9- that information facilitates a structured\",{\"children\":\" that information facilitates a structured\"}],[\"$\",\"$1\",\"10-, step-by-step problem-solving process.\",{\"children\":\", step-by-step problem-solving process.\"}],[\"$\",\"$1\",\"11- This elevates the importance of context organization and coherence within the input.\",{\"children\":\" This elevates the importance of context organization and coherence within the input.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"$1\",\"21740d70fdf58034b829fad21730876f,21740d70fdf58034b829fad21730876f\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf58034b829fad21730876f\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf58034b829fad21730876f\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-1.4 Purpose and Scope of the Report\",{\"children\":\"1.4 Purpose and Scope of the Report\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"p\",\"21740d70fdf580a7a5e1c5589a27f4b4,21740d70fdf580a7a5e1c5589a27f4b4\",{\"id\":\"block-21740d70fdf580a7a5e1c5589a27f4b4\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM's capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.\",{\"children\":\"The primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM's capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"49:[\"$\",\"$1\",\"21740d70fdf5803d9807e94a97e877c9,21740d70fdf5803d9807e94a97e877c9\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5803d9807e94a97e877c9\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf5803d9807e94a97e877c9\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-2. Chunking and Retrieval in RAG Architectures\",{\"children\":\"2. Chunking and Retrieval in RAG Architectures\"}]}]],\"$undefined\"]}]]}]\n4a:[\"$\",\"p\",\"21740d70fdf580428548dda1c2b7ff6a,21740d70fdf580428548dda1c2b7ff6a\",{\"id\":\"block-21740d70fdf580428548dda1c2b7ff6a\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.\",{\"children\":\"The efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.\"}]],\"$undefined\"]}]\n4b:[\"$\",\"$1\",\"21740d70fdf580e68ebfe97e77da4cf0,21740d70fdf580e68ebfe97e77da4cf0\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580e68ebfe97e77da4cf0\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580e68ebfe97e77da4cf0\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-2.1 Principles of Document Chunking for RAG\",{\"children\":\"2.1 Principles of Document Chunking for RAG\"}]}]],\"$undefined\"]}]]}]\n4c:[\"$\",\"p\",\"21740d70fdf580ceb7d1e138749fece9,21740d70fdf580ceb7d1e138749fece9\",{\"id\":\"block-21740d70fdf580ceb7d1e138749fece9\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Chunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.\",{\"children\":\"Chunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc0\"]}],[\"$\",\"$1\",\"2- These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.\",{\"children\":\" These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc1\"]}],[\"$\",\"$1\",\"4- The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.\",{\"children\":\" The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"4d:[\"$\",\"p\",\"21740d70fdf5802594fac1a34fc2b5be,21740d70fdf5802594fac1a34fc2b5be\",{\"id\":\"block-21740d70fdf5802594fac1a34fc2b5be\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.\",{\"children\":\"The necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc2\"]}],[\"$\",\"$1\",\"2- By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.\",{\"children\":\" By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc3\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"4e:[\"$\",\"p\",\"21740d70fdf580aca4b9f403a1abb511,21740d70fdf580aca4b9f403a1abb511\",{\"id\":\"block-21740d70fdf580aca4b9f403a1abb511\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Several chunking strategies are employed, each with distinct advantages and use cases:\",{\"children\":\"Several chunking strategies are employed, each with distinct advantages and use cases:\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"4f:[\"$\",\"ul\",\"21740d70fdf5807685ece0dd6484ea2e,21740d70fdf5807685ece0dd6484ea2e\",{\"className\":\"notion-bulleted-list\",\"children\":[[\"$\",\"li\",\"21740d70fdf5807685ece0dd6484ea2e\",{\"id\":\"block-21740d70fdf5807685ece0dd6484ea2e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Fixed Size Chunking:\",{\"children\":\"Fixed Size Chunking:\"}]}],[\"$\",\"$1\",\"1- This straightforward approach divides text into uniform chunks based on a predefined character or token count.\",{\"children\":\" This straightforward approach divides text into uniform chunks based on a predefined character or token count.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc4\"]}],[\"$\",\"$1\",\"3- For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.\",{\"children\":\" For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc5\"]}],[\"$\",\"$1\",\"5- While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.\",{\"children\":\" While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc6\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58026839cfc6fc508c42b\",{\"id\":\"block-21740d70fdf58026839cfc6fc508c42b\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Recursive-Based Chunking:\",{\"children\":\"Recursive-Based Chunking:\"}]}],[\"$\",\"$1\",\"1- A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.\",{\"children\":\" A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc7\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580efb9a6db405332f049\",{\"id\":\"block-21740d70fdf580efb9a6db405332f049\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Sentence-based Chunking:\",{\"children\":\"Sentence-based Chunking:\"}]}],[\"$\",\"$1\",\"1- This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.\",{\"children\":\" This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lc8\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf5800fae7ed100c5ecd009\",{\"id\":\"block-21740d70fdf5800fae7ed100c5ecd009\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Document Structure-based Chunking:\",{\"children\":\"Document Structure-based Chunking:\"}]}],[\"$\",\"$1\",\"1- This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.\",{\"children\":\" This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.\"}],\"$Lc9\"],\"$undefined\"]}],\"$Lca\"]}]\n"])</script><script>self.__next_f.push([1,"50:[\"$\",\"p\",\"21740d70fdf580c0b810f7633ce50e32,21740d70fdf580c0b810f7633ce50e32\",{\"id\":\"block-21740d70fdf580c0b810f7633ce50e32\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM's ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.\",{\"children\":\"The choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM's ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"51:[\"$\",\"$1\",\"21740d70fdf5807b8ea1e3f0ef8cce21,21740d70fdf5807b8ea1e3f0ef8cce21\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5807b8ea1e3f0ef8cce21\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf5807b8ea1e3f0ef8cce21\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-2.2 Overview of Retrieval Mechanisms in RAG\",{\"children\":\"2.2 Overview of Retrieval Mechanisms in RAG\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"52:[\"$\",\"p\",\"21740d70fdf580408d4fc76f4eef75dc,21740d70fdf580408d4fc76f4eef75dc\",{\"id\":\"block-21740d70fdf580408d4fc76f4eef75dc\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-A RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a \",{\"children\":\"A RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-Query Encoder\",{\"children\":\"Query Encoder\"}]}],[\"$\",\"$1\",\"2- transforms the user's input query into a representation suitable for searching the knowledge base.\",{\"children\":\" transforms the user's input query into a representation suitable for searching the knowledge base.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lcb\"]}],[\"$\",\"$1\",\"4- Second, a \",{\"children\":\" Second, a \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-Retriever\",{\"children\":\"Retriever\"}]}],[\"$\",\"$1\",\"6- takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.\",{\"children\":\" takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lcc\"]}],[\"$\",\"$1\",\"8-Finally, a \",{\"children\":\"Finally, a \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"9-Generator\",{\"children\":\"Generator\"}]}],[\"$\",\"$1\",\"10-, typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.\",{\"children\":\", typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lcd\"]}],[\"$\",\"$1\",\"12-\\n\",{\"children\":\"\\n\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"53:[\"$\",\"p\",\"21740d70fdf580d78798c7861511ed2e,21740d70fdf580d78798c7861511ed2e\",{\"id\":\"block-21740d70fdf580d78798c7861511ed2e\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Retrievers can be broadly categorized based on their underlying mechanisms:\",{\"children\":\"Retrievers can be broadly categorized based on their underlying mechanisms:\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"54:[\"$\",\"ul\",\"21740d70fdf5805299ddcb51cebbc031,21740d70fdf5805299ddcb51cebbc031\",{\"className\":\"notion-bulleted-list\",\"children\":[[\"$\",\"li\",\"21740d70fdf5805299ddcb51cebbc031\",{\"id\":\"block-21740d70fdf5805299ddcb51cebbc031\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Sparse Retrievers:\",{\"children\":\"Sparse Retrievers:\"}]}],[\"$\",\"$1\",\"1- These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.\",{\"children\":\" These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lce\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580f880d2ce188348e4e8\",{\"id\":\"block-21740d70fdf580f880d2ce188348e4e8\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Dense Retrievers:\",{\"children\":\"Dense Retrievers:\"}]}],[\"$\",\"$1\",\"1- Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.\",{\"children\":\" Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lcf\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580778da5dc116592082c\",{\"id\":\"block-21740d70fdf580778da5dc116592082c\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Hybrid Search:\",{\"children\":\"Hybrid Search:\"}]}],[\"$\",\"$1\",\"1- Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.\",{\"children\":\" Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ld0\"]}],[\"$\",\"$1\",\"3-\\nThe retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.\",{\"children\":\"\\nThe retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ld1\"]}],[\"$\",\"$1\",\"5- After the initial retrieval, an essential step often involves \",{\"children\":\" After the initial retrieval, an essential step often involves \"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580f39177ff83be84b5c3\",{\"id\":\"block-21740d70fdf580f39177ff83be84b5c3\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-re-rankers\",{\"children\":\"re-rankers\"}]}],[\"$\",\"$1\",\"1-. These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.\",{\"children\":\". These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ld2\"]}],[\"$\",\"$1\",\"3- This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.\",{\"children\":\" This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.\"}],\"$Ld3\",\"$Ld4\",\"$Ld5\",\"$Ld6\",\"$Ld7\",\"$Ld8\",\"$Ld9\",\"$Lda\"],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"55:[\"$\",\"$1\",\"21740d70fdf58015b472e155c66d727f,21740d70fdf58015b472e155c66d727f\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf58015b472e155c66d727f\"}],[\"$\",\"h3\",null,{\"id\":\"block-21740d70fdf58015b472e155c66d727f\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-3. The Critical Role of Chunk Retrieval Sequence\",{\"children\":\"3. The Critical Role of Chunk Retrieval Sequence\"}]}]],\"$undefined\"]}]]}]\n56:[\"$\",\"p\",\"21740d70fdf580a68709d4be6ca361e1,21740d70fdf580a68709d4be6ca361e1\",{\"id\":\"block-21740d70fdf580a68709d4be6ca361e1\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM's ability to reason effectively.\",{\"children\":\"The order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM's ability to reason effectively.\"}]],\"$undefined\"]}]\n57:[\"$\",\"$1\",\"21740d70fdf580c4a559c3eff45d8257,21740d70fdf580c4a559c3eff45d8257\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580c4a559c3eff45d8257\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580c4a559c3eff45d8257\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-3.1 Impact of Context Order on LLM Performance\",{\"children\":\"3.1 Impact of Context Order on LLM Performance\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"58:[\"$\",\"p\",\"21740d70fdf5800585f8e60d24de38aa,21740d70fdf5800585f8e60d24de38aa\",{\"id\":\"block-21740d70fdf5800585f8e60d24de38aa\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Observations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.\",{\"children\":\"Observations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ldb\"]}],[\"$\",\"$1\",\"2- This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM's internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.\",{\"children\":\" This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM's internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ldc\"]}],[\"$\",\"$1\",\"4- For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model's accuracy.\",{\"children\":\" For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model's accuracy.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ldd\"]}],[\"$\",\"$1\",\"6- This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.\",{\"children\":\" This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lde\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"59:[\"$\",\"p\",\"21740d70fdf580aebbeaf862f0f1bb84,21740d70fdf580aebbeaf862f0f1bb84\",{\"id\":\"block-21740d70fdf580aebbeaf862f0f1bb84\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Conversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.\",{\"children\":\"Conversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Ldf\"]}],[\"$\",\"$1\",\"2- This \\\"ordering effect\\\" is further exacerbated when irrelevant premises are introduced into the prompt.\",{\"children\":\" This \\\"ordering effect\\\" is further exacerbated when irrelevant premises are introduced into the prompt.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le0\"]}],[\"$\",\"$1\",\"4- When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model's ability to synthesize information and produce coherent responses.\",{\"children\":\" When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model's ability to synthesize information and produce coherent responses.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le1\"]}],[\"$\",\"$1\",\"6- This indicates that LLMs, despite their advanced capabilities, exhibit a form of \\\"cognitive linearity\\\" in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a \\\"ground truth proof\\\" \",{\"children\":\" This indicates that LLMs, despite their advanced capabilities, exhibit a form of \\\"cognitive linearity\\\" in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a \\\"ground truth proof\\\" \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le2\"]}],[\"$\",\"$1\",\"8- suggests that the LLM's internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.\",{\"children\":\" suggests that the LLM's internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"5a:[\"$\",\"$1\",\"21740d70fdf580d8a90af2ad47b882d5,21740d70fdf580d8a90af2ad47b882d5\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580d8a90af2ad47b882d5\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580d8a90af2ad47b882d5\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-3.2 Document's Original Structure (DOS RAG) and its Benefits\",{\"children\":\"3.2 Document's Original Structure (DOS RAG) and its Benefits\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"5b:[\"$\",\"p\",\"21740d70fdf580ba874bff70dae33e6c,21740d70fdf580ba874bff70dae33e6c\",{\"id\":\"block-21740d70fdf580ba874bff70dae33e6c\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Document's Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.\",{\"children\":\"Document's Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le3\"]}],[\"$\",\"$1\",\"2- This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.\",{\"children\":\" This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le4\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"5c:[\"$\",\"p\",\"21740d70fdf580a281e2d404fdd2bcc7,21740d70fdf580a281e2d404fdd2bcc7\",{\"id\":\"block-21740d70fdf580a281e2d404fdd2bcc7\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The benefits of DOS RAG are significant and empirically validated. It primarily \",{\"children\":\"The benefits of DOS RAG are significant and empirically validated. It primarily \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-preserves passage continuity\",{\"children\":\"preserves passage continuity\"}]}],[\"$\",\"$1\",\"2-, maintaining the document's structural integrity and narrative flow.\",{\"children\":\", maintaining the document's structural integrity and narrative flow.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le5\"]}],[\"$\",\"$1\",\"4- This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves \\n\",{\"children\":\" This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves \\n\"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-improved accuracy\",{\"children\":\"improved accuracy\"}]}],[\"$\",\"$1\",\"6-, outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including Bench, QuALITY, and NarrativeQA.\",{\"children\":\", outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including Bench, QuALITY, and NarrativeQA.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le6\"]}],[\"$\",\"$1\",\"8- This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.\",{\"children\":\" This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le7\"]}],[\"$\",\"$1\",\"10- For instance, on the Bench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG's 87.8%.\",{\"children\":\" For instance, on the Bench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG's 87.8%.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le8\"]}],[\"$\",\"$1\",\"12- Furthermore, DOS RAG demonstrates notable \\n\",{\"children\":\" Furthermore, DOS RAG demonstrates notable \\n\"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"13-efficiency\",{\"children\":\"efficiency\"}]}],[\"$\",\"$1\",\"14-, often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.\",{\"children\":\", often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Le9\"]}],[\"$\",\"$1\",\"16- This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\",{\"children\":\" This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lea\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"5d:[\"$\",\"p\",\"21740d70fdf580848881d08f51d59cdf,21740d70fdf580848881d08f51d59cdf\",{\"id\":\"block-21740d70fdf580848881d08f51d59cdf\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, \",{\"children\":\"The consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-contextual coherence\",{\"children\":\"contextual coherence\"}]}],[\"$\",\"$1\",\"2- and \",{\"children\":\" and \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"3-narrative flow\",{\"children\":\"narrative flow\"}]}],[\"$\",\"$1\",\"4-, as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.\",{\"children\":\", as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Leb\"]}],[\"$\",\"$1\",\"6- However, DOS RAG's consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the \",{\"children\":\" However, DOS RAG's consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"7-relationship\",{\"children\":\"relationship\"}]}],[\"$\",\"$1\",\"8- between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.\",{\"children\":\" between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lec\"]}],[\"$\",\"$1\",\"10- Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.\",{\"children\":\" Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Led\"]}],[\"$\",\"$1\",\"12- Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM's processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of \\\"relevance\\\" for multi-step tasks should be broadened to include \\\"contextual relevance\\\" or \\\"narrative relevance\\\" in addition to traditional semantic similarity.\",{\"children\":\" Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM's processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of \\\"relevance\\\" for multi-step tasks should be broadened to include \\\"contextual relevance\\\" or \\\"narrative relevance\\\" in addition to traditional semantic similarity.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"5e:[\"$\",\"$1\",\"21740d70fdf5808f9238d67cfb9d9d82,21740d70fdf5808f9238d67cfb9d9d82\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5808f9238d67cfb9d9d82\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf5808f9238d67cfb9d9d82\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-3.3 Reranking Strategies and Context Reordering\",{\"children\":\"3.3 Reranking Strategies and Context Reordering\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"5f:[\"$\",\"p\",\"21740d70fdf58081a3ecd927972a059f,21740d70fdf58081a3ecd927972a059f\",{\"id\":\"block-21740d70fdf58081a3ecd927972a059f\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.\",{\"children\":\"Reranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lee\"]}],[\"$\",\"$1\",\"2- This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.\",{\"children\":\" This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lef\"]}],[\"$\",\"$1\",\"4-\\nVarious types of rerankers are employed, each with distinct characteristics:\",{\"children\":\"\\nVarious types of rerankers are employed, each with distinct characteristics:\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"60:[\"$\",\"ul\",\"21740d70fdf58058a46cfc4b81aaecc4,21740d70fdf58058a46cfc4b81aaecc4\",{\"className\":\"notion-bulleted-list\",\"children\":[[\"$\",\"li\",\"21740d70fdf58058a46cfc4b81aaecc4\",{\"id\":\"block-21740d70fdf58058a46cfc4b81aaecc4\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Cross-Encoders:\",{\"children\":\"Cross-Encoders:\"}]}],[\"$\",\"$1\",\"1- These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.\",{\"children\":\" These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf0\"]}],[\"$\",\"$1\",\"3- Examples include Sentence Transformers, Flashrank, and BGE-M3.\",{\"children\":\" Examples include Sentence Transformers, Flashrank, and BGE-M3.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf1\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf5807490e3ef6ba727c883\",{\"id\":\"block-21740d70fdf5807490e3ef6ba727c883\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Multi-Vector Rerankers:\",{\"children\":\"Multi-Vector Rerankers:\"}]}],[\"$\",\"$1\",\"1- Models like ColBERT use a \\\"late interaction\\\" approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.\",{\"children\":\" Models like ColBERT use a \\\"late interaction\\\" approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf2\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58003b9c1f7a840f17ca1\",{\"id\":\"block-21740d70fdf58003b9c1f7a840f17ca1\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Fine-tuned LLM Rerankers:\",{\"children\":\"Fine-tuned LLM Rerankers:\"}]}],[\"$\",\"$1\",\"1- Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.\",{\"children\":\" Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf3\"]}],[\"$\",\"$1\",\"3- These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).\",{\"children\":\" These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf4\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf5807382e1cfca2e9abdb2\",{\"id\":\"block-21740d70fdf5807382e1cfca2e9abdb2\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-LLM as a Judge:\",{\"children\":\"LLM as a Judge:\"}]}],[\"$\",\"$1\",\"1- This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.\",{\"children\":\" This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lf5\"]}],[\"$\",\"$1\",\"3- While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.\",{\"children\":\" While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.\"}],\"$Lf6\",\"$Lf7\",\"$Lf8\"],\"$undefined\"]}],\"$Lf9\"]}]\n"])</script><script>self.__next_f.push([1,"61:[\"$\",\"p\",\"21740d70fdf5808da1edd71553e8d49c,21740d70fdf5808da1edd71553e8d49c\",{\"id\":\"block-21740d70fdf5808da1edd71553e8d49c\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Beyond simple relevance scoring, context reordering within the reranking process also plays a role. \",{\"children\":\"Beyond simple relevance scoring, context reordering within the reranking process also plays a role. \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-Inverted Context Ordering\",{\"children\":\"Inverted Context Ordering\"}]}],[\"$\",\"$1\",\"2- is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.\",{\"children\":\" is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lfa\"]}],[\"$\",\"$1\",\"4- This method has demonstrated a performance increase in correctness for multi-hop QA tasks.\",{\"children\":\" This method has demonstrated a performance increase in correctness for multi-hop QA tasks.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lfb\"]}],[\"$\",\"$1\",\"6- Other advanced approaches include \",{\"children\":\" Other advanced approaches include \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"7-Fusion-based Reranking\",{\"children\":\"Fusion-based Reranking\"}]}],[\"$\",\"$1\",\"8-, which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks \",{\"children\":\", which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lfc\"]}],[\"$\",\"$1\",\"10-, and \",{\"children\":\", and \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"11-Adaptive Reranking\",{\"children\":\"Adaptive Reranking\"}]}],[\"$\",\"$1\",\"12-, which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).\",{\"children\":\", which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lfd\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"62:[\"$\",\"p\",\"21740d70fdf5807d866be836369721de,21740d70fdf5807d866be836369721de\",{\"id\":\"block-21740d70fdf5807d866be836369721de\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-While reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where \\\"better\\\" relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the \",{\"children\":\"While reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where \\\"better\\\" relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-most\",{\"children\":\"most\"}]}],[\"$\",\"$1\",\"2- relevant context.\",{\"children\":\" relevant context.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lfe\"]}],[\"$\",\"$1\",\"4- However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.\",{\"children\":\" However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$Lff\"]}],[\"$\",\"$1\",\"6- This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system's usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.\",{\"children\":\" This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system's usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L100\"]}],[\"$\",\"$1\",\"8- This implies that the \\\"best\\\" reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM.\\n\",{\"children\":\" This implies that the \\\"best\\\" reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM.\\n\"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"9-\\nTable 1: Comparison of Key Chunking and Reordering Strategies in RAG\",{\"children\":\"\\nTable 1: Comparison of Key Chunking and Reordering Strategies in RAG\"}]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"63:[\"$\",\"div\",\"21740d70fdf58018b8d2ed528c1162a2,21740d70fdf58018b8d2ed528c1162a2\",{\"className\":\"notion-table__wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"notion-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"21740d70-fdf5-80e1-a5b0-d2ac98b3c690\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Strategy\",{\"children\":\"Strategy\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Description\",{\"children\":\"Description\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Primary Goal\",{\"children\":\"Primary Goal\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Impact on Multi-Step Inference\",{\"children\":\"Impact on Multi-Step Inference\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Advantages\",{\"children\":\"Advantages\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Disadvantages\",{\"children\":\"Disadvantages\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Relevant Snippets\",{\"children\":\"Relevant Snippets\"}]}]],\"$undefined\"]}]}]}]]}],[\"$\",\"tr\",\"21740d70-fdf5-8050-80a5-c3692e5eeb09\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Fixed Size Chunking\",{\"children\":\"Fixed Size Chunking\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Divides text into uniform segments (e.g., 500 tokens), often with overlap.\",{\"children\":\"Divides text into uniform segments (e.g., 500 tokens), often with overlap.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Efficiency, Simplicity\",{\"children\":\"Efficiency, Simplicity\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Can fragment context, hindering logical flow.\",{\"children\":\"Can fragment context, hindering logical flow.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Easy to implement, fast, consistent.\",{\"children\":\"Easy to implement, fast, consistent.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":\"$L101\"}],\"$L102\"]}],\"$L103\",\"$L104\",\"$L105\",\"$L106\",\"$L107\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"64:[\"$\",\"$1\",\"21740d70fdf5807c929cdc484058c538,21740d70fdf5807c929cdc484058c538\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5807c929cdc484058c538\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf5807c929cdc484058c538\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-4. Factors Influencing Multi-Step Inference Performance in RAG\",{\"children\":\"4. Factors Influencing Multi-Step Inference Performance in RAG\"}]}]],\"$undefined\"]}]]}]\n65:[\"$\",\"p\",\"21740d70fdf580f9821be4ec0e5fc9d4,21740d70fdf580f9821be4ec0e5fc9d4\",{\"id\":\"block-21740d70fdf580f9821be4ec0e5fc9d4\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Beyond the direct ordering of retrieved chunks, several other factors interact with the LLM's context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.\",{\"children\":\"Beyond the direct ordering of retrieved chunks, several other factors interact with the LLM's context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.\"}]],\"$undefined\"]}]\n66:[\"$\",\"$1\",\"21740d70fdf5805f9dc2d7b972b2041d,21740d70fdf5805f9dc2d7b972b2041d\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5805f9dc2d7b972b2041d\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf5805f9dc2d7b972b2041d\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-4.1 Positional Bias and the \\\"Lost-in-the-Middle\\\" Effect\",{\"children\":\"4.1 Positional Bias and the \\\"Lost-in-the-Middle\\\" Effect\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"67:[\"$\",\"p\",\"21740d70fdf5806b9a45ea2b164dfdd2,21740d70fdf5806b9a45ea2b164dfdd2\",{\"id\":\"block-21740d70fdf5806b9a45ea2b164dfdd2\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Positional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.\",{\"children\":\"Positional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L108\"]}],[\"$\",\"$1\",\"2- A specific manifestation of this is the \\\"lost-in-the-middle\\\" effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.\",{\"children\":\" A specific manifestation of this is the \\\"lost-in-the-middle\\\" effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L109\"]}],[\"$\",\"$1\",\"4- This bias can affect both the LLM's capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.\",{\"children\":\" This bias can affect both the LLM's capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10a\"]}],[\"$\",\"$1\",\"6- Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.\",{\"children\":\" Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10b\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"68:[\"$\",\"p\",\"21740d70fdf580d4ba66c5c4765273a0,21740d70fdf580d4ba66c5c4765273a0\",{\"id\":\"block-21740d70fdf580d4ba66c5c4765273a0\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-While earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.\",{\"children\":\"While earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10c\"]}],[\"$\",\"$1\",\"2- This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.\",{\"children\":\" This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10d\"]}],[\"$\",\"$1\",\"4- Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM's presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.\",{\"children\":\" Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM's presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10e\"]}],[\"$\",\"$1\",\"6- This is attributed to a \\\"contrastive effect\\\", where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.\",{\"children\":\" This is attributed to a \\\"contrastive effect\\\", where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L10f\"]}],[\"$\",\"$1\",\"8- Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a \\\"parametric bias\\\", relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.\",{\"children\":\" Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a \\\"parametric bias\\\", relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L110\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"69:[\"$\",\"p\",\"21740d70fdf580459646fcd5a2a23040,21740d70fdf580459646fcd5a2a23040\",{\"id\":\"block-21740d70fdf580459646fcd5a2a23040\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The \\\"lost-in-the-middle\\\" effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to \\\"trick\\\" the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the \",{\"children\":\"The \\\"lost-in-the-middle\\\" effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to \\\"trick\\\" the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-quality\",{\"children\":\"quality\"}]}],[\"$\",\"$1\",\"2- of retrieved content and enhancing the LLM's inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.\",{\"children\":\" of retrieved content and enhancing the LLM's inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L111\"]}],[\"$\",\"$1\",\"4- However, in practical RAG systems, where retrievers often fetch \",{\"children\":\" However, in practical RAG systems, where retrievers often fetch \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"5-both\",{\"children\":\"both\"}]}],[\"$\",\"$1\",\"6- relevant and highly distracting passages \",{\"children\":\" relevant and highly distracting passages \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L112\"]}],[\"$\",\"$1\",\"8-, the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the \\\"best\\\" chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.\",{\"children\":\", the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the \\\"best\\\" chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L113\"]}],[\"$\",\"$1\",\"10- This shifts the focus from \",{\"children\":\" This shifts the focus from \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"11-where\",{\"children\":\"where\"}]}],[\"$\",\"$1\",\"12- to place chunks to \",{\"children\":\" to place chunks to \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"13-what\",{\"children\":\"what\"}]}],[\"$\",\"$1\",\"14- chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.\",{\"children\":\" chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"6a:[\"$\",\"$1\",\"21740d70fdf5804cb009d3d8e8e002a5,21740d70fdf5804cb009d3d8e8e002a5\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf5804cb009d3d8e8e002a5\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf5804cb009d3d8e8e002a5\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-4.2 The Detrimental Impact of Irrelevant and Distracting Information\",{\"children\":\"4.2 The Detrimental Impact of Irrelevant and Distracting Information\"}]}]],\"$undefined\"]}]]}]\n6b:[\"$\",\"p\",\"21740d70fdf58004ad5fd2eeaf65e692,21740d70fdf58004ad5fd2eeaf65e692\",{\"id\":\"block-21740d70fdf58004ad5fd2eeaf65e692\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-A well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, \\\"distracting passages\\\", contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.\",{\"children\":\"A well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, \\\"distracting passages\\\", contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L114\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"6c:[\"$\",\"p\",\"21740d70fdf580ba9bdfd3bff3ab2e3e,21740d70fdf580ba9bdfd3bff3ab2e3e\",{\"id\":\"block-21740d70fdf580ba9bdfd3bff3ab2e3e\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.\",{\"children\":\"The presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L115\"]}],[\"$\",\"$1\",\"2- Studies have shown that \\\"hard distracting passages\\\", those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to \\\"weak\\\" ones, and this detrimental effect persists even in larger LLMs.\",{\"children\":\" Studies have shown that \\\"hard distracting passages\\\", those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to \\\"weak\\\" ones, and this detrimental effect persists even in larger LLMs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L116\"]}],[\"$\",\"$1\",\"4-\\nParadoxically, \\\"stronger\\\" retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver \",{\"children\":\"\\nParadoxically, \\\"stronger\\\" retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"5-more harmful distractors\",{\"children\":\"more harmful distractors\"}]}],[\"$\",\"$1\",\"6-.\",{\"children\":\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L117\"]}],[\"$\",\"$1\",\"8- This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.\",{\"children\":\" This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L118\"]}],[\"$\",\"$1\",\"10- Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.\",{\"children\":\" Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L119\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"6d:[\"$\",\"p\",\"21740d70fdf580f7923dca2a57ce5607,21740d70fdf580f7923dca2a57ce5607\",{\"id\":\"block-21740d70fdf580f7923dca2a57ce5607\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The very act of retrieval, especially with \\\"stronger\\\" retrievers, presents a \\\"double-edged sword.\\\" While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG's core purpose is to provide relevant external knowledge.\",{\"children\":\"The very act of retrieval, especially with \\\"stronger\\\" retrievers, presents a \\\"double-edged sword.\\\" While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG's core purpose is to provide relevant external knowledge.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11a\"]}],[\"$\",\"$1\",\"2- However, no retriever is perfect, and they often return irrelevant or \\\"distracting\\\" passages.\",{\"children\":\" However, no retriever is perfect, and they often return irrelevant or \\\"distracting\\\" passages.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11b\"]}],[\"$\",\"$1\",\"4- The critical observation here is that \",{\"children\":\" The critical observation here is that \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"5-stronger\",{\"children\":\"stronger\"}]}],[\"$\",\"$1\",\"6- retrievers, which are designed to find more relevant information, also tend to retrieve \",{\"children\":\" retrievers, which are designed to find more relevant information, also tend to retrieve \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"7-more harmful\",{\"children\":\"more harmful\"}]}],[\"$\",\"$1\",\"8- distracting passages.\",{\"children\":\" distracting passages.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11c\"]}],[\"$\",\"$1\",\"10- This creates a paradox: improving the retriever's primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for \\\"relevance\\\" (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the \",{\"children\":\" This creates a paradox: improving the retriever's primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for \\\"relevance\\\" (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"11-distracting effect\",{\"children\":\"distracting effect\"}]}],[\"$\",\"$1\",\"12- to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.\",{\"children\":\" to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"6e:[\"$\",\"$1\",\"21740d70fdf580e2a4f9c97d9434d4cb,21740d70fdf580e2a4f9c97d9434d4cb\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580e2a4f9c97d9434d4cb\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580e2a4f9c97d9434d4cb\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-4.3 Cognitive Load and Context Window Management\",{\"children\":\"4.3 Cognitive Load and Context Window Management\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"6f:[\"$\",\"p\",\"21740d70fdf580d28441d48edb42e729,21740d70fdf580d28441d48edb42e729\",{\"id\":\"block-21740d70fdf580d28441d48edb42e729\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Large Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.\",{\"children\":\"Large Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11d\"]}],[\"$\",\"$1\",\"2- The concept of \\\"cognitive load\\\", analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).\",{\"children\":\" The concept of \\\"cognitive load\\\", analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11e\"]}],[\"$\",\"$1\",\"4- For LLMs, the inherent \\\"content complexity\\\" of the input is a dominant factor influencing their processing efficiency.\",{\"children\":\" For LLMs, the inherent \\\"content complexity\\\" of the input is a dominant factor influencing their processing efficiency.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L11f\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"70:[\"$\",\"p\",\"21740d70fdf580139869c3006b44178b,21740d70fdf580139869c3006b44178b\",{\"id\":\"block-21740d70fdf580139869c3006b44178b\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Presenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its \\\"cognitive load\\\".\",{\"children\":\"Presenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its \\\"cognitive load\\\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L120\"]}],[\"$\",\"$1\",\"2- This overload can lead to reduced selection accuracy and an increase in hallucinations.\",{\"children\":\" This overload can lead to reduced selection accuracy and an increase in hallucinations.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L121\"]}],[\"$\",\"$1\",\"4- Conversely, supplying \",{\"children\":\" Conversely, supplying \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"5-only\",{\"children\":\"only\"}]}],[\"$\",\"$1\",\"6- the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of \\\"prompt bloat\\\" directly lowers the LLM's cognitive load.\",{\"children\":\" the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of \\\"prompt bloat\\\" directly lowers the LLM's cognitive load.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L122\"]}],[\"$\",\"$1\",\"8- By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM's decision-making capabilities are markedly improved.\",{\"children\":\" By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM's decision-making capabilities are markedly improved.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L123\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"71:[\"$\",\"p\",\"21740d70fdf58011839fd05b5333a270,21740d70fdf58011839fd05b5333a270\",{\"id\":\"block-21740d70fdf58011839fd05b5333a270\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-While long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.\",{\"children\":\"While long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L124\"]}],[\"$\",\"$1\",\"2- The key lies in identifying the \\\"sweet spot\\\" for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.\",{\"children\":\" The key lies in identifying the \\\"sweet spot\\\" for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L125\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"72:[\"$\",\"p\",\"21740d70fdf5809dbe10fa8f1c49222f,21740d70fdf5809dbe10fa8f1c49222f\",{\"id\":\"block-21740d70fdf5809dbe10fa8f1c49222f\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The concept of \\\"cognitive load\\\" in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the \",{\"children\":\"The concept of \\\"cognitive load\\\" in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-quality and conciseness\",{\"children\":\"quality and conciseness\"}]}],[\"$\",\"$1\",\"2- of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts \",{\"children\":\" of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L126\"]}],[\"$\",\"$1\",\"4-, the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.\",{\"children\":\", the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L127\"]}],[\"$\",\"$1\",\"6- This is framed in terms of \\\"cognitive load\\\".\",{\"children\":\" This is framed in terms of \\\"cognitive load\\\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L128\"]}],[\"$\",\"$1\",\"8- If the LLM is forced to \\\"sift through hundreds of distractors\\\" \",{\"children\":\" If the LLM is forced to \\\"sift through hundreds of distractors\\\" \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L129\"]}],[\"$\",\"$1\",\"10-, it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about \",{\"children\":\", it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"11-what\",{\"children\":\"what\"}]}],[\"$\",\"$1\",\"12- to retrieve, but \",{\"children\":\" to retrieve, but \"}],[\"$\",\"em\",\"1-i\",{\"children\":[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"13-how much\",{\"children\":\"how much\"}]}]}],[\"$\",\"$1\",\"14- and \",{\"children\":\" and \"}],[\"$\",\"em\",\"1-i\",{\"children\":[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"15-how clean\",{\"children\":\"how clean\"}]}]}],[\"$\",\"$1\",\"16- that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance.\\n\",{\"children\":\" that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance.\\n\"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"17-\\nTable 2: Factors Affecting LLM Performance in Long Contexts\",{\"children\":\"\\nTable 2: Factors Affecting LLM Performance in Long Contexts\"}]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"73:[\"$\",\"div\",\"21740d70fdf580acad63f4eef3953f6b,21740d70fdf580acad63f4eef3953f6b\",{\"className\":\"notion-table__wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"notion-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"21740d70-fdf5-80cb-bd66-d2a097e2ea7c\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"CVMg\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Factor\",{\"children\":\"Factor\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"xXUs\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Description\",{\"children\":\"Description\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MdJE\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Impact on Multi-Step Inference\",{\"children\":\"Impact on Multi-Step Inference\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"sWuZ\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Interaction with Chunk Order\",{\"children\":\"Interaction with Chunk Order\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\";h~{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Mitigation Strategies\",{\"children\":\"Mitigation Strategies\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"TSJy\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Relevant Snippets\",{\"children\":\"Relevant Snippets\"}]}]],\"$undefined\"]}]}]}]]}],[\"$\",\"tr\",\"21740d70-fdf5-80b5-8848-d3af96845184\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"CVMg\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Positional Bias\",{\"children\":\"Positional Bias\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"xXUs\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-LLMs weigh information differently based on its position (e.g., \\\"lost-in-the-middle\\\" effect).\",{\"children\":\"LLMs weigh information differently based on its position (e.g., \\\"lost-in-the-middle\\\" effect).\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MdJE\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Can cause LLMs to ignore relevant info or be misled by distractors in middle positions.\",{\"children\":\"Can cause LLMs to ignore relevant info or be misled by distractors in middle positions.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"sWuZ\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.\",{\"children\":\"Reordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\";h~{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":\"$L12a\"}]}]}],\"$L12b\"]}],\"$L12c\",\"$L12d\",\"$L12e\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"74:[\"$\",\"$1\",\"21740d70fdf580e4a606e6f4424b31e5,21740d70fdf580e4a606e6f4424b31e5\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580e4a606e6f4424b31e5\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf580e4a606e6f4424b31e5\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-5. Empirical Evidence and Performance Analysis\",{\"children\":\"5. Empirical Evidence and Performance Analysis\"}]}]],\"$undefined\"]}]]}]\n75:[\"$\",\"p\",\"21740d70fdf58093a210cb9d25926543,21740d70fdf58093a210cb9d25926543\",{\"id\":\"block-21740d70fdf58093a210cb9d25926543\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Empirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.\",{\"children\":\"Empirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.\"}]],\"$undefined\"]}]\n76:[\"$\",\"$1\",\"21740d70fdf580ad845cdef42ba8a4e7,21740d70fdf580ad845cdef42ba8a4e7\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580ad845cdef42ba8a4e7\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580ad845cdef42ba8a4e7\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-5.1 Case Studies on Chunk Order and Multi-Step Question Answering\",{\"children\":\"5.1 Case Studies on Chunk Order and Multi-Step Question Answering\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"77:[\"$\",\"p\",\"21740d70fdf580948a38d453a10fe70d,21740d70fdf580948a38d453a10fe70d\",{\"id\":\"block-21740d70fdf580948a38d453a10fe70d\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Comparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including Bench, QuALITY, and NarrativeQA.\",{\"children\":\"Comparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including Bench, QuALITY, and NarrativeQA.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L12f\"]}],[\"$\",\"$1\",\"2- This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.\",{\"children\":\" This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L130\"]}],[\"$\",\"$1\",\"4- For example, on the Bench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.\",{\"children\":\" For example, on the Bench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L131\"]}],[\"$\",\"$1\",\"6- This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs' multi-step reasoning capabilities are profoundly tied to the \\n\",{\"children\":\" This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs' multi-step reasoning capabilities are profoundly tied to the \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"7-narrative and structural coherence\",{\"children\":\"narrative and structural coherence\"}]}],[\"$\",\"$1\",\"8- of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.\",{\"children\":\" of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"78:[\"$\",\"p\",\"21740d70fdf58044bc06dbd3566e29b3,21740d70fdf58044bc06dbd3566e29b3\",{\"id\":\"block-21740d70fdf58044bc06dbd3566e29b3\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Furthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.\",{\"children\":\"Furthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L132\"]}],[\"$\",\"$1\",\"2- This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\",{\"children\":\" This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L133\"]}],[\"$\",\"$1\",\"4- However, there is a \\\"sweet spot\\\" for context length: DOS RAG's performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).\",{\"children\":\" However, there is a \\\"sweet spot\\\" for context length: DOS RAG's performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L134\"]}],[\"$\",\"$1\",\"6- This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.\",{\"children\":\" This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"79:[\"$\",\"p\",\"21740d70fdf580c58320e68a6859b88f,21740d70fdf580c58320e68a6859b88f\",{\"id\":\"block-21740d70fdf580c58320e68a6859b88f\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Research on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.\",{\"children\":\"Research on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L135\"]}],[\"$\",\"$1\",\"2- LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.\",{\"children\":\" LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L136\"]}],[\"$\",\"$1\",\"4- In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in \",{\"children\":\" In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"5-correctness\",{\"children\":\"correctness\"}]}],[\"$\",\"$1\",\"6-.\",{\"children\":\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L137\"]}],[\"$\",\"$1\",\"8- When rerankers like BGE-M3 are combined with higher \",{\"children\":\" When rerankers like BGE-M3 are combined with higher \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"9-retrieval@k\",{\"children\":\"retrieval@k\"}]}],[\"$\",\"$1\",\"10- values, more \\\"gold documents\\\" (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.\",{\"children\":\" values, more \\\"gold documents\\\" (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L138\"]}],[\"$\",\"$1\",\"12- However, increasing \",{\"children\":\" However, increasing \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"13-rerank@k\",{\"children\":\"rerank@k\"}]}],[\"$\",\"$1\",\"14- with a fixed \",{\"children\":\" with a fixed \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"15-retrieval@k\",{\"children\":\"retrieval@k\"}]}],[\"$\",\"$1\",\"16- can introduce higher variation in \",{\"children\":\" can introduce higher variation in \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"17-correctness\",{\"children\":\"correctness\"}]}],[\"$\",\"$1\",\"18- scores, ranging from 1% to 25%.\",{\"children\":\" scores, ranging from 1% to 25%.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L139\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"7a:[\"$\",\"$1\",\"21740d70fdf580889f7bc0535e95d69a,21740d70fdf580889f7bc0535e95d69a\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580889f7bc0535e95d69a\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580889f7bc0535e95d69a\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-5.2 Evaluation Benchmarks and Metrics\",{\"children\":\"5.2 Evaluation Benchmarks and Metrics\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"7b:[\"$\",\"p\",\"21740d70fdf5806baa0cd10113df6c6c,21740d70fdf5806baa0cd10113df6c6c\",{\"id\":\"block-21740d70fdf5806baa0cd10113df6c6c\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\",{\"children\":\"The field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13a\"]}],[\"$\",\"$1\",\"2- This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.\",{\"children\":\" This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13b\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"7c:[\"$\",\"p\",\"21740d70fdf580cf802debf13bd62d5a,21740d70fdf580cf802debf13bd62d5a\",{\"id\":\"block-21740d70fdf580cf802debf13bd62d5a\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Another important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.\",{\"children\":\"Another important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13c\"]}],[\"$\",\"$1\",\"2- Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.\",{\"children\":\" Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13d\"]}],[\"$\",\"$1\",\"4- DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.\",{\"children\":\" DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13e\"]}],[\"$\",\"$1\",\"6- For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval \",{\"children\":\" For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"7-accuracy\",{\"children\":\"accuracy\"}]}],[\"$\",\"$1\",\"8-.\",{\"children\":\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L13f\"]}],[\"$\",\"$1\",\"10- Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.\",{\"children\":\" Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L140\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"7d:[\"$\",\"p\",\"21740d70fdf580d5978af3c7e30f6305,21740d70fdf580d5978af3c7e30f6305\",{\"id\":\"block-21740d70fdf580d5978af3c7e30f6305\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-These specialized benchmarks employ a variety of evaluation metrics. \",{\"children\":\"These specialized benchmarks employ a variety of evaluation metrics. \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"1-Accuracy\",{\"children\":\"Accuracy\"}]}],[\"$\",\"$1\",\"2- is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.\",{\"children\":\" is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L141\"]}],[\"$\",\"$1\",\"4- For more nuanced assessments, metrics like \",{\"children\":\" For more nuanced assessments, metrics like \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"5-F1\",{\"children\":\"F1\"}]}],[\"$\",\"$1\",\"6-, \",{\"children\":\", \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"7-BLEU-1\",{\"children\":\"BLEU-1\"}]}],[\"$\",\"$1\",\"8-, \",{\"children\":\", \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"9-BLEU-4\",{\"children\":\"BLEU-4\"}]}],[\"$\",\"$1\",\"10-, \",{\"children\":\", \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"11-ROUGE-L\",{\"children\":\"ROUGE-L\"}]}],[\"$\",\"$1\",\"12-, and \",{\"children\":\", and \"}],[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"13-METEOR\",{\"children\":\"METEOR\"}]}],[\"$\",\"$1\",\"14- are employed, particularly for tasks like NarrativeQA.\",{\"children\":\" are employed, particularly for tasks like NarrativeQA.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L142\"]}],[\"$\",\"$1\",\"16- In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.\",{\"children\":\" In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L143\"]}],[\"$\",\"$1\",\"18- The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks \",{\"children\":\" The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L144\"]}],[\"$\",\"$1\",\"20-, step-by-step procedure following \",{\"children\":\", step-by-step procedure following \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L145\"]}],[\"$\",\"$1\",\"22-, and multi-hop questions \",{\"children\":\", and multi-hop questions \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L146\"]}],[\"$\",\"$1\",\"24-, demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like \\\"\",{\"children\":\", demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like \\\"\"}],\"$L147\",\"$L148\",\"$L149\",\"$L14a\"],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"7e:[\"$\",\"p\",\"21740d70fdf580c2b190c8654f824624,21740d70fdf580c2b190c8654f824624\",{\"id\":\"block-21740d70fdf580c2b190c8654f824624\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-\\nTable 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation\",{\"children\":\"\\nTable 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation\"}]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"7f:[\"$\",\"div\",\"21740d70fdf58090ae04ef67d7ee79bd,21740d70fdf58090ae04ef67d7ee79bd\",{\"className\":\"notion-table__wrapper\",\"children\":[\"$\",\"table\",null,{\"className\":\"notion-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",\"21740d70-fdf5-80c7-9fdc-d8f4809988c3\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Benchmark\",{\"children\":\"Benchmark\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Primary Focus\",{\"children\":\"Primary Focus\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Key Features Relevant to Chunk Order/Multi-Step QA\",{\"children\":\"Key Features Relevant to Chunk Order/Multi-Step QA\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Key Findings Related to Chunk Order\",{\"children\":\"Key Findings Related to Chunk Order\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Relevant Snippets\",{\"children\":\"Relevant Snippets\"}]}]],\"$undefined\"]}]}]}]]}],[\"$\",\"tr\",\"21740d70-fdf5-80af-ac80-ed2acd69217b\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-MTI Bench\",{\"children\":\"MTI Bench\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Multi-Task Inference (sequential \u0026 non-sequential sub-tasks)\",{\"children\":\"Multi-Task Inference (sequential \u0026 non-sequential sub-tasks)\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Evaluates LLMs' ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.\",{\"children\":\"Evaluates LLMs' ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Stronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.\",{\"children\":\"Stronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-10\",{\"children\":\"10\"}]],\"$undefined\"]}]}]}]]}],[\"$\",\"tr\",\"21740d70-fdf5-8054-a514-cc91e8cb3ec0\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[\"$L14b\",\"$L14c\",\"$L14d\",\"$L14e\",\"$L14f\"]}],\"$L150\",\"$L151\",\"$L152\",\"$L153\",\"$L154\",\"$L155\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"80:[\"$\",\"$1\",\"21740d70fdf580e0a67ddf16d23ac95b,21740d70fdf580e0a67ddf16d23ac95b\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580e0a67ddf16d23ac95b\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf580e0a67ddf16d23ac95b\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\",{\"children\":\"6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\"}]}]],\"$undefined\"]}]]}]\n81:[\"$\",\"p\",\"21740d70fdf580bdab4cf02a41144b18,21740d70fdf580bdab4cf02a41144b18\",{\"id\":\"block-21740d70fdf580bdab4cf02a41144b18\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Translating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.\",{\"children\":\"Translating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.\"}]],\"$undefined\"]}]\n82:[\"$\",\"$1\",\"21740d70fdf58085858fd4791e4f101b,21740d70fdf58085858fd4791e4f101b\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf58085858fd4791e4f101b\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf58085858fd4791e4f101b\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-6.1 Best Practices for Chunking and Reordering\",{\"children\":\"6.1 Best Practices for Chunking and Reordering\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"83:[\"$\",\"p\",\"21740d70fdf58011a43df811886b1920,21740d70fdf58011a43df811886b1920\",{\"id\":\"block-21740d70fdf58011a43df811886b1920\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-To optimize chunk retrieval sequence, a primary focus must be placed on \",{\"children\":\"To optimize chunk retrieval sequence, a primary focus must be placed on \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-prioritizing contextual coherence\",{\"children\":\"prioritizing contextual coherence\"}]}],[\"$\",\"$1\",\"2-. For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.\",{\"children\":\". For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L156\"]}],[\"$\",\"$1\",\"4- Given its consistent outperformance across various benchmarks, \",{\"children\":\" Given its consistent outperformance across various benchmarks, \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-adopting DOS RAG as a baseline\",{\"children\":\"adopting DOS RAG as a baseline\"}]}],[\"$\",\"$1\",\"6- is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.\",{\"children\":\" is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L157\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"84:[\"$\",\"p\",\"21740d70fdf5800bb60debab93527a57,21740d70fdf5800bb60debab93527a57\",{\"id\":\"block-21740d70fdf5800bb60debab93527a57\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-While initial retrieval provides a set of relevant chunks, a \",{\"children\":\"While initial retrieval provides a set of relevant chunks, a \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-strategic reranking\",{\"children\":\"strategic reranking\"}]}],[\"$\",\"$1\",\"2- step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.\",{\"children\":\" step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L158\"]}],[\"$\",\"$1\",\"4- Furthermore, implementing \",{\"children\":\" Furthermore, implementing \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-inverted context ordering\",{\"children\":\"inverted context ordering\"}]}],[\"$\",\"$1\",\"6-, where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.\",{\"children\":\", where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L159\"]}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"85:[\"$\",\"p\",\"21740d70fdf58049bce1f1a47e2d8051,21740d70fdf58049bce1f1a47e2d8051\",{\"id\":\"block-21740d70fdf58049bce1f1a47e2d8051\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Optimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure \",{\"children\":\"Optimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15a\"]}],[\"$\",\"$1\",\"2-, effective reranking can still improve the \\n\",{\"children\":\", effective reranking can still improve the \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"3-selection\",{\"children\":\"selection\"}]}],[\"$\",\"$1\",\"4- of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).\",{\"children\":\" of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15b\"]}],[\"$\",\"$1\",\"6- This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.\",{\"children\":\" This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"86:[\"$\",\"$1\",\"21740d70fdf580e49c36f4912bd6ac93,21740d70fdf580e49c36f4912bd6ac93\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580e49c36f4912bd6ac93\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580e49c36f4912bd6ac93\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-6.2 Strategies for Mitigating Positional Bias and Distraction\",{\"children\":\"6.2 Strategies for Mitigating Positional Bias and Distraction\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"87:[\"$\",\"p\",\"21740d70fdf580e895f8f04cd3bdbc49,21740d70fdf580e895f8f04cd3bdbc49\",{\"id\":\"block-21740d70fdf580e895f8f04cd3bdbc49\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-To effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing \",{\"children\":\"To effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages\",{\"children\":\"retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages\"}]}],[\"$\",\"$1\",\"2-.\",{\"children\":\".\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15c\"]}],[\"$\",\"$1\",\"4- This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, \",{\"children\":\" This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"5-robust LLM fine-tuning\",{\"children\":\"robust LLM fine-tuning\"}]}],[\"$\",\"$1\",\"6- with carefully selected \\\"hard distracting passages\\\" can significantly increase the LLM's accuracy and resilience against noise.\",{\"children\":\" with carefully selected \\\"hard distracting passages\\\" can significantly increase the LLM's accuracy and resilience against noise.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15d\"]}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"8- \",{\"children\":\" \"}]}],[\"$\",\"$1\",\"9-Third, implementing \",{\"children\":\"Third, implementing \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"10-LLM-driven chunk filtering\",{\"children\":\"LLM-driven chunk filtering\"}]}],[\"$\",\"$1\",\"11- (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.\",{\"children\":\" (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15e\"]}],[\"$\",\"$1\",\"13- Fourth, for complex multi-step queries, \",{\"children\":\" Fourth, for complex multi-step queries, \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"14-query rewriting or decomposition\",{\"children\":\"query rewriting or decomposition\"}]}],[\"$\",\"$1\",\"15- into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.\",{\"children\":\" into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L15f\"]}],[\"$\",\"$1\",\"17- Finally, active \",{\"children\":\" Finally, active \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"18-context window management\",{\"children\":\"context window management\"}]}],[\"$\",\"$1\",\"19- is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.\",{\"children\":\" is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L160\"]}],[\"$\",\"$1\",\"21- Identifying the \\\"sweet spot\\\" for context length, where recall is maximized without introducing excessive noise, is also paramount.\",{\"children\":\" Identifying the \\\"sweet spot\\\" for context length, where recall is maximized without introducing excessive noise, is also paramount.\"}],\"$L161\"],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"88:[\"$\",\"p\",\"21740d70fdf58025a2fec1c13e30b145,21740d70fdf58025a2fec1c13e30b145\",{\"id\":\"block-21740d70fdf58025a2fec1c13e30b145\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Mitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the \",{\"children\":\"Mitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-quality and focus\",{\"children\":\"quality and focus\"}]}],[\"$\",\"$1\",\"2- of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.\",{\"children\":\" of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L162\"]}],[\"$\",\"$1\",\"4- Simply reordering \",{\"children\":\" Simply reordering \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"5-after\",{\"children\":\"after\"}]}],[\"$\",\"$1\",\"6- retrieval is often insufficient to address these issues.\",{\"children\":\" retrieval is often insufficient to address these issues.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L163\"]}],[\"$\",\"$1\",\"8- Therefore, the solution must involve proactive measures. This includes improving the \",{\"children\":\" Therefore, the solution must involve proactive measures. This includes improving the \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"9-initial retrieval\",{\"children\":\"initial retrieval\"}]}],[\"$\",\"$1\",\"10- to be less prone to fetching distractors, and then employing \",{\"children\":\" to be less prone to fetching distractors, and then employing \"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"11-strong filtering\",{\"children\":\"strong filtering\"}]}],[\"$\",\"$1\",\"12- (like ChunkRAG) to eliminate noise before it ever reaches the LLM's context window.\",{\"children\":\" (like ChunkRAG) to eliminate noise before it ever reaches the LLM's context window.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L164\"]}],[\"$\",\"$1\",\"14- Furthermore, making the LLM itself more robust through fine-tuning with challenging examples \",{\"children\":\" Furthermore, making the LLM itself more robust through fine-tuning with challenging examples \"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L165\"]}],[\"$\",\"$1\",\"16- creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.\",{\"children\":\" creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"89:[\"$\",\"$1\",\"21740d70fdf580b3bb55f18aef19ec66,21740d70fdf580b3bb55f18aef19ec66\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf580b3bb55f18aef19ec66\"}],[\"$\",\"h2\",null,{\"id\":\"block-21740d70fdf580b3bb55f18aef19ec66\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-6.3 Advanced Techniques for Multi-Hop Reasoning\",{\"children\":\"6.3 Advanced Techniques for Multi-Hop Reasoning\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"8a:[\"$\",\"p\",\"21740d70fdf580f4b7a0d4c4313da49c,21740d70fdf580f4b7a0d4c4313da49c\",{\"id\":\"block-21740d70fdf580f4b7a0d4c4313da49c\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Addressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, \",{\"children\":\"Addressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"1-iterative retrieval\",{\"children\":\"iterative retrieval\"}]}],[\"$\",\"$1\",\"2- becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.\",{\"children\":\" becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L166\"]}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"4- Adaptive retrieval\",{\"children\":\" Adaptive retrieval\"}]}],[\"$\",\"$1\",\"5- mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.\",{\"children\":\" mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L167\"]}],[\"$\",\"$1\",\"7- The integration of structured knowledge, such as \",{\"children\":\" The integration of structured knowledge, such as \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"8-graph-based RAG\",{\"children\":\"graph-based RAG\"}]}],[\"$\",\"$1\",\"9- (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.\",{\"children\":\" (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L168\"]}],[\"$\",\"$1\",\"11- This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of \",{\"children\":\" This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of \"}],[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"12-prompt-based reasoning chains\",{\"children\":\"prompt-based reasoning chains\"}]}],[\"$\",\"$1\",\"13- like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM's reasoning process step-by-step, enhancing its ability to perform complex deductions.\",{\"children\":\" like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM's reasoning process step-by-step, enhancing its ability to perform complex deductions.\"}],\"$L169\",\"$L16a\"],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"8b:[\"$\",\"$1\",\"21740d70fdf58057bc2bc6c92d18067c,21740d70fdf58057bc2bc6c92d18067c\",{\"children\":[[\"$\",\"span\",null,{\"className\":\"notion-heading__anchor\",\"id\":\"21740d70fdf58057bc2bc6c92d18067c\"}],[\"$\",\"h1\",null,{\"id\":\"block-21740d70fdf58057bc2bc6c92d18067c\",\"className\":\"notion-heading notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-7. Conclusion and Future Directions\",{\"children\":\"7. Conclusion and Future Directions\"}]}]],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"8c:[\"$\",\"p\",\"21740d70fdf580c9a5c9cbbe9a3723c1,21740d70fdf580c9a5c9cbbe9a3723c1\",{\"id\":\"block-21740d70fdf580c9a5c9cbbe9a3723c1\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model's multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.\",{\"children\":\"The analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model's multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"8d:[\"$\",\"p\",\"21740d70fdf580059d5be7ecaceed316,21740d70fdf580059d5be7ecaceed316\",{\"id\":\"block-21740d70fdf580059d5be7ecaceed316\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-The implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy.\",{\"children\":\"The implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy.\"}]],\"$undefined\"]}]\n8e:[\"$\",\"p\",\"21740d70fdf580ca8329d34ae124a4ac,21740d70fdf580ca8329d34ae124a4ac\",{\"id\":\"block-21740d70fdf580ca8329d34ae124a4ac\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Future research and development should focus on several promising directions:\",{\"children\":\"Future research and development should focus on several promising directions:\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"8f:[\"$\",\"ul\",\"21740d70fdf580bb9f60cd579f054e8f,21740d70fdf580bb9f60cd579f054e8f\",{\"className\":\"notion-bulleted-list\",\"children\":[[\"$\",\"li\",\"21740d70fdf580bb9f60cd579f054e8f\",{\"id\":\"block-21740d70fdf580bb9f60cd579f054e8f\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Adaptive Retrieval Architectures:\",{\"children\":\"Adaptive Retrieval Architectures:\"}]}],[\"$\",\"$1\",\"1- Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.\",{\"children\":\" Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L16b\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58072ab0fc215757eb48b\",{\"id\":\"block-21740d70fdf58072ab0fc215757eb48b\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Real-time Retrieval Integration:\",{\"children\":\"Real-time Retrieval Integration:\"}]}],[\"$\",\"$1\",\"1- Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.\",{\"children\":\" Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L16c\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf5802cb5a5e4c8136d32b4\",{\"id\":\"block-21740d70fdf5802cb5a5e4c8136d32b4\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Structured Reasoning over Multi-Hop Evidence:\",{\"children\":\"Structured Reasoning over Multi-Hop Evidence:\"}]}],[\"$\",\"$1\",\"1- Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.\",{\"children\":\" Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L16d\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580438856f1a03608dfd0\",{\"id\":\"block-21740d70fdf580438856f1a03608dfd0\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Robustness to Adversarial Inputs:\",{\"children\":\"Robustness to Adversarial Inputs:\"}]}],[\"$\",\"$1\",\"1- Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.\",{\"children\":\" Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],\"$L16e\"]}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58012ab78f973a8a870ee\",{\"id\":\"block-21740d70fdf58012ab78f973a8a870ee\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Cross-Modal and Multi-Lingual RAG:\",{\"children\":\"Cross-Modal and Multi-Lingual RAG:\"}]}],[\"$\",\"$1\",\"1- Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.\",{\"children\":\" Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.\"}],\"$L16f\"],\"$undefined\"]}],\"$L170\"]}]\n"])</script><script>self.__next_f.push([1,"90:[\"$\",\"p\",\"21740d70fdf580aa9d37d06095234106,21740d70fdf580aa9d37d06095234106\",{\"id\":\"block-21740d70fdf580aa9d37d06095234106\",\"className\":\"notion-text notion-text__content notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-These future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.\",{\"children\":\"These future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.\"}]],\"$undefined\"]}]\n"])</script><script>self.__next_f.push([1,"91:[\"$\",\"$L171\",\"21740d70fdf580388756d009905ff072,21740d70fdf580388756d009905ff072\",{\"id\":\"21740d70fdf580388756d009905ff072\",\"children\":[[\"$\",\"ol\",\"21740d70fdf580cbb88acbb7fa5d007e,21740d70fdf580cbb88acbb7fa5d007e\",{\"start\":\"$undefined\",\"type\":\"1\",\"className\":\"notion-numbered-list\",\"children\":[[\"$\",\"li\",\"21740d70fdf580cbb88acbb7fa5d007e\",{\"id\":\"block-21740d70fdf580cbb88acbb7fa5d007e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-IBM. (n.d.). \\n\",{\"children\":\"IBM. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-What Are Large Language Models (LLMs)?\",{\"children\":\"What Are Large Language Models (LLMs)?\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.ibm.com/think/topics/large-language-models#:~:text=LLMs%20consist%20of%20multiple%20layers,specific%20parts%20of%20data%20sets.\",{\"children\":\". Retrieved from https://www.ibm.com/think/topics/large-language-models#:~:text=LLMs%20consist%20of%20multiple%20layers,specific%20parts%20of%20data%20sets.\"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580a2af76f5bd3e1d8ed2\",{\"id\":\"block-21740d70fdf580a2af76f5bd3e1d8ed2\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Kasneci, E., et al. (2023). \\n\",{\"children\":\"Kasneci, E., et al. (2023). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Editorial  The Use of Large Language Models in Science: Opportunities and Challenges.\",{\"children\":\"Editorial  The Use of Large Language Models in Science: Opportunities and Challenges.\"}]}],[\"$\",\"$1\",\"2- PMC. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC10485814/.\",{\"children\":\" PMC. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC10485814/.\"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580dcba69e2d2b53e2344\",{\"id\":\"block-21740d70fdf580dcba69e2d2b53e2344\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-IBM. (2023, November 2). \\n\",{\"children\":\"IBM. (2023, November 2). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-What are Large Language Models (LLMs)?\",{\"children\":\"What are Large Language Models (LLMs)?\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.ibm.com/think/topics/large-language-models.\",{\"children\":\". Retrieved from https://www.ibm.com/think/topics/large-language-models.\"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58009bdead36180daf5d1\",{\"id\":\"block-21740d70fdf58009bdead36180daf5d1\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Google Cloud. (n.d.). \\n\",{\"children\":\"Google Cloud. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-What is Retrieval-Augmented Generation (RAG)?\",{\"children\":\"What is Retrieval-Augmented Generation (RAG)?\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://cloud.google.com/use-cases/retrieval-augmented-generation.\",{\"children\":\". Retrieved from https://cloud.google.com/use-cases/retrieval-augmented-generation.\"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf580ff9eb4c885d46d800a\",{\"id\":\"block-21740d70fdf580ff9eb4c885d46d800a\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Zhang, Y., et al. (2025). \\n\",{\"children\":\"Zhang, Y., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Empowering LLMs with Logical Reasoning: A Comprehensive Survey\",{\"children\":\"Empowering LLMs with Logical Reasoning: A Comprehensive Survey\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2502.15652v2.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2502.15652v2.\"}]],\"$undefined\"]}],[\"$\",\"li\",\"21740d70fdf58036811defbe9753cd2e\",{\"id\":\"block-21740d70fdf58036811defbe9753cd2e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Zhang, Y., et al. (2025). \\n\",{\"children\":\"Zhang, Y., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Empowering LLMs with Logical Reasoning: A Comprehensive Survey\",{\"children\":\"Empowering LLMs with Logical Reasoning: A Comprehensive Survey\"}]}],\"$L172\"],\"$undefined\"]}],\"$L173\",\"$L174\",\"$L175\",\"$L176\",\"$L177\",\"$L178\",\"$L179\",\"$L17a\",\"$L17b\",\"$L17c\",\"$L17d\",\"$L17e\",\"$L17f\",\"$L180\",\"$L181\",\"$L182\",\"$L183\",\"$L184\",\"$L185\",\"$L186\",\"$L187\",\"$L188\",\"$L189\",\"$L18a\",\"$L18b\",\"$L18c\",\"$L18d\",\"$L18e\",\"$L18f\",\"$L190\",\"$L191\",\"$L192\",\"$L193\",\"$L194\"]}]],\"hasContent\":true,\"parentId\":\"blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models\",\"title\":\"$6:props:records:block:21740d70fdf580388756d009905ff072:title\",\"updatedAt\":1750303136567,\"type\":\"toggle\",\"subType\":\"header\",\"depth\":1,\"color\":null}]\n"])</script><script>self.__next_f.push([1,"92:[\"$\",\"div\",\"21740d70fdf580fe8ae4f3d4e38c4ec2,21740d70fdf580fe8ae4f3d4e38c4ec2\",{\"id\":\"block-21740d70fdf580fe8ae4f3d4e38c4ec2\",\"className\":\"notion-text\"}]\n"])</script><script>self.__next_f.push([1,"196:I[189146,[\"/_next/static/chunks/f051bbd12aec0cc1.js\",\"/_next/static/chunks/81e796a7b8c3a175.js\",\"/_next/static/chunks/547a8eca1774889f.js\",\"/_next/static/chunks/d0383f817159b1cf.js\",\"/_next/static/chunks/c020afdb26b53a60.js\",\"/_next/static/chunks/7f22801e85c972ca.js\",\"/_next/static/chunks/1b70408e1ee0ede3.js\",\"/_next/static/chunks/ee5c4fc589f91413.js\",\"/_next/static/chunks/8d3945c9ea1274d1.js\"],\"Equation\"]\n195:T4ef,M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z99:[\"$\",\"$L97\",\"https://discord.com/users/_jinnkunn\",{\"uri\":\"https://discord.com/users/_jinnkunn\",\"children\":[\"$\",\"$1\",\"discord\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":[88246]}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"width\":24,\"height\":24,\"children\":[[\"$\",\"title\",null,{\"children\":\"Discord\"}],[\"$\",\"path\",null,{\"d\":\"$195\"}]]}]]}]}]\n9a:[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580c4a559c3eff45d8257\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-3.1 Impact of Context Order on LLM Performance\",{\"children\":\"3.1 Impact of Context Order on LLM Performance\"}]],\"$undefined\"]}]}]\n9b:[\"$\",\"li\",\"21740d70fdf580d8a90af2ad47b882d5\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580d8a90af2ad47b882d5\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-3.2 Document's Original Structure (DOS RAG) and its Benefits\",{\"children\":\"3.2 Document's Original Structure (DOS RAG) and its Benefits\"}]],\"$undefined\"]}]}]}]\n9c:[\"$\",\"li\",\"21740d70fdf5808f9238d67cfb9d9d82\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5808f9238d67cfb9d9d82\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-3.3 Reranking Strategies and Context Reordering\",{\"children\":\"3.3 Reranking Strategies and Context Reordering\"}]],\"$undefined\"]}]}]}]\n9d:[\"$\",\"li\",\"21740d70fdf5807c929cdc484058c538\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5807c929cdc484058c538\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-4. Factors Influencing Multi-Step Inference Performance in RAG\",{\"children\":\"4. Factors Influencing Multi-Step Inference Performance in RAG\"}]],\"$undefined\"]}]}]}]\n9e:[\"$\",\"li\",\"21740d70fdf5805f9dc2d7b972b2041d\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5805f9dc2d7b972b2041d\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"childre"])</script><script>self.__next_f.push([1,"n\":[\"$undefined\",[[\"$\",\"$1\",\"0-4.1 Positional Bias and the \\\"Lost-in-the-Middle\\\" Effect\",{\"children\":\"4.1 Positional Bias and the \\\"Lost-in-the-Middle\\\" Effect\"}]],\"$undefined\"]}]}]}]\n9f:[\"$\",\"li\",\"21740d70fdf5804cb009d3d8e8e002a5\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf5804cb009d3d8e8e002a5\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-4.2 The Detrimental Impact of Irrelevant and Distracting Information\",{\"children\":\"4.2 The Detrimental Impact of Irrelevant and Distracting Information\"}]],\"$undefined\"]}]}]}]\na0:[\"$\",\"li\",\"21740d70fdf580e2a4f9c97d9434d4cb\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580e2a4f9c97d9434d4cb\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-4.3 Cognitive Load and Context Window Management\",{\"children\":\"4.3 Cognitive Load and Context Window Management\"}]],\"$undefined\"]}]}]}]\na1:[\"$\",\"li\",\"21740d70fdf580e4a606e6f4424b31e5\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580e4a606e6f4424b31e5\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-5. Empirical Evidence and Performance Analysis\",{\"children\":\"5. Empirical Evidence and Performance Analysis\"}]],\"$undefined\"]}]}]}]\na2:[\"$\",\"li\",\"21740d70fdf580ad845cdef42ba8a4e7\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580ad845cdef42ba8a4e7\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-5.1 Case Studies on Chunk Order and Multi-Step Question Answering\",{\"children\":\"5.1 Case Studies on Chunk Order and Multi-Step Question Answering\"}]],\"$undefined\"]}]}]}]\na3:[\"$\",\"li\",\"21740d70fdf580889f7bc0535e95d69a\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580889f7bc0535e95d69a\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-5.2 Evaluation Benchmarks and Metrics\",{\"children\":\"5.2 Evaluation Benchmarks and Metrics\"}]],\"$undefined\"]}]}]}]\na4:[\"$\",\"li\",\"21740d70fdf580e0a67ddf16d23ac95b\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580e0a67ddf16d23ac95b\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\",{\"children\":\"6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning\"}]],\"$undefined\"]}]}]}]\na5:[\"$\",\"li\",\"21740d70fdf58085858fd4791e4f101b\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf58085858fd4791e4f101b\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-6.1 Best Practices for Chunking and Reordering\",{\"children\":\"6.1 Best Practices for Chunking and Reordering\"}]],\"$undefined\"]}]}]}]\na6:[\"$\",\"li\",\"21740d70fdf580e49c36f4912bd6ac93\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580e49c36f4912bd6ac93\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-6.2 Strategies for Mitigating Positional Bias and Distraction\",{\"children\":\"6.2 Strategies for Mitigating Positional Bias and Distraction\"}]],\"$undefined\"]}]}]}]\na7:[\"$\",\"li\",\"21740d70fdf580b3bb55f18aef19ec66\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580b3bb55f18aef19ec66\",\"children\":[\"$\",\"div\",null,{\"style\":{\"ma"])</script><script>self.__next_f.push([1,"rginInlineStart\":\"24px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-6.3 Advanced Techniques for Multi-Hop Reasoning\",{\"children\":\"6.3 Advanced Techniques for Multi-Hop Reasoning\"}]],\"$undefined\"]}]}]}]\na8:[\"$\",\"li\",\"21740d70fdf58057bc2bc6c92d18067c\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf58057bc2bc6c92d18067c\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-7. Conclusion and Future Directions\",{\"children\":\"7. Conclusion and Future Directions\"}]],\"$undefined\"]}]}]}]\na9:[\"$\",\"li\",\"21740d70fdf580388756d009905ff072\",{\"className\":\"notion-table-of-contents__item\",\"children\":[\"$\",\"$L97\",null,{\"uri\":\"#block-21740d70fdf580388756d009905ff072\",\"children\":[\"$\",\"div\",null,{\"style\":{\"marginInlineStart\":\"0px\"},\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-References\",{\"children\":\"References\"}]],\"$undefined\"]}]}]}]\nc9:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]]}]\nca:[\"$\",\"li\",\"21740d70fdf580959badf2d68cc1f896\",{\"id\":\"block-21740d70fdf580959badf2d68cc1f896\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Semantic Chunking:\",{\"children\":\"Semantic Chunking:\"}]}],[\"$\",\"$1\",\"1- This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.\",{\"children\":\" This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{14}\",\"inline\":true}]]}]],\"$undefined\"]}]\nd3:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]]}]\nd4:[\"$\",\"$1\",\"5- The effectiveness of RAG is therefore highly dependent on the retriever's ability to provide relevant information and the re-ranker's capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM's performance can degrade, leading to responses that, while \\\"grounded\\\" in the provided context, might be off-topic or factually incorrect.\",{\"children\":\" The effectiveness of RAG is therefore highly dependent on the retriever's ability to provide relevant information and the re-ranker's capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM's performance can degrade, leading to responses that, while \\\"grounded\\\" in the provided context, might be off-topic or factually incorrect.\"}]\nd5:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]]}]\nd6:[\"$\",\"$1\",\"7- The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the  \",{\"children\":\" The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the  \"}]\nd7:[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"8-most relevant\",{\"children\":\"most relevant\"}]}]\nd8:[\"$\",\"$1\",\"9- and \",{\"children\":\" and \"}]\nd9:[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"10-least distracting\",{\"children\":\"least distracting\"}]}]\nda:[\"$\",\"$1\",\"11- cont"])</script><script>self.__next_f.push([1,"ent, a factor that profoundly influences the subsequent chunk ordering.\",{\"children\":\" content, a factor that profoundly influences the subsequent chunk ordering.\"}]\nf6:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]]}]\nf7:[\"$\",\"$1\",\"5- Examples include GPT, Claude, and Gemini.\",{\"children\":\" Examples include GPT, Claude, and Gemini.\"}]\nf8:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]]}]\nf9:[\"$\",\"li\",\"21740d70fdf5802cb8e3e90f285484c4\",{\"id\":\"block-21740d70fdf5802cb8e3e90f285484c4\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Reranking APIs:\",{\"children\":\"Reranking APIs:\"}]}],[\"$\",\"$1\",\"1- Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.\",{\"children\":\" Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]]}],[\"$\",\"$1\",\"3- Examples include Cohere, Jina, and Mixedbread.\",{\"children\":\" Examples include Cohere, Jina, and Mixedbread.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]]}]],\"$undefined\"]}]\n101:[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Context fragmentation, information loss, inflexible.\",{\"children\":\"Context fragmentation, information loss, inflexible.\"}]],\"$undefined\"]}]}]\n102:[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-13\",{\"children\":\"13\"}]],\"$undefined\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"103:[\"$\",\"tr\",\"21740d70-fdf5-807c-a905-e19a618e0a48\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Recursive-Based Chunking\",{\"children\":\"Recursive-Based Chunking\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Uses multiple separators (paragraphs, sentences) to find meaningful boundaries.\",{\"children\":\"Uses multiple separators (paragraphs, sentences) to find meaningful boundaries.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Context Preservation\",{\"children\":\"Context Preservation\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Better at maintaining logical units for sequential understanding.\",{\"children\":\"Better at maintaining logical units for sequential understanding.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Adaptive, preserves logical flow.\",{\"children\":\"Adaptive, preserves logical flow.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-More complex to implement.\",{\"children\":\"More complex to implement.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-13\",{\"children\":\"13\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"104:[\"$\",\"tr\",\"21740d70-fdf5-804d-a0c1-f4d6f02bbd23\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Sentence-based Chunking\",{\"children\":\"Sentence-based Chunking\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Divides text into complete sentences.\",{\"children\":\"Divides text into complete sentences.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Preserve Complete Thoughts\",{\"children\":\"Preserve Complete Thoughts\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Supports logical flow, good for connecting ideas.\",{\"children\":\"Supports logical flow, good for connecting ideas.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Ensures complete thoughts, natural boundaries.\",{\"children\":\"Ensures complete thoughts, natural boundaries.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-May create very small chunks, less efficient for long documents.\",{\"children\":\"May create very small chunks, less efficient for long documents.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-13\",{\"children\":\"13\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"105:[\"$\",\"tr\",\"21740d70-fdf5-8033-8f32-d47185f32232\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Document's Original Structure (DOS RAG)\",{\"children\":\"Document's Original Structure (DOS RAG)\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Retrieves chunks and reorders them to match their original document sequence.\",{\"children\":\"Retrieves chunks and reorders them to match their original document sequence.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Narrative Continuity, Contextual Coherence\",{\"children\":\"Narrative Continuity, Contextual Coherence\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Significantly improves performance by maintaining logical progression; crucial for multi-hop QA.\",{\"children\":\"Significantly improves performance by maintaining logical progression; crucial for multi-hop QA.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Preserves narrative, robust QA, often outperforms relevance-based sorting.\",{\"children\":\"Preserves narrative, robust QA, often outperforms relevance-based sorting.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Requires tracking chunk positions; may include less relevant chunks if not filtered.\",{\"children\":\"Requires tracking chunk positions; may include less relevant chunks if not filtered.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-17\",{\"children\":\"17\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"106:[\"$\",\"tr\",\"21740d70-fdf5-805b-9621-fff137881fa9\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Inverted Context Ordering\",{\"children\":\"Inverted Context Ordering\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Arranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.\",{\"children\":\"Arranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Prioritize Most Relevant\",{\"children\":\"Prioritize Most Relevant\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Can improve correctness; focuses LLM on key information.\",{\"children\":\"Can improve correctness; focuses LLM on key information.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Directs LLM to top relevant info immediately.\",{\"children\":\"Directs LLM to top relevant info immediately.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Still relies on relevance score, may disrupt original narrative flow.\",{\"children\":\"Still relies on relevance score, may disrupt original narrative flow.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-16\",{\"children\":\"16\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"107:[\"$\",\"tr\",\"21740d70-fdf5-80f5-9c37-e3f38a6cc678\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"I{;C\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Semantic Chunking\",{\"children\":\"Semantic Chunking\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"Bmzb\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Divides documents into semantically coherent and non-overlapping chunks.\",{\"children\":\"Divides documents into semantically coherent and non-overlapping chunks.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"h]Bv\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reduce Irrelevance, Improve Accuracy\",{\"children\":\"Reduce Irrelevance, Improve Accuracy\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"a|Q{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Enhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.\",{\"children\":\"Enhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cJ@_\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reduces hallucinations, improves factual accuracy, aligned with query needs.\",{\"children\":\"Reduces hallucinations, improves factual accuracy, aligned with query needs.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MGks\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Requires sophisticated LLM-based relevance scoring.\",{\"children\":\"Requires sophisticated LLM-based relevance scoring.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JNqP\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-14\",{\"children\":\"14\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"110:[\"$\",\"$L196\",null,{\"latex\":\"^{27}\",\"inline\":true}]\n12a:[\"$undefined\",[[\"$\",\"$1\",\"0-Improve retrieval quality, LLM robustness to distraction, avoid simple rearrangement.\",{\"children\":\"Improve retrieval quality, LLM robustness to distraction, avoid simple rearrangement.\"}]],\"$undefined\"]\n12b:[\"$\",\"td\",\"TSJy\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-25\",{\"children\":\"25\"}]],\"$undefined\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"12c:[\"$\",\"tr\",\"21740d70-fdf5-80e2-8903-da195f82d8ec\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"CVMg\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Irrelevant/Distracting Information\",{\"children\":\"Irrelevant/Distracting Information\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"xXUs\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Passages that are semantically similar but do not contain the answer or mislead the LLM.\",{\"children\":\"Passages that are semantically similar but do not contain the answer or mislead the LLM.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MdJE\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Significantly degrades accuracy, even when relevant info is present; can derail reasoning chain.\",{\"children\":\"Significantly degrades accuracy, even when relevant info is present; can derail reasoning chain.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"sWuZ\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Strong retrievers can inadvertently bring more harmful distractors to top ranks.\",{\"children\":\"Strong retrievers can inadvertently bring more harmful distractors to top ranks.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\";h~{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Robust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.\",{\"children\":\"Robust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"TSJy\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-14\",{\"children\":\"14\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"12d:[\"$\",\"tr\",\"21740d70-fdf5-80d6-929f-cad27d2c4a26\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"CVMg\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Cognitive Load/Context Window Overload\",{\"children\":\"Cognitive Load/Context Window Overload\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"xXUs\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-LLM struggles to process excessive or noisy information within its limited context.\",{\"children\":\"LLM struggles to process excessive or noisy information within its limited context.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MdJE\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reduces selection accuracy, increases hallucinations, hinders efficient reasoning.\",{\"children\":\"Reduces selection accuracy, increases hallucinations, hinders efficient reasoning.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"sWuZ\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Too many chunks (even if somewhat relevant) can overwhelm the model.\",{\"children\":\"Too many chunks (even if somewhat relevant) can overwhelm the model.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\";h~{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Supplying only relevant context, precise chunking, adaptive streaming, efficient filtering.\",{\"children\":\"Supplying only relevant context, precise chunking, adaptive streaming, efficient filtering.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"TSJy\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-17\",{\"children\":\"17\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"12e:[\"$\",\"tr\",\"21740d70-fdf5-8059-8b35-dc40007ce750\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"CVMg\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Lack of Narrative Continuity\",{\"children\":\"Lack of Narrative Continuity\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"xXUs\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Disjointed or shuffled presentation of information.\",{\"children\":\"Disjointed or shuffled presentation of information.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"MdJE\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Impairs sequential reasoning, makes it harder for LLM to build a coherent understanding.\",{\"children\":\"Impairs sequential reasoning, makes it harder for LLM to build a coherent understanding.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"sWuZ\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Direct result of relevance-only sorting; addressed by DOS RAG.\",{\"children\":\"Direct result of relevance-only sorting; addressed by DOS RAG.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\";h~{\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Preserving original document structure (DOS RAG) or logical flow.\",{\"children\":\"Preserving original document structure (DOS RAG) or logical flow.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"TSJy\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-17\",{\"children\":\"17\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"147:[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"25-correctness\",{\"children\":\"correctness\"}]}]\n148:[\"$\",\"$1\",\"26-\\\" and \\\"\",{\"children\":\"\\\" and \\\"\"}]\n149:[\"$\",\"code\",\"0-c\",{\"className\":\"code\",\"children\":[\"$\",\"$1\",\"27-faithfulness\",{\"children\":\"faithfulness\"}]}]\n14a:[\"$\",\"$1\",\"28-\\\" further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.\\n\",{\"children\":\"\\\" further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.\\n\"}]\n14b:[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-ProcBench\",{\"children\":\"ProcBench\"}]}]],\"$undefined\"]}]}]}]\n14c:[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Multi-Step Reasoning \u0026 Procedure Following\",{\"children\":\"Multi-Step Reasoning \u0026 Procedure Following\"}]],\"$undefined\"]}]}]}]\n14d:[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Dataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.\",{\"children\":\"Dataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.\"}]],\"$undefined\"]}]}]}]\n14e:[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Highlights critical gap in current assessments focusing exclusively on multi-step inference.\",{\"children\":\"Highlights critical gap in current assessments focusing exclusively on multi-step inference.\"}]],\"$undefined\"]}]}]}]\n14f:[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-11\",{\"children\":\"11\"}]],\"$undefined\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"150:[\"$\",\"tr\",\"21740d70-fdf5-80ff-a82c-da40a0c79cfd\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Bench\",{\"children\":\"Bench\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Long-Context Question Answering\",{\"children\":\"Long-Context Question Answering\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Evaluates performance under varying retrieval token budgets (1.5K to 40K tokens).\",{\"children\":\"Evaluates performance under varying retrieval token budgets (1.5K to 40K tokens).\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-DOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.\",{\"children\":\"DOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-19\",{\"children\":\"19\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"151:[\"$\",\"tr\",\"21740d70-fdf5-80b0-b60e-f81f2bec4b6b\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-QuALITY\",{\"children\":\"QuALITY\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Long-Context Question Answering (narrative understanding)\",{\"children\":\"Long-Context Question Answering (narrative understanding)\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Requires understanding underlying narrative rather than shallow pattern matching.\",{\"children\":\"Requires understanding underlying narrative rather than shallow pattern matching.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Full-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.\",{\"children\":\"Full-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-19\",{\"children\":\"19\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"152:[\"$\",\"tr\",\"21740d70-fdf5-80fd-a60d-d6416e47442a\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-NarrativeQA\",{\"children\":\"NarrativeQA\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Long-Context Question Answering (narrative understanding)\",{\"children\":\"Long-Context Question Answering (narrative understanding)\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Questions require understanding the underlying narrative.\",{\"children\":\"Questions require understanding the underlying narrative.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-DOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).\",{\"children\":\"DOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-19\",{\"children\":\"19\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"153:[\"$\",\"tr\",\"21740d70-fdf5-80ef-9223-e24767ecd4dc\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-DataMorgana\",{\"children\":\"DataMorgana\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-QA-pair Generation (single-hop \u0026 multi-hop)\",{\"children\":\"QA-pair Generation (single-hop \u0026 multi-hop)\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Creates highly customizable synthetic benchmarks; used in LiveRAG Challenge.\",{\"children\":\"Creates highly customizable synthetic benchmarks; used in LiveRAG Challenge.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Used to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.\",{\"children\":\"Used to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-15\",{\"children\":\"15\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"154:[\"$\",\"tr\",\"21740d70-fdf5-80bf-a7dc-d61c06a605c9\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-mmRAG\",{\"children\":\"mmRAG\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Multi-modal RAG Evaluation\",{\"children\":\"Multi-modal RAG Evaluation\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Modular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.\",{\"children\":\"Modular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Provides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.\",{\"children\":\"Provides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-35\",{\"children\":\"35\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"155:[\"$\",\"tr\",\"21740d70-fdf5-801a-8113-e4f7fb6fa68d\",{\"style\":{\"background\":\"\",\"color\":\"var(--color-text-default)\"},\"children\":[[\"$\",\"td\",\"@KyG\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-ChunkRAG\",{\"children\":\"ChunkRAG\"}]}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"`hHL\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-LLM-driven Chunk Filtering\",{\"children\":\"LLM-driven Chunk Filtering\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"cE^M\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Enhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.\",{\"children\":\"Enhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"uaj?\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Outperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.\",{\"children\":\"Outperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.\"}]],\"$undefined\"]}]}]}],[\"$\",\"td\",\"JygU\",{\"style\":{\"minWidth\":\"120px\",\"maxWidth\":\"240px\",\"color\":\"\",\"background\":\"$undefined\"},\"children\":[\"$\",\"div\",null,{\"className\":\"notion-table__cell\",\"children\":[\"$\",\"span\",null,{\"className\":\"notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-14\",{\"children\":\"14\"}]],\"$undefined\"]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"161:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]]}]\n169:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^6\",\"inline\":true}]]}]\n16a:[\"$\",\"$1\",\"15- These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.\",{\"children\":\" These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.\"}]\n16f:[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^{4,35,40}\",\"inline\":true}]]}]\n170:[\"$\",\"li\",\"21740d70fdf580ba9d25c88f6cfa1e49\",{\"id\":\"block-21740d70fdf580ba9d25c88f6cfa1e49\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"strong\",\"0-b\",{\"children\":[\"$\",\"$1\",\"0-Evaluation Methodologies:\",{\"children\":\"Evaluation Methodologies:\"}]}],[\"$\",\"$1\",\"1- Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.\",{\"children\":\" Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.\"}],[\"$\",\"$1\",\"1-e\",{\"children\":[[\"$\",\"$L98\",null,{\"moduleIds\":\"$3c:props:children:1:1:props:children:0:props:moduleIds\"}],[\"$\",\"$L196\",null,{\"latex\":\"^9\",\"inline\":true}]]}]],\"$undefined\"]}]\n172:[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2502.15652v3.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2502.15652v3.\"}]\n173:[\"$\",\"li\",\"21740d70fdf5806da33bea329cf410a0\",{\"id\":\"block-21740d70fdf5806da33bea329cf410a0\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-AWS. (n.d.). \\n\",{\"children\":\"AWS. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-What is RAG (Retrieval-Augmented Generation)?\",{\"children\":\"What is RAG (Retrieval-Augmented Generation)?\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.\",{\"children\":\". Retrieved from https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.\"}]],\"$undefined\"]}]\n174:[\"$\",\"li\",\"21740d70fdf580a6be25eaf64652eaae\",{\"id\":\"block-21740d70fdf580a6be25eaf64652eaae\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Sharma, C. (2025). \\n\",{\"children\":\"Sharma, C. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\",{\"children\":\"Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2506.00054v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2506.00054v1.\"}]],\"$undefined\"]}]\n175:[\"$\",\"li\",\"21740d70fdf5804dbeeeffd1f4ae88c4\",{\"id\":\"block-21740d70fdf5804dbeeeffd1f4ae88c4\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Sharma, C. (2025). \\n\",{\"children\":\"Sharma, C. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\",{\"children\":\"Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from \",{\"children\":\". arXiv. Retrieved from \"}],[\"$\",\"$L97\",\"https://arxiv.org/abs/2506.00054\",{\"className\""])</script><script>self.__next_f.push([1,":\"link\",\"uri\":\"https://arxiv.org/abs/2506.00054\",\"serverHref\":\"$undefined\",\"children\":[\"$\",\"$1\",\"3-https://arxiv.org/abs/2506.00054\",{\"children\":\"https://arxiv.org/abs/2506.00054\"}]}],[\"$\",\"$1\",\"4-.\",{\"children\":\".\"}]],\"$undefined\"]}]\n176:[\"$\",\"li\",\"21740d70fdf580259622f329546dfb2f\",{\"id\":\"block-21740d70fdf580259622f329546dfb2f\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Xu, K., et al. (2024). \\n\",{\"children\":\"Xu, K., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Multi-Task Inference: Do LLMs Hold the Capability to Handle Multiple Instructions Simultaneously?\",{\"children\":\"Multi-Task Inference: Do LLMs Hold the Capability to Handle Multiple Instructions Simultaneously?\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2402.11597v2.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2402.11597v2.\"}]],\"$undefined\"]}]\n177:[\"$\",\"li\",\"21740d70fdf58095905ac7ad4b41ae74\",{\"id\":\"block-21740d70fdf58095905ac7ad4b41ae74\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-About AI. (2024, May 22). \\n\",{\"children\":\"About AI. (2024, May 22). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-How Well Do Large Language Models (LLMs) Actually Handle Multi-Step Reasoning?\",{\"children\":\"How Well Do Large Language Models (LLMs) Actually Handle Multi-Step Reasoning?\"}]}],[\"$\",\"$1\",\"2-. Medium. Retrieved from https://medium.com/about-ai/how-well-do-large-language-models-llms-actually-handle-multi-step-reasoning-83ce37f7fc32.\",{\"children\":\". Medium. Retrieved from https://medium.com/about-ai/how-well-do-large-language-models-llms-actually-handle-multi-step-reasoning-83ce37f7fc32.\"}]],\"$undefined\"]}]\n178:[\"$\",\"li\",\"21740d70fdf5807ab0f4d6304119f55e\",{\"id\":\"block-21740d70fdf5807ab0f4d6304119f55e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-F22 Labs. (n.d.). \\n\",{\"children\":\"F22 Labs. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-7 Chunking Strategies in RAG You Need To Know\",{\"children\":\"7 Chunking Strategies in RAG You Need To Know\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/#:~:text=Retrieval%20Augmented%20Generation%20(RAG)%20enhances,for%20faster%20retrieval%20and%20processing.\",{\"children\":\". Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/#:~:text=Retrieval%20Augmented%20Generation%20(RAG)%20enhances,for%20faster%20retrieval%20and%20processing.\"}]],\"$undefined\"]}]\n179:[\"$\",\"li\",\"21740d70fdf580679df9f9f2652d4f26\",{\"id\":\"block-21740d70fdf580679df9f9f2652d4f26\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-F22 Labs. (n.d.). \\n\",{\"children\":\"F22 Labs. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-7 Chunking Strategies in RAG You Need To Know\",{\"children\":\"7 Chunking Strategies in RAG You Need To Know\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/.\",{\"children\":\". Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/.\"}]],\"$undefined\"]}]\n17a:[\"$\",\"li\",\"21740d70fdf580c684aaf266ad8e32a5\",{\"id\":\"block-21740d70fdf580c684aaf266ad8e32a5\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Mallen, E., et al. (2024). \\n\",{\"children\":\"Mallen, E., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\",{\"children\":\"ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2410.19572v2.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2410.19572v2.\"}]],\"$undefined\"]}]\n17b:[\"$\",\"li\",\"21740d70fdf5807d9c4dfa847b6cef28\",{\"id\":\"block-21740d70fdf5807d9c4dfa847b6cef28\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Filice, S., et al. (2025). \\n\",{\"children\":\"Filice, S., et al. (2025). \\n\"}"])</script><script>self.__next_f.push([1,"],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",{\"children\":\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2506.14412v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2506.14412v1.\"}]],\"$undefined\"]}]\n17c:[\"$\",\"li\",\"21740d70fdf58086b271ef77f650390a\",{\"id\":\"block-21740d70fdf58086b271ef77f650390a\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Filice, S., et al. (2025). \\n\",{\"children\":\"Filice, S., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",{\"children\":\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.14412.\",{\"children\":\". arXiv. Retrieved from https://www.arxiv.org/pdf/2506.14412.\"}]],\"$undefined\"]}]\n17d:[\"$\",\"li\",\"21740d70fdf58078b78cdffcd8fd875e\",{\"id\":\"block-21740d70fdf58078b78cdffcd8fd875e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-SuperAnnotate. (n.d.). \\n\",{\"children\":\"SuperAnnotate. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAG vs. Long Context LLMs\",{\"children\":\"RAG vs. Long Context LLMs\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.superannotate.com/blog/rag-vs-long-context-llms.\",{\"children\":\". Retrieved from https://www.superannotate.com/blog/rag-vs-long-context-llms.\"}]],\"$undefined\"]}]\n17e:[\"$\",\"li\",\"21740d70fdf58059a4e7c03e14bd6215\",{\"id\":\"block-21740d70fdf58059a4e7c03e14bd6215\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Liu, Y., et al. (2024). \\n\",{\"children\":\"Liu, Y., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-The Impact of Premise Order on LLM Reasoning\",{\"children\":\"The Impact of Premise Order on LLM Reasoning\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2402.08939v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2402.08939v1.\"}]],\"$undefined\"]}]\n17f:[\"$\",\"li\",\"21740d70fdf580d08492fb9b92c328c8\",{\"id\":\"block-21740d70fdf580d08492fb9b92c328c8\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Cuconasu, F., et al. (2025, June 4). \\n\",{\"children\":\"Cuconasu, F., et al. (2025, June 4). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\",{\"children\":\"Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.03989.\",{\"children\":\". arXiv. Retrieved from https://www.arxiv.org/pdf/2506.03989.\"}]],\"$undefined\"]}]\n180:[\"$\",\"li\",\"21740d70fdf5808ab1dbe83434ff1e1f\",{\"id\":\"block-21740d70fdf5808ab1dbe83434ff1e1f\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Cuconasu, F., et al. (2025). \\n\",{\"children\":\"Cuconasu, F., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\",{\"children\":\"Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2506.03989v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2506.03989v1.\"}]],\"$undefined\"]}]\n181:[\"$\",\"li\",\"21740d70fdf5809ba01edf97c0736c92\",{\"id\":\"block-21740d70fdf5809ba01edf97c0736c92\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-DataCamp. (n.d.). \\n\",{\"children\":\"DataCamp. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Advanced RAG Techniques\",{\"children\":\"Advanced RAG Techniques\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https:"])</script><script>self.__next_f.push([1,"//www.datacamp.com/blog/rag-advanced.\",{\"children\":\". Retrieved from https://www.datacamp.com/blog/rag-advanced.\"}]],\"$undefined\"]}]\n182:[\"$\",\"li\",\"21740d70fdf580f999e2db1523518c77\",{\"id\":\"block-21740d70fdf580f999e2db1523518c77\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Analytics Vidhya. (2025, March 28). \\n\",{\"children\":\"Analytics Vidhya. (2025, March 28). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Comprehensive Guide on Reranker for RAG\",{\"children\":\"Comprehensive Guide on Reranker for RAG\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.analyticsvidhya.com/blog/2025/03/reranker-for-rag/.\",{\"children\":\". Retrieved from https://www.analyticsvidhya.com/blog/2025/03/reranker-for-rag/.\"}]],\"$undefined\"]}]\n183:[\"$\",\"li\",\"21740d70fdf5807ba8dbdede8f9371bd\",{\"id\":\"block-21740d70fdf5807ba8dbdede8f9371bd\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Galileo AI. (n.d.). \\n\",{\"children\":\"Galileo AI. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Mastering RAG: How to Select a Reranking Model\",{\"children\":\"Mastering RAG: How to Select a Reranking Model\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model.\",{\"children\":\". Retrieved from https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model.\"}]],\"$undefined\"]}]\n184:[\"$\",\"li\",\"21740d70fdf580ffbfd5e1c2b853d383\",{\"id\":\"block-21740d70fdf580ffbfd5e1c2b853d383\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Filice, S., et al. (2025). \\n\",{\"children\":\"Filice, S., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\",{\"children\":\"RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2506.14412.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2506.14412.\"}]],\"$undefined\"]}]\n185:[\"$\",\"li\",\"21740d70fdf580b1aa1afd59d3f6ac19\",{\"id\":\"block-21740d70fdf580b1aa1afd59d3f6ac19\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Cuconasu, F., et al. (2025). \\n\",{\"children\":\"Cuconasu, F., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Do RAG Systems Suffer From Positional Bias?\",{\"children\":\"Do RAG Systems Suffer From Positional Bias?\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2505.15561v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2505.15561v1.\"}]],\"$undefined\"]}]\n186:[\"$\",\"li\",\"21740d70fdf58020b6c2f23ce562d30d\",{\"id\":\"block-21740d70fdf58020b6c2f23ce562d30d\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Cuconasu, F., et al. (2025). \\n\",{\"children\":\"Cuconasu, F., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Do RAG Systems Suffer From Positional Bias?\",{\"children\":\"Do RAG Systems Suffer From Positional Bias?\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/abs/2505.15561.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/abs/2505.15561.\"}]],\"$undefined\"]}]\n187:[\"$\",\"li\",\"21740d70fdf58084b30fdb94d2ea101e\",{\"id\":\"block-21740d70fdf58084b30fdb94d2ea101e\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Cuconasu, F., et al. (2025). \\n\",{\"children\":\"Cuconasu, F., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Investigate positional bias in RAG systems, including the 'lost-in-the-middle' effect and impact of distracting passages\",{\"children\":\"Investigate positional bias in RAG systems, including the 'lost-in-the-middle' effect and impact of distracting passages\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/pdf/2505.15561.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/pdf/2505.15561.\"}]],\"$undefined\"]}]\n188:[\"$\",\"li\",\"21740d70fdf58077896bcd067145878b\",{\"id\":\"block-21740d70fdf580778"])</script><script>self.__next_f.push([1,"96bcd067145878b\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Amiraz, C., et al. (2025, May 11). \\n\",{\"children\":\"Amiraz, C., et al. (2025, May 11). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Describe the 'distracting effect' of irrelevant passages in RAG and its impact on LLM accuracy\",{\"children\":\"Describe the 'distracting effect' of irrelevant passages in RAG and its impact on LLM accuracy\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/abs/2505.06914.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/abs/2505.06914.\"}]],\"$undefined\"]}]\n189:[\"$\",\"li\",\"21740d70fdf58073ae6dd2cc789c3e2f\",{\"id\":\"block-21740d70fdf58073ae6dd2cc789c3e2f\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Amiraz, C., et al. (2025). \\n\",{\"children\":\"Amiraz, C., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-The Distracting Effect: Understanding Irrelevant Passages in RAG\",{\"children\":\"The Distracting Effect: Understanding Irrelevant Passages in RAG\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2505.06914v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2505.06914v1.\"}]],\"$undefined\"]}]\n18a:[\"$\",\"li\",\"21740d70fdf580968828d188c0c8ee1d\",{\"id\":\"block-21740d70fdf580968828d188c0c8ee1d\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Li, Y., et al. (2025). \\n\",{\"children\":\"Li, Y., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAG-MCP: Retrieval-Augmented Generation for Model Context Protocol\",{\"children\":\"RAG-MCP: Retrieval-Augmented Generation for Model Context Protocol\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2505.03275v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2505.03275v1.\"}]],\"$undefined\"]}]\n18b:[\"$\",\"li\",\"21740d70fdf5807897acec3822f47bf9\",{\"id\":\"block-21740d70fdf5807897acec3822f47bf9\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Zhang, M., et al. (2025). \\n\",{\"children\":\"Zhang, M., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Cognitive-Aware LLM Streaming\",{\"children\":\"Cognitive-Aware LLM Streaming\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2504.17999v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2504.17999v1.\"}]],\"$undefined\"]}]\n18c:[\"$\",\"li\",\"21740d70fdf5802486baf24d75884d9a\",{\"id\":\"block-21740d70fdf5802486baf24d75884d9a\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Li, Y., et al. (2025). \\n\",{\"children\":\"Li, Y., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-How does supplying only relevant context in RAG reduce cognitive load and improve LLM decision making?\",{\"children\":\"How does supplying only relevant context in RAG reduce cognitive load and improve LLM decision making?\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/pdf/2505.03275.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/pdf/2505.03275.\"}]],\"$undefined\"]}]\n18d:[\"$\",\"li\",\"21740d70fdf5809e9c01f527c2c10ef8\",{\"id\":\"block-21740d70fdf5809e9c01f527c2c10ef8\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Wang, Y., et al. (2024). \\n\",{\"children\":\"Wang, Y., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-InfLLM: Enabling LLMs to Understand Extremely Long Sequences Without Any Fine-tuning\",{\"children\":\"InfLLM: Enabling LLMs to Understand Extremely Long Sequences Without Any Fine-tuning\"}]}],[\"$\",\"$1\",\"2-. NeurIPS. Retrieved from https://neurips.cc/virtual/2024/poster/94480.\",{\"children\":\". NeurIPS. Retrieved from https://neurips.cc/virtual/2024/poster/94480.\"}]],\"$undefined\"]}]\n18e:[\"$\",\"li\",\"21740d70fdf5800facf1d89be0a5874c\",{\"id\":\"block-21740d70fdf5800facf1d89be0a5874c\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Xu, J., et al. (2025). \\n\",{\"children\":\"Xu, J., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Long Context"])</script><script>self.__next_f.push([1," vs. RAG for LLMs: An Evaluation and Revisits\",{\"children\":\"Long Context vs. RAG for LLMs: An Evaluation and Revisits\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2501.01880v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2501.01880v1.\"}]],\"$undefined\"]}]\n18f:[\"$\",\"li\",\"21740d70fdf580c89d4df57ac8cd0fd7\",{\"id\":\"block-21740d70fdf580c89d4df57ac8cd0fd7\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Chen, C., et al. (2025). \\n\",{\"children\":\"Chen, C., et al. (2025). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs\",{\"children\":\"mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs\"}]}],[\"$\",\"$1\",\"2-. arXiv. Retrieved from https://arxiv.org/html/2505.11180v1.\",{\"children\":\". arXiv. Retrieved from https://arxiv.org/html/2505.11180v1.\"}]],\"$undefined\"]}]\n190:[\"$\",\"li\",\"21740d70fdf58026936ddc81616067a1\",{\"id\":\"block-21740d70fdf58026936ddc81616067a1\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Zhang, Z., et al. (2024). \\n\",{\"children\":\"Zhang, Z., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAGChecker: A Fine-grained Evaluation Framework for Retrieval-Augmented Generation\",{\"children\":\"RAGChecker: A Fine-grained Evaluation Framework for Retrieval-Augmented Generation\"}]}],[\"$\",\"$1\",\"2-. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html.\",{\"children\":\". NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html.\"}]],\"$undefined\"]}]\n191:[\"$\",\"li\",\"21740d70fdf580e3a8fae2409c074f7a\",{\"id\":\"block-21740d70fdf580e3a8fae2409c074f7a\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Diamant, N. (n.d.). \\n\",{\"children\":\"Diamant, N. (n.d.). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAG_Techniques\",{\"children\":\"RAG_Techniques\"}]}],[\"$\",\"$1\",\"2-. GitHub. Retrieved from https://github.com/NirDiamant/RAG_Techniques.\",{\"children\":\". GitHub. Retrieved from https://github.com/NirDiamant/RAG_Techniques.\"}]],\"$undefined\"]}]\n192:[\"$\",\"li\",\"21740d70fdf580449387e309ff20589a\",{\"id\":\"block-21740d70fdf580449387e309ff20589a\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Mallen, E., et al. (2024). \\n\",{\"children\":\"Mallen, E., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models\",{\"children\":\"Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models\"}]}],[\"$\",\"$1\",\"2-. ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-emnlp.831/.\",{\"children\":\". ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-emnlp.831/.\"}]],\"$undefined\"]}]\n193:[\"$\",\"li\",\"21740d70fdf5802b9c93c4c6f0a88a9b\",{\"id\":\"block-21740d70fdf5802b9c93c4c6f0a88a9b\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Li, H., et al. (2024). \\n\",{\"children\":\"Li, H., et al. (2024). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-RAGRAPH: A General Retrieval-Augmented Graph Learning Framework\",{\"children\":\"RAGRAPH: A General Retrieval-Augmented Graph Learning Framework\"}]}],[\"$\",\"$1\",\"2-. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/file/34d6c7090bc5af0b96aeaf92fa074899-Paper-Conference.pdf.\",{\"children\":\". NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/file/34d6c7090bc5af0b96aeaf92fa074899-Paper-Conference.pdf.\"}]],\"$undefined\"]}]\n194:[\"$\",\"li\",\"21740d70fdf5800a8c63e200b143bcc2\",{\"id\":\"block-21740d70fdf5800a8c63e200b143bcc2\",\"className\":\"notion-list-item notion-semantic-string\",\"children\":[\"$undefined\",[[\"$\",\"$1\",\"0-Reddit. (2024, March 17). \\n\",{\"childr"])</script><script>self.__next_f.push([1,"en\":\"Reddit. (2024, March 17). \\n\"}],[\"$\",\"em\",\"0-i\",{\"children\":[\"$\",\"$1\",\"1-Advanced Chunking/Retrieving Strategies for Legal Documents\",{\"children\":\"Advanced Chunking/Retrieving Strategies for Legal Documents\"}]}],[\"$\",\"$1\",\"2-. Retrieved from https://www.reddit.com/r/Rag/comments/1jdi4sg/advanced_chunkingretrieving_strategies_for_legal/.\",{\"children\":\". Retrieved from https://www.reddit.com/r/Rag/comments/1jdi4sg/advanced_chunkingretrieving_strategies_for_legal/.\"}]],\"$undefined\"]}]\naa:[\"$\",\"$L196\",null,{\"latex\":\"^1\",\"inline\":true}]\nab:[\"$\",\"$L196\",null,{\"latex\":\"^1\",\"inline\":true}]\nac:[\"$\",\"$L196\",null,{\"latex\":\"^2\",\"inline\":true}]\nad:[\"$\",\"$L196\",null,{\"latex\":\"^3\",\"inline\":true}]\nae:[\"$\",\"$L196\",null,{\"latex\":\"^2\",\"inline\":true}]\naf:[\"$\",\"$L196\",null,{\"latex\":\"^2\",\"inline\":true}]\nb0:[\"$\",\"$L196\",null,{\"latex\":\"^5\",\"inline\":true}]\nb1:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb2:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb3:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb4:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb5:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb6:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb7:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nb8:[\"$\",\"$L196\",null,{\"latex\":\"^5\",\"inline\":true}]\nb9:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\nba:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\nbb:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\nbc:[\"$\",\"$L196\",null,{\"latex\":\"^5\",\"inline\":true}]\nbd:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\nbe:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\nbf:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\nc0:[\"$\",\"$L196\",null,{\"latex\":\"^{12}\",\"inline\":true}]\nc1:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc2:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc3:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc4:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc5:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc6:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc7:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\nc8:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\ncb:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\ncc:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\ncd:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\nce:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\ncf:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nd0:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nd1:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\nd2:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\ndb:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\ndc:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\ndd:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\nde:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\ndf:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\ne0:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\ne1:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\ne2:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\ne3:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ne4:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ne5:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\ne6:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ne7:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ne8:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ne9:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\nea:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\neb:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\nec:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\ned:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\nee:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\nef:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nf0:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nf1:[\"$\",\"$L196\",null,{\"latex\":\"^{15}\",\"inline\":true}]\nf2:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nf3:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nf4:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nf5:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\nfa:[\"$\",\"$L196\",null,{\"latex\":\"^{16}\",\"inline\":true}]\nfb:[\"$\",\"$L196\",null,{\"latex\":\"^{24}\",\"inline\":true}]\nfc:[\"$\""])</script><script>self.__next_f.push([1,",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\nfd:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\nfe:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\nff:[\"$\",\"$L196\",null,{\"latex\":\"^{15}\",\"inline\":true}]\n100:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n108:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n109:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n10a:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n10b:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n10c:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n10d:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n10e:[\"$\",\"$L196\",null,{\"latex\":\"^{26}\",\"inline\":true}]\n10f:[\"$\",\"$L196\",null,{\"latex\":\"^{27}\",\"inline\":true}]\n111:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n112:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n113:[\"$\",\"$L196\",null,{\"latex\":\"^{27}\",\"inline\":true}]\n114:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n115:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n116:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n117:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n118:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n119:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n11a:[\"$\",\"$L196\",null,{\"latex\":\"^4\",\"inline\":true}]\n11b:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n11c:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n11d:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n11e:[\"$\",\"$L196\",null,{\"latex\":\"^{31}\",\"inline\":true}]\n11f:[\"$\",\"$L196\",null,{\"latex\":\"^{31}\",\"inline\":true}]\n120:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n121:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n122:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n123:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n124:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\n125:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\n126:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n127:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\n128:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n129:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n12f:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n130:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n131:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n132:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n133:[\"$\",\"$L196\",null,{\"latex\":\"^{20}\",\"inline\":true}]\n134:[\"$\",\"$L196\",null,{\"latex\":\"^{17}\",\"inline\":true}]\n135:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\n136:[\"$\",\"$L196\",null,{\"latex\":\"^{18}\",\"inline\":true}]\n137:[\"$\",\"$L196\",null,{\"latex\":\"^{24}\",\"inline\":true}]\n138:[\"$\",\"$L196\",null,{\"latex\":\"^{24}\",\"inline\":true}]\n139:[\"$\",\"$L196\",null,{\"latex\":\"^{24}\",\"inline\":true}]\n13a:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\n13b:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\n13c:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\n13d:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\n13e:[\"$\",\"$L196\",null,{\"latex\":\"^{15}\",\"inline\":true}]\n13f:[\"$\",\"$L196\",null,{\"latex\":\"^{35}\",\"inline\":true}]\n140:[\"$\",\"$L196\",null,{\"latex\":\"^{36}\",\"inline\":true}]\n141:[\"$\",\"$L196\",null,{\"latex\":\"^{14}\",\"inline\":true}]\n142:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n143:[\"$\",\"$L196\",null,{\"latex\":\"^{16}\",\"inline\":true}]\n144:[\"$\",\"$L196\",null,{\"latex\":\"^{10}\",\"inline\":true}]\n145:[\"$\",\"$L196\",null,{\"latex\":\"^{11}\",\"inline\":true}]\n146:[\"$\",\"$L196\",null,{\"latex\":\"^{15}\",\"inline\":true}]\n156:[\"$\",\"$L196\",null,{\"latex\":\"^{13}\",\"inline\":true}]\n157:[\"$\",\"$L196\",null,{\"latex\":\"^{19}\",\"inline\":true}]\n158:[\"$\",\"$L196\",null,{\"latex\":\"^{22}\",\"inline\":true}]\n159:[\"$\",\"$L196\",null,{\"latex\":\"^{16}\",\"inline\":true}]\n15a:[\"$\",\"$L196\",null,{\"latex\":\"^{20}\",\"inline\":true}]\n15b:[\"$\",\"$L196\",null,{\"latex\":\"^{16}\",\"inline\":true}]\n15c:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n15d:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n15e:[\"$\",\"$L196\",null,{\"latex\":\"^{14}\",\"inline\":true}]\n15f:[\"$\",\"$L196\",null,{\"latex\":\"^{14}\",\"inline\":true}]\n160:[\"$\",\"$L196\",null,{\"latex\":\"^{30}\",\"inline\":true}]\n162:[\"$\",\"$L196\",null,{\"latex\":\"^{25}\",\"inline\":true}]\n163:[\"$\",\"$L196\",null,{\"latex\":\"^{26}\",\"inline\":t"])</script><script>self.__next_f.push([1,"rue}]\n164:[\"$\",\"$L196\",null,{\"latex\":\"^{14}\",\"inline\":true}]\n165:[\"$\",\"$L196\",null,{\"latex\":\"^{28}\",\"inline\":true}]\n166:[\"$\",\"$L196\",null,{\"latex\":\"^{16}\",\"inline\":true}]\n167:[\"$\",\"$L196\",null,{\"latex\":\"^{38}\",\"inline\":true}]\n168:[\"$\",\"$L196\",null,{\"latex\":\"^{35,39}\",\"inline\":true}]\n16b:[\"$\",\"$L196\",null,{\"latex\":\"^8\",\"inline\":true}]\n16c:[\"$\",\"$L196\",null,{\"latex\":\"^9\",\"inline\":true}]\n16d:[\"$\",\"$L196\",null,{\"latex\":\"^{6,39}\",\"inline\":true}]\n16e:[\"$\",\"$L196\",null,{\"latex\":\"^{8,28}\",\"inline\":true}]\n"])</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"version":"2024.11.0","token":"8e1671f964d94778802ec9b15ec30896","r":1,"server_timing":{"name":{"cfCacheStatus":true,"cfEdge":true,"cfExtPri":true,"cfL4":true,"cfOrigin":true,"cfSpeedBrain":true},"location_startswith":null}}' crossorigin="anonymous"></script>
</body></html>