<main id="page-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" class="super-content page__blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models parent-page__blog-list"><div class="super-navbar__breadcrumbs" style="position:absolute"><div class="notion-breadcrumb"><a id="block-8d6dfeef4c7f4d678b4899d2198877cb" href="/" class="notion-link notion-breadcrumb__item"><div class="notion-navbar__title notion-breadcrumb__title">Hi there!</div></a><span class="notion-breadcrumb__divider">/</span><a id="block-blog" href="/blog" class="notion-link notion-breadcrumb__item"><div class="notion-navbar__title notion-breadcrumb__title">Blog</div></a><span class="notion-breadcrumb__divider">/</span><a id="block-blog-list" href="/blog/list" class="notion-link notion-breadcrumb__item"><div class="notion-navbar__title notion-breadcrumb__title">List</div></a><span class="notion-breadcrumb__divider">/</span><a id="block-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" href="/blog/list/the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" class="notion-link notion-breadcrumb__item"><div class="notion-navbar__title notion-breadcrumb__title">The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models</div></a></div></div><div class="notion-header page"><div class="notion-header__cover no-cover no-icon"></div><div class="notion-header__content max-width no-cover no-icon"><div class="notion-header__title-wrapper"><h1 class="notion-header__title">The Effect of Chunk Retrieval Sequence in RAG on Multi-Step Inference Performance of Large Language Models</h1></div></div></div><article id="block-blog-list-the-effect-of-chunk-retrieval-sequence-in-rag-on-multi-step-inference-performance-of-large-language-models" class="notion-root max-width has-footer"><div class="notion-page__properties"><div class="notion-page__property"><div class="notion-page__property-name-wrapper"><div class="notion-page__property-icon-wrapper"><svg viewBox="0 0 16 16" style="width:16px;height:16px"><path d="M3.29688 14.4561H12.7031C14.1797 14.4561 14.9453 13.6904 14.9453 12.2344V3.91504C14.9453 2.45215 14.1797 1.69336 12.7031 1.69336H3.29688C1.82031 1.69336 1.05469 2.45215 1.05469 3.91504V12.2344C1.05469 13.6973 1.82031 14.4561 3.29688 14.4561ZM3.27637 13.1162C2.70898 13.1162 2.39453 12.8154 2.39453 12.2207V5.9043C2.39453 5.30273 2.70898 5.00879 3.27637 5.00879H12.71C13.2842 5.00879 13.6055 5.30273 13.6055 5.9043V12.2207C13.6055 12.8154 13.2842 13.1162 12.71 13.1162H3.27637ZM6.68066 7.38086H7.08398C7.33008 7.38086 7.41211 7.30566 7.41211 7.05957V6.66309C7.41211 6.41699 7.33008 6.3418 7.08398 6.3418H6.68066C6.44141 6.3418 6.35938 6.41699 6.35938 6.66309V7.05957C6.35938 7.30566 6.44141 7.38086 6.68066 7.38086ZM8.92285 7.38086H9.31934C9.56543 7.38086 9.64746 7.30566 9.64746 7.05957V6.66309C9.64746 6.41699 9.56543 6.3418 9.31934 6.3418H8.92285C8.67676 6.3418 8.59473 6.41699 8.59473 6.66309V7.05957C8.59473 7.30566 8.67676 7.38086 8.92285 7.38086ZM11.1582 7.38086H11.5547C11.8008 7.38086 11.8828 7.30566 11.8828 7.05957V6.66309C11.8828 6.41699 11.8008 6.3418 11.5547 6.3418H11.1582C10.9121 6.3418 10.8301 6.41699 10.8301 6.66309V7.05957C10.8301 7.30566 10.9121 7.38086 11.1582 7.38086ZM4.44531 9.58203H4.84863C5.09473 9.58203 5.17676 9.50684 5.17676 9.26074V8.86426C5.17676 8.61816 5.09473 8.54297 4.84863 8.54297H4.44531C4.20605 8.54297 4.12402 8.61816 4.12402 8.86426V9.26074C4.12402 9.50684 4.20605 9.58203 4.44531 9.58203ZM6.68066 9.58203H7.08398C7.33008 9.58203 7.41211 9.50684 7.41211 9.26074V8.86426C7.41211 8.61816 7.33008 8.54297 7.08398 8.54297H6.68066C6.44141 8.54297 6.35938 8.61816 6.35938 8.86426V9.26074C6.35938 9.50684 6.44141 9.58203 6.68066 9.58203ZM8.92285 9.58203H9.31934C9.56543 9.58203 9.64746 9.50684 9.64746 9.26074V8.86426C9.64746 8.61816 9.56543 8.54297 9.31934 8.54297H8.92285C8.67676 8.54297 8.59473 8.61816 8.59473 8.86426V9.26074C8.59473 9.50684 8.67676 9.58203 8.92285 9.58203ZM11.1582 9.58203H11.5547C11.8008 9.58203 11.8828 9.50684 11.8828 9.26074V8.86426C11.8828 8.61816 11.8008 8.54297 11.5547 8.54297H11.1582C10.9121 8.54297 10.8301 8.61816 10.8301 8.86426V9.26074C10.8301 9.50684 10.9121 9.58203 11.1582 9.58203ZM4.44531 11.7832H4.84863C5.09473 11.7832 5.17676 11.708 5.17676 11.4619V11.0654C5.17676 10.8193 5.09473 10.7441 4.84863 10.7441H4.44531C4.20605 10.7441 4.12402 10.8193 4.12402 11.0654V11.4619C4.12402 11.708 4.20605 11.7832 4.44531 11.7832ZM6.68066 11.7832H7.08398C7.33008 11.7832 7.41211 11.708 7.41211 11.4619V11.0654C7.41211 10.8193 7.33008 10.7441 7.08398 10.7441H6.68066C6.44141 10.7441 6.35938 10.8193 6.35938 11.0654V11.4619C6.35938 11.708 6.44141 11.7832 6.68066 11.7832ZM8.92285 11.7832H9.31934C9.56543 11.7832 9.64746 11.708 9.64746 11.4619V11.0654C9.64746 10.8193 9.56543 10.7441 9.31934 10.7441H8.92285C8.67676 10.7441 8.59473 10.8193 8.59473 11.0654V11.4619C8.59473 11.708 8.67676 11.7832 8.92285 11.7832Z"></path></svg></div><div class="notion-page__property-name"><span>Date</span></div></div><div class="notion-property notion-property__date property-5d71516e notion-semantic-string"><span class="date">June 19, 2025</span></div></div><div class="notion-page__property"><div class="notion-page__property-name-wrapper"><div class="notion-page__property-icon-wrapper"><svg viewBox="0 0 16 16" style="width:16px;height:16px"><path d="M10.9536 7.90088C12.217 7.90088 13.2559 6.79468 13.2559 5.38525C13.2559 4.01514 12.2114 2.92017 10.9536 2.92017C9.70142 2.92017 8.65137 4.02637 8.65698 5.39087C8.6626 6.79468 9.69019 7.90088 10.9536 7.90088ZM4.4231 8.03003C5.52368 8.03003 6.42212 7.05859 6.42212 5.83447C6.42212 4.63843 5.51245 3.68945 4.4231 3.68945C3.33374 3.68945 2.41846 4.64966 2.41846 5.84009C2.42407 7.05859 3.32251 8.03003 4.4231 8.03003ZM1.37964 13.168H5.49561C4.87231 12.292 5.43384 10.6074 6.78711 9.51807C6.18628 9.14746 5.37769 8.87231 4.4231 8.87231C1.95239 8.87231 0.262207 10.6917 0.262207 12.1628C0.262207 12.7974 0.548584 13.168 1.37964 13.168ZM7.50024 13.168H14.407C15.4009 13.168 15.7322 12.8423 15.7322 12.2864C15.7322 10.8489 13.8679 8.88354 10.9536 8.88354C8.04492 8.88354 6.17505 10.8489 6.17505 12.2864C6.17505 12.8423 6.50635 13.168 7.50024 13.168Z"></path></svg></div><div class="notion-page__property-name"><span>Author</span></div></div><div class="notion-property notion-property__person property-5d51766e notion-semantic-string no-wrap"><span class="individual-with-image"><div class="individual-letter-avatar">J</div><span>Jinkun Chen</span></span></div></div><div id="block-root-divider" class="notion-divider"></div></div><p id="block-37f7210c96014e2a9f56aecbc694e2c5" class="notion-text notion-text__content notion-semantic-string"><em>Why the order of retrieved information can quietly change how AI reasons step by step</em></p><p id="block-6c8403d936fb490e8e24f1bf3563e2eb" class="notion-text notion-text__content notion-semantic-string">You give an AI the same set of facts, but present them in a slightly different order, and the final answer changes. For users, this can feel confusing or even unsettling, especially when the task requires multiple steps of reasoning.</p><p id="block-2cbe5c0189b147baaaf2eebd568f3e11" class="notion-text notion-text__content notion-semantic-string">This behavior becomes especially visible in systems that retrieve information step by step while reasoning. In this report, I examine how the retrieval sequence of supporting passages ("chunks") shapes multi-step inference performance in Retrieval-Augmented Generation (RAG) systems. Here, "multi-step inference" refers to tasks where the model must connect several pieces of information over time, rather than responding based on a single fact.</p><p id="block-4a8efe9c8c844c6099a647fae633e65a" class="notion-text notion-text__content notion-semantic-string">A key finding is that preserving the original document structure, as seen in Document's Original Structure RAG (DOS RAG), often yields superior performance compared to methods that solely prioritize relevance-based sorting. This is attributed to the maintenance of narrative continuity, which facilitates the LLM's sequential processing.</p><p id="block-d95145e3a15c47179bb2c31dae95d2ef" class="notion-text notion-text__content notion-semantic-string">For users, this means that even small changes in how information is presented can influence whether the model stays on track or gradually drifts during reasoning.</p><p id="block-975a6c020a53497d81afe6ed91c2ac38" class="notion-text notion-text__content notion-semantic-string">The analysis further reveals that LLMs exhibit a "cognitive linearity", performing optimally when information flows logically. Challenges such as positional bias and the detrimental effects of irrelevant or distracting information can significantly impede multi-step reasoning, even with highly relevant chunks. For users, this can look like a model latching onto an early detail and never fully recovering, even when later context would correct it. These issues necessitate robust reranking and filtering mechanisms, alongside a careful balance of context window size to avoid cognitive overload. The report concludes that optimizing chunk retrieval sequence requires a holistic approach, integrating intelligent chunking, strategic reranking, and proactive mitigation of noise to design robust RAG systems capable of advanced, knowledge-intensive tasks.</p><p id="block-15da42c0920c4fe5a384039880acbf25" class="notion-text notion-text__content notion-semantic-string">Beyond system design, these findings help explain why AI reasoning can feel fragile in everyday use. When reasoning depends on a sequence of retrieved information, confidence alone does not guarantee stability, especially in longer chains of thought.</p><div id="block-21740d70fdf5809e9b08e50fd38ec2c9" class="notion-text"></div><div id="block-21740d70fdf5807cb076cbe47bde3bc5" class="notion-divider"></div><ul id="block-21740d70fdf5803ba492f8f7a0f1124b" class="notion-table-of-contents color-gray"><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580789318c4b68c9dc180"><div class="notion-semantic-string" style="margin-inline-start: 0px;">1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580998476c769158dc800"><div class="notion-semantic-string" style="margin-inline-start: 24px;">1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5804ea1b3c814f4776242"><div class="notion-semantic-string" style="margin-inline-start: 24px;">1.2 Understanding Retrieval-Augmented Generation (RAG)</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580b78f06c6a97022bc5c"><div class="notion-semantic-string" style="margin-inline-start: 24px;">1.3 The Essence of Multi-Step Inference in LLMs</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf58034b829fad21730876f"><div class="notion-semantic-string" style="margin-inline-start: 24px;">1.4 Purpose and Scope of the Report</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5803d9807e94a97e877c9"><div class="notion-semantic-string" style="margin-inline-start: 0px;">2. Chunking and Retrieval in RAG Architectures</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580e68ebfe97e77da4cf0"><div class="notion-semantic-string" style="margin-inline-start: 24px;">2.1 Principles of Document Chunking for RAG</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5807b8ea1e3f0ef8cce21"><div class="notion-semantic-string" style="margin-inline-start: 24px;">2.2 Overview of Retrieval Mechanisms in RAG</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf58015b472e155c66d727f"><div class="notion-semantic-string" style="margin-inline-start: 48px;">3. The Critical Role of Chunk Retrieval Sequence</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580c4a559c3eff45d8257"><div class="notion-semantic-string" style="margin-inline-start: 24px;">3.1 Impact of Context Order on LLM Performance</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580d8a90af2ad47b882d5"><div class="notion-semantic-string" style="margin-inline-start: 24px;">3.2 Document's Original Structure (DOS RAG) and its Benefits</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5808f9238d67cfb9d9d82"><div class="notion-semantic-string" style="margin-inline-start: 24px;">3.3 Reranking Strategies and Context Reordering</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5807c929cdc484058c538"><div class="notion-semantic-string" style="margin-inline-start: 0px;">4. Factors Influencing Multi-Step Inference Performance in RAG</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5805f9dc2d7b972b2041d"><div class="notion-semantic-string" style="margin-inline-start: 24px;">4.1 Positional Bias and the "Lost-in-the-Middle" Effect</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf5804cb009d3d8e8e002a5"><div class="notion-semantic-string" style="margin-inline-start: 24px;">4.2 The Detrimental Impact of Irrelevant and Distracting Information</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580e2a4f9c97d9434d4cb"><div class="notion-semantic-string" style="margin-inline-start: 24px;">4.3 Cognitive Load and Context Window Management</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580e4a606e6f4424b31e5"><div class="notion-semantic-string" style="margin-inline-start: 0px;">5. Empirical Evidence and Performance Analysis</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580ad845cdef42ba8a4e7"><div class="notion-semantic-string" style="margin-inline-start: 24px;">5.1 Case Studies on Chunk Order and Multi-Step Question Answering</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580889f7bc0535e95d69a"><div class="notion-semantic-string" style="margin-inline-start: 24px;">5.2 Evaluation Benchmarks and Metrics</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580e0a67ddf16d23ac95b"><div class="notion-semantic-string" style="margin-inline-start: 0px;">6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf58085858fd4791e4f101b"><div class="notion-semantic-string" style="margin-inline-start: 24px;">6.1 Best Practices for Chunking and Reordering</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580e49c36f4912bd6ac93"><div class="notion-semantic-string" style="margin-inline-start: 24px;">6.2 Strategies for Mitigating Positional Bias and Distraction</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580b3bb55f18aef19ec66"><div class="notion-semantic-string" style="margin-inline-start: 24px;">6.3 Advanced Techniques for Multi-Hop Reasoning</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf58057bc2bc6c92d18067c"><div class="notion-semantic-string" style="margin-inline-start: 0px;">7. Conclusion and Future Directions</div></a></li><li class="notion-table-of-contents__item"><a class="notion-link" href="#block-21740d70fdf580388756d009905ff072"><div class="notion-semantic-string" style="margin-inline-start: 0px;">References</div></a></li></ul><div id="block-21740d70fdf5804496dffff09e93c171" class="notion-divider"></div><div id="block-21740d70fdf58019b4e5f82038300e4e" class="notion-text"></div><span class="notion-heading__anchor" id="21740d70fdf580789318c4b68c9dc180"></span><h1 id="block-21740d70fdf580789318c4b68c9dc180" class="notion-heading notion-semantic-string"><strong>1. Introduction: The Interplay of RAG, LLMs, and Multi-Step Reasoning</strong></h1><p id="block-21740d70fdf580b59114ebb95e594cb4" class="notion-text notion-text__content notion-semantic-string">The landscape of artificial intelligence has been profoundly reshaped by the emergence of Large Language Models (LLMs), which demonstrate remarkable abilities in understanding and generating human-like text. However, their inherent limitations have spurred the development of advanced frameworks like Retrieval-Augmented Generation (RAG) to unlock even more sophisticated capabilities, particularly in multi-step inference. This report delves into the intricate relationship between the sequence in which information is retrieved and presented to LLMs within RAG systems and its subsequent effect on their ability to perform complex, multi-step reasoning.</p><span class="notion-heading__anchor" id="21740d70fdf580998476c769158dc800"></span><h2 id="block-21740d70fdf580998476c769158dc800" class="notion-heading notion-semantic-string"><strong>1.1 Defining Large Language Models (LLMs) and their Reasoning Capabilities</strong></h2><p id="block-21740d70fdf580ae8f73f007b0a8ec92" class="notion-text notion-text__content notion-semantic-string">Large Language Models are sophisticated artificial intelligence systems built upon deep neural networks, trained on vast datasets of text to interpret natural language and generate human-like responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> These models comprise numerous layers of neural networks, featuring billions of parameters that are fine-tuned during training. Their architecture is further enhanced by attention mechanisms, which enable them to focus on specific parts of the input data, thereby improving their contextual understanding.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> LLMs demonstrate proficiency across a wide array of natural language processing tasks, including language translation, text summarization, question-answering, and content generation.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> Through extensive training, they acquire a deep understanding of grammar, semantics, and complex conceptual relationships inherent in human language.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5807ba813e037c8c3158e" class="notion-text notion-text__content notion-semantic-string">Despite their impressive performance, LLMs face several inherent limitations. A significant challenge is their reliance on static, pre-trained data, which means their knowledge base is frozen at the time of training.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> This characteristic can lead to outdated or potentially inaccurate responses and a phenomenon known as "hallucinations", where the model generates factually incorrect or nonsensical information not present in its training data.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, LLMs often struggle with complex logical reasoning, particularly tasks requiring sophisticated deductive, inductive, or adductive inference, and can sometimes produce self-contradictory outputs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> These fundamental constraints, especially the static nature of their knowledge and the propensity for factual errors, directly highlighted the necessity for external knowledge augmentation techniques. A mechanism was required to inject dynamic, up-to-date, and verifiable information at inference time, leading to the emergence and widespread adoption of RAG.</p><div id="block-21740d70fdf580e8a51ad82068f715f8" class="notion-text"></div><span class="notion-heading__anchor" id="21740d70fdf5804ea1b3c814f4776242"></span><h2 id="block-21740d70fdf5804ea1b3c814f4776242" class="notion-heading notion-semantic-string"><strong>1.2 Understanding Retrieval-Augmented Generation (RAG)</strong></h2><p id="block-21740d70fdf5804ea49fd96ef424e931" class="notion-text notion-text__content notion-semantic-string">
Retrieval-Augmented Generation (RAG) is an AI framework designed to optimize the output of LLMs by enabling them to reference an authoritative knowledge base external to their training data before generating a response.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This framework effectively combines the strengths of traditional information retrieval systems, such as search engines and databases, with the generative capabilities of large language models.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58009ba9df8dfcba9a012" class="notion-text notion-text__content notion-semantic-string">The core process of RAG typically involves two main stages. First, <strong>Retrieval and Pre-processing</strong> occurs, where powerful search algorithms query external data sources, including web pages, knowledge bases, and databases. Once retrieved, this relevant information undergoes pre-processing steps such as tokenization, stemming, and the removal of stop words.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The second stage is <strong>Grounded Generation</strong>, where the pre-processed, retrieved information is seamlessly incorporated into the pre-trained LLM's context. This integration significantly enhances the LLM's understanding of the topic, allowing it to produce more precise, informative, and engaging responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580849758fee1679a61da" class="notion-text notion-text__content notion-semantic-string">RAG offers several distinct advantages over conventional text generation methods, particularly for factual or data-driven responses. It provides LLMs with access to fresh, up-to-date information, overcoming the limitations of their pre-trained data.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This factual grounding is crucial for mitigating "gen AI hallucinations" by supplying verifiable facts as part of the input prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The framework also leverages advanced search techniques, including vector databases and relevancy re-rankers, to ensure that the most pertinent information is retrieved, thereby improving the overall relevance, accuracy, and quality of the LLM's outputs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This capability effectively transforms the LLM from a purely generative model into a knowledge-aware reasoning engine, capable of producing responses grounded in verifiable facts.</p><span class="notion-heading__anchor" id="21740d70fdf580b78f06c6a97022bc5c"></span><h2 id="block-21740d70fdf580b78f06c6a97022bc5c" class="notion-heading notion-semantic-string"><strong>1.3 The Essence of Multi-Step Inference in LLMs</strong></h2><p id="block-21740d70fdf580e887d9cfc1af9f673c" class="notion-text notion-text__content notion-semantic-string">Multi-step inference, also referred to as multi-step reasoning or multi-task inference, denotes an LLM's capacity to process multiple pieces of information in a sequential manner, apply logical operations, and execute a series of sub-tasks to arrive at a conclusion.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> This capability extends beyond merely following a single instruction or performing a singular task.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580b3a0d7c0300ffd118a" class="notion-text notion-text__content notion-semantic-string">The ability to perform multi-step reasoning is paramount for addressing complex, real-world challenges where each subsequent step builds upon the preceding one, demanding a deeper level of comprehension and structured problem-solving.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> It is widely recognized as a key indicator of advanced intelligence in AI systems.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> However, LLMs frequently encounter difficulties with intricate logical problems that necessitate sophisticated deductive, inductive, or adductive reasoning. They can also exhibit a tendency to produce self-contradictory responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>5</mn></msup></mrow><annotation encoding="application/x-tex">^5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> While existing datasets for multi-hop reasoning, such as HotpotQA and StrategyQA, are designed to test internal reasoning processes, they do not always offer a comprehensive method for assessing the accuracy of intermediate steps or for comparing concurrent versus sequential processing approaches.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d1b729df508408cc3d" class="notion-text notion-text__content notion-semantic-string">To address these assessment gaps, new evaluation benchmarks have been developed. The MTI Bench, for instance, is specifically designed to analyze the multi-task inference capabilities of LLMs, differentiating between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> Similarly, ProcBench focuses on evaluating multi-step reasoning by presenting LLMs with explicit instructions and questions that require strict adherence to provided steps.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> The increasing emphasis on these specialized benchmarks indicates a significant evolution in LLM evaluation. It reflects a growing understanding that raw knowledge alone is insufficient; LLMs must also possess robust structured processing capabilities to be truly effective. <!-- -->This shift underscores that future advancements in LLMs and RAG systems must prioritize not just <em><strong><u>what</u></strong></em> information is retrieved, but <em><strong><u>how</u></strong></em> that information facilitates a structured<!-- -->, step-by-step problem-solving process.<!-- --> This elevates the importance of context organization and coherence within the input.</p><span class="notion-heading__anchor" id="21740d70fdf58034b829fad21730876f"></span><h2 id="block-21740d70fdf58034b829fad21730876f" class="notion-heading notion-semantic-string"><strong>1.4 Purpose and Scope of the Report</strong></h2><p id="block-21740d70fdf580a7a5e1c5589a27f4b4" class="notion-text notion-text__content notion-semantic-string">The primary objective of this report is to analyze the intricate relationship between the chunk retrieval sequence within RAG frameworks and the multi-step inference performance of Large Language Models. The scope of this analysis encompasses a detailed examination of various chunking strategies, the mechanisms of information retrieval, and a critical assessment of how the order in which information is presented influences an LLM's capacity to execute complex reasoning tasks. The report will integrate empirical findings from recent studies, discuss pervasive challenges such as positional bias and the impact of distracting information, and propose optimization strategies derived from these observations. This document is intended for AI/ML Researchers, Senior AI Engineers, and Technical Leads seeking to enhance the robustness and efficiency of RAG systems for knowledge-intensive applications.</p><span class="notion-heading__anchor" id="21740d70fdf5803d9807e94a97e877c9"></span><h1 id="block-21740d70fdf5803d9807e94a97e877c9" class="notion-heading notion-semantic-string"><strong>2. Chunking and Retrieval in RAG Architectures</strong></h1><p id="block-21740d70fdf580428548dda1c2b7ff6a" class="notion-text notion-text__content notion-semantic-string">The efficacy of Retrieval-Augmented Generation (RAG) systems heavily relies on how external knowledge is prepared and accessed. This involves two foundational processes: chunking, which breaks down large documents into manageable pieces, and retrieval, which identifies and fetches the most relevant of these pieces. Understanding these processes is crucial for appreciating how the sequence of retrieved information impacts LLM performance.</p><span class="notion-heading__anchor" id="21740d70fdf580e68ebfe97e77da4cf0"></span><h2 id="block-21740d70fdf580e68ebfe97e77da4cf0" class="notion-heading notion-semantic-string"><strong>2.1 Principles of Document Chunking for RAG</strong></h2><p id="block-21740d70fdf580ceb7d1e138749fece9" class="notion-text notion-text__content notion-semantic-string">Chunking, in the context of AI, refers to the process of dividing extensive documents into smaller, more manageable segments known as chunks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>12</mn></msup></mrow><annotation encoding="application/x-tex">^{12}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span></span></span></span></span> These segments can vary in granularity, ranging from entire paragraphs or individual sentences to token-limited blocks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> The primary purpose of chunking is to enhance the efficiency of both retrieval and subsequent processing by the LLM.</p><p id="block-21740d70fdf5802594fac1a34fc2b5be" class="notion-text notion-text__content notion-semantic-string">The necessity of chunking arises from the vastness of knowledge bases, which can contain millions of words or documents. Without effective chunking, retrieving relevant information efficiently from such large datasets would be computationally prohibitive.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> By breaking down documents, chunking enables more precise matching between user queries and relevant text, thereby reducing noise and the inclusion of irrelevant information. Moreover, smaller chunks are processed more rapidly and utilize memory more efficiently, allowing RAG systems to handle large datasets effectively.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580aca4b9f403a1abb511" class="notion-text notion-text__content notion-semantic-string">Several chunking strategies are employed, each with distinct advantages and use cases:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf5807685ece0dd6484ea2e" class="notion-list-item notion-semantic-string"><strong>Fixed Size Chunking:</strong> This straightforward approach divides text into uniform chunks based on a predefined character or token count.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> For instance, a document might be split into 500-token chunks, often with an overlap feature to maintain context across boundaries and prevent loss of meaning.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> While simple to implement, efficient for large datasets, and consistent in size, this method can lead to context fragmentation, splitting sentences or logical units. Its inflexibility makes it sub-optimal for heterogeneous content.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58026839cfc6fc508c42b" class="notion-list-item notion-semantic-string"><strong>Recursive-Based Chunking:</strong> A more adaptive strategy, this method breaks text into chunks by applying multiple separators (e.g., paragraphs, sentences, or specific markers) in a specified order of importance. The goal is to identify the most meaningful boundaries within the text, thereby preserving logical flow.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580efb9a6db405332f049" class="notion-list-item notion-semantic-string"><strong>Sentence-based Chunking:</strong> This method ensures that each chunk contains complete thoughts by dividing text into full sentences. It helps maintain the natural logical progression of information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5800fae7ed100c5ecd009" class="notion-list-item notion-semantic-string"><strong>Document Structure-based Chunking:</strong> This approach chunks documents according to their inherent structural integrity, such as individual sections, headings, or even specific charges within a legal document. This method is crucial for ensuring that key information and its surrounding context remain intact, implicitly supporting narrative continuity.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580959badf2d68cc1f896" class="notion-list-item notion-semantic-string"><strong>Semantic Chunking:</strong> This strategy involves segmenting documents into semantically coherent and non-overlapping chunks that are more closely aligned with the specific information needs of a query.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf580c0b810f7633ce50e32" class="notion-text notion-text__content notion-semantic-string">The choice of chunking strategy introduces a critical trade-off between simplicity and efficiency on one hand (e.g., fixed-size chunking) and context preservation and accuracy on the other (e.g., recursive, semantic, or structure-based chunking). For multi-step inference, where the LLM must connect information across multiple segments to build a coherent understanding, chunking strategies that prioritize contextual integrity over simple size uniformity are likely to yield superior results. This is because fixed-size chunking risks breaking logical units, which can impede the LLM's ability to follow a sequential argument. Therefore, the optimal approach to chunking is not universal but depends heavily on the document type and the complexity of the queries, with multi-step reasoning tasks often benefiting significantly from meaningful segmentation that supports logical flow.</p><span class="notion-heading__anchor" id="21740d70fdf5807b8ea1e3f0ef8cce21"></span><h2 id="block-21740d70fdf5807b8ea1e3f0ef8cce21" class="notion-heading notion-semantic-string"><strong>2.2 Overview of Retrieval Mechanisms in RAG</strong></h2><p id="block-21740d70fdf580408d4fc76f4eef75dc" class="notion-text notion-text__content notion-semantic-string">A RAG system is fundamentally composed of three key modules that work in concert to enhance LLM performance. First, a <strong>Query Encoder</strong> transforms the user's input query into a representation suitable for searching the knowledge base.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> Second, a <strong>Retriever</strong> takes this query representation and fetches a ranked list of relevant documents or chunks from a vast corpus.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>Finally, a <strong>Generator</strong>, typically a pre-trained LLM, conditions its output on both the original input query and the retrieved documents to produce the final response.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>
</p><p id="block-21740d70fdf580d78798c7861511ed2e" class="notion-text notion-text__content notion-semantic-string">Retrievers can be broadly categorized based on their underlying mechanisms:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf5805299ddcb51cebbc031" class="notion-list-item notion-semantic-string"><strong>Sparse Retrievers:</strong> These methods rely on keyword matching, such as the BM25 algorithm, to identify relevant documents.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580f880d2ce188348e4e8" class="notion-list-item notion-semantic-string"><strong>Dense Retrievers:</strong> Utilizing embeddings, these retrievers perform semantic similarity searches within vector databases. This allows for fast and accurate retrieval based on the meaning of the query rather than just keyword overlap.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580778da5dc116592082c" class="notion-list-item notion-semantic-string"><strong>Hybrid Search:</strong> Many advanced RAG systems combine both semantic and keyword search techniques to achieve a more comprehensive and relevant set of results.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span>
The retrieval process in RAG involves powerful search algorithms querying external data sources. Prior to lookup, sophisticated search engines may even transform queries and correct misspellings to optimize relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> After the initial retrieval, an essential step often involves </li><li id="block-21740d70fdf580f39177ff83be84b5c3" class="notion-list-item notion-semantic-string"><strong>re-rankers</strong>. These components act as a second-pass filter, reordering the retrieved documents or chunks based on a more refined assessment of their relevance to the query. The top-K most relevant chunks are then passed to the generator as factual context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> This re-ranking step is critical for ensuring that the LLM receives the most pertinent information, effectively reducing noise and improving the overall quality and accuracy of the generated output.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The effectiveness of RAG is therefore highly dependent on the retriever's ability to provide relevant information and the re-ranker's capacity to prioritize the most pertinent chunks. If the retriever fetches irrelevant or noisy information, the LLM's performance can degrade, leading to responses that, while "grounded" in the provided context, might be off-topic or factually incorrect.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> The re-ranker serves as a crucial gatekeeper, refining these initial results to ensure that only the highest-quality, most relevant information is presented to the LLM. This highlights that successful retrieval is not merely about finding any relevant information, but about identifying the  <!-- --><em>most relevant</em> and <!-- --><em>least distracting</em> content, a factor that profoundly influences the subsequent chunk ordering.<!-- --></li></ul><span class="notion-heading__anchor" id="21740d70fdf58015b472e155c66d727f"></span><h3 id="block-21740d70fdf58015b472e155c66d727f" class="notion-heading notion-semantic-string"><strong>3. The Critical Role of Chunk Retrieval Sequence</strong></h3><p id="block-21740d70fdf580a68709d4be6ca361e1" class="notion-text notion-text__content notion-semantic-string">The order in which retrieved chunks are presented to a Large Language Model is not a trivial detail but a critical determinant of its performance, particularly for tasks requiring multi-step inference. This section explores how context order directly influences an LLM's ability to reason effectively.</p><span class="notion-heading__anchor" id="21740d70fdf580c4a559c3eff45d8257"></span><h2 id="block-21740d70fdf580c4a559c3eff45d8257" class="notion-heading notion-semantic-string"><strong>3.1 Impact of Context Order on LLM Performance</strong></h2><p id="block-21740d70fdf5800585f8e60d24de38aa" class="notion-text notion-text__content notion-semantic-string">Observations indicate that the sequence in which text chunks are retrieved and subsequently presented to an LLM significantly influences its overall performance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This impact extends beyond simple relevance sorting, suggesting a deeper interaction with the LLM's internal processing mechanisms. LLMs demonstrate a distinct preference for premise order in reasoning tasks, achieving optimal performance when the information sequence aligns with the intermediate steps required for logical deduction.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> For example, in deductive reasoning problems, presenting premises in the same order as a ground truth proof can drastically increase the model's accuracy.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that LLMs operate more effectively when processing information in a left-to-right, sequential manner, rather than having to search back and forth across a disordered context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580aebbeaf862f0f1bb84" class="notion-text notion-text__content notion-semantic-string">Conversely, permuting the order of premises can lead to a substantial performance degradation, with drops exceeding 30% observed in some LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> This "ordering effect" is further exacerbated when irrelevant premises are introduced into the prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> When the context provided to the LLM is disjointed or randomly shuffled, it negatively impacts the model's ability to synthesize information and produce coherent responses.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This indicates that LLMs, despite their advanced capabilities, exhibit a form of "cognitive linearity" in their processing. They perform optimally when information is presented in a sequential, logically flowing manner. This observation challenges the assumption that LLMs can perfectly synthesize information regardless of its arrangement within the context window. The consistent improvement seen when premises are ordered according to a "ground truth proof" <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> suggests that the LLM's internal mechanisms, possibly due to their auto-regressive design or biases learned from training data, are more efficient when information is presented sequentially. This parallels human cognitive processes, where understanding is often built step-by-step. If information is jumbled, the LLM must expend additional computational effort to re-establish logical connections, which can lead to reduced performance. For multi-step inference, which inherently relies on sequential processing and building upon previous deductions, maintaining a coherent narrative or logical progression in the input context becomes paramount.</p><span class="notion-heading__anchor" id="21740d70fdf580d8a90af2ad47b882d5"></span><h2 id="block-21740d70fdf580d8a90af2ad47b882d5" class="notion-heading notion-semantic-string"><strong>3.2 Document's Original Structure (DOS RAG) and its Benefits</strong></h2><p id="block-21740d70fdf580ba874bff70dae33e6c" class="notion-text notion-text__content notion-semantic-string">Document's Original Structure RAG (DOS RAG) is a retrieve-then-read strategy that introduces a crucial refinement to the standard RAG pipeline. Instead of solely sorting retrieved chunks by their similarity score to the query, DOS RAG reorders these chunks to match their original sequence within the source document.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This reordering is made possible by tracking the original positions of the chunks during the initial processing phase.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580a281e2d404fdd2bcc7" class="notion-text notion-text__content notion-semantic-string">The benefits of DOS RAG are significant and empirically validated. It primarily <strong>preserves passage continuity</strong>, maintaining the document's structural integrity and narrative flow.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> This is particularly crucial for tasks that require understanding underlying narratives or performing complex multi-hop question answering. Studies consistently show that DOS RAG achieves 
<strong>improved accuracy</strong>, outperforming traditional Vanilla RAG (which relies on relevance-sorted chunks) across various benchmarks, including Bench, QuALITY, and NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This performance gain is especially pronounced when the retrieval budget is expanded to tens of thousands of tokens.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> For instance, on the Bench, DOS RAG reached 93.1% accuracy at 30K tokens, surpassing Vanilla RAG's 87.8%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, DOS RAG demonstrates notable 
<strong>efficiency</strong>, often achieving superior results while utilizing fewer tokens compared to more complex multi-stage methods like ReadAgent.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that the added complexity of multi-stage approaches does not always translate to better performance when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580848881d08f51d59cdf" class="notion-text notion-text__content notion-semantic-string">The consistent empirical outperformance of DOS RAG over relevance-sorted retrieval fundamentally challenges the prevailing assumption that semantic similarity alone dictates optimal chunk presentation. This observation highlights that for multi-step reasoning, <em>contextual coherence</em> and <em>narrative flow</em>, as preserved by the original document order, are often more critical than isolated high-relevance scores. Traditional RAG pipelines often prioritize retrieving chunks based on their individual semantic similarity to the query, then sorting them by this score, with the expectation that the LLM will best utilize the most relevant information first.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> However, DOS RAG's consistent superiority demonstrates that for tasks requiring multi-step reasoning or the understanding of a narrative, the <em>relationship</em> between chunks (specifically, their original sequence) is more valuable than their individual relevance rank.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> Complex reasoning frequently requires building a mental model from sequential information, where each piece logically follows the last.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> Disrupting this natural flow, even with highly relevant but disjointed chunks, can increase the LLM's processing burden and hinder its ability to perform multi-hop reasoning effectively. This implies that the definition of "relevance" for multi-step tasks should be broadened to include "contextual relevance" or "narrative relevance" in addition to traditional semantic similarity.</p><span class="notion-heading__anchor" id="21740d70fdf5808f9238d67cfb9d9d82"></span><h2 id="block-21740d70fdf5808f9238d67cfb9d9d82" class="notion-heading notion-semantic-string"><strong>3.3 Reranking Strategies and Context Reordering</strong></h2><p id="block-21740d70fdf58081a3ecd927972a059f" class="notion-text notion-text__content notion-semantic-string">Reranking serves as a crucial second-pass filter in RAG systems, refining the initial set of retrieved documents or chunks by reordering them based on a more precise assessment of query-document relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> This process is vital for enhancing the quality of the context provided to the LLM, ensuring that the most pertinent information is presented, and ultimately helping to filter out irrelevant documents that could lead to hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span>
Various types of rerankers are employed, each with distinct characteristics:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf58058a46cfc4b81aaecc4" class="notion-list-item notion-semantic-string"><strong>Cross-Encoders:</strong> These models analyze the query and document pair together, enabling a deep and nuanced understanding of their relevance. They offer high precision but are generally computationally intensive.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include Sentence Transformers, Flashrank, and BGE-M3.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5807490e3ef6ba727c883" class="notion-list-item notion-semantic-string"><strong>Multi-Vector Rerankers:</strong> Models like ColBERT use a "late interaction" approach, encoding query and document representations independently before their interaction and relevance scoring occur. This approach balances performance and efficiency.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58003b9c1f7a840f17ca1" class="notion-list-item notion-semantic-string"><strong>Fine-tuned LLM Rerankers:</strong> Pre-trained LLMs are fine-tuned on specific ranking datasets (e.g., MS MARCO) to enhance their ability to measure query-document relevance.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> These can be structured as encoder-decoder models (e.g., RankT5) or decoder-only models (e.g., RankZephyr, RankGPT).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5807382e1cfca2e9abdb2" class="notion-list-item notion-semantic-string"><strong>LLM as a Judge:</strong> This approach leverages the inherent reasoning capabilities of LLMs to directly assess document relevance through various prompting strategies, including pointwise, listwise, and pairwise methods.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> While offering competitive effectiveness, the high computational cost and latency associated with using LLMs directly for reranking can be a practical barrier.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include GPT, Claude, and Gemini.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5802cb8e3e90f285484c4" class="notion-list-item notion-semantic-string"><strong>Reranking APIs:</strong> Commercial services provide convenient solutions for semantic relevance enhancement without requiring significant infrastructure investment.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Examples include Cohere, Jina, and Mixedbread.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf5808da1edd71553e8d49c" class="notion-text notion-text__content notion-semantic-string">Beyond simple relevance scoring, context reordering within the reranking process also plays a role. <strong>Inverted Context Ordering</strong> is one such strategy, where retrieved or reranked documents are arranged in descending order of relevance, with the highest-ranked document placed immediately before the question.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> This method has demonstrated a performance increase in correctness for multi-hop QA tasks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> Other advanced approaches include <strong>Fusion-based Reranking</strong>, which aggregates evidence from multiple query variants (e.g., RAG-Fusion, R2AG) and is particularly effective for multi-hop and ambiguous tasks <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span>, and <strong>Adaptive Reranking</strong>, which dynamically adjusts the number of documents reranked based on query complexity (e.g., RLT, ToolRerank).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5807d866be836369721de" class="notion-text notion-text__content notion-semantic-string">While reranking is essential for refining relevance, certain advanced reranking methods (e.g., LLM-as-a-judge, Rank-R1) introduce significant computational overhead. Their benefits might be offset by increased latency, especially for real-time applications or when simpler methods like DOS RAG already leverage long context windows effectively. This creates an optimization paradox where "better" relevance comes at a cost that might negate its practical advantage. The primary goal of reranking is to provide the LLM with the <em>most</em> relevant context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span> However, methods like Rank-R1, despite their explicit reasoning capabilities, can take up to 100 seconds for a single query, making them impractical for time-constrained scenarios.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span> This illustrates a critical trade-off: a more sophisticated reranker might theoretically provide a more perfectly ordered context, but the practical latency introduced can severely impact the overall system's usability and efficiency. Furthermore, the success of DOS RAG suggests that simply reordering by original document flow can be more effective than complex relevance-based reranking for multi-step tasks, especially with long-context LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This implies that the "best" reranking strategy is not solely about maximizing relevance scores but about achieving a holistic balance with operational constraints and the specific reasoning demands of the LLM.
<strong>
Table 1: Comparison of Key Chunking and Reordering Strategies in RAG</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Strategy</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Description</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Primary Goal</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Impact on Multi-Step Inference</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Advantages</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Disadvantages</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Fixed Size Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides text into uniform segments (e.g., 500 tokens), often with overlap.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Efficiency, Simplicity</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can fragment context, hindering logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Easy to implement, fast, consistent.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Context fragmentation, information loss, inflexible.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Recursive-Based Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Uses multiple separators (paragraphs, sentences) to find meaningful boundaries.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Context Preservation</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Better at maintaining logical units for sequential understanding.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Adaptive, preserves logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">More complex to implement.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Sentence-based Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides text into complete sentences.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserve Complete Thoughts</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Supports logical flow, good for connecting ideas.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Ensures complete thoughts, natural boundaries.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">May create very small chunks, less efficient for long documents.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">13</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Document's Original Structure (DOS RAG)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Retrieves chunks and reorders them to match their original document sequence.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Narrative Continuity, Contextual Coherence</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Significantly improves performance by maintaining logical progression; crucial for multi-hop QA.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserves narrative, robust QA, often outperforms relevance-based sorting.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires tracking chunk positions; may include less relevant chunks if not filtered.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Inverted Context Ordering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Arranges retrieved/reranked documents in descending order of relevance, highest-ranked before query.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Prioritize Most Relevant</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can improve correctness; focuses LLM on key information.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Directs LLM to top relevant info immediately.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Still relies on relevance score, may disrupt original narrative flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">16</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Semantic Chunking</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Divides documents into semantically coherent and non-overlapping chunks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduce Irrelevance, Improve Accuracy</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Enhances reliability for fact-checking and multi-hop reasoning by filtering less pertinent chunks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduces hallucinations, improves factual accuracy, aligned with query needs.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires sophisticated LLM-based relevance scoring.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf5807c929cdc484058c538"></span><h1 id="block-21740d70fdf5807c929cdc484058c538" class="notion-heading notion-semantic-string"><strong>4. Factors Influencing Multi-Step Inference Performance in RAG</strong></h1><p id="block-21740d70fdf580f9821be4ec0e5fc9d4" class="notion-text notion-text__content notion-semantic-string">Beyond the direct ordering of retrieved chunks, several other factors interact with the LLM's context window and the presentation of information to significantly affect its ability to perform multi-step inference. These factors highlight the complexities involved in designing truly effective RAG systems.</p><span class="notion-heading__anchor" id="21740d70fdf5805f9dc2d7b972b2041d"></span><h2 id="block-21740d70fdf5805f9dc2d7b972b2041d" class="notion-heading notion-semantic-string"><strong>4.1 Positional Bias and the "Lost-in-the-Middle" Effect</strong></h2><p id="block-21740d70fdf5806b9a45ea2b164dfdd2" class="notion-text notion-text__content notion-semantic-string">Positional bias refers to the observed tendency of Large Language Models to assign different weights or importance to information based on its location within the input prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> A specific manifestation of this is the "lost-in-the-middle" effect, where LLMs tend to focus predominantly on text appearing at the beginning or end of their prompt, often overlooking content situated in the middle.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This bias can affect both the LLM's capacity to leverage relevant passages effectively and its susceptibility to being misled by distracting ones.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Even with the implementation of advanced positional encoding methods, LLMs can still be influenced by this phenomenon.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d4ba66c5c4765273a0" class="notion-text notion-text__content notion-semantic-string">While earlier analyses frequently reported a prominent positional bias in controlled experimental settings, for instance, by rotating the position of a single relevant passage within an otherwise irrelevant context, its impact has been found to be marginal in real-world RAG scenarios.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This difference arises because practical retrieval pipelines often return both genuinely relevant and highly distracting passages simultaneously. In such complex contexts, the positional bias penalizes both types of passages, effectively balancing out its overall impact.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Consequently, sophisticated strategies that attempt to rearrange passages based on an LLM's presumed positional preferences (e.g., placing the most relevant information at the beginning or end) do not consistently outperform random shuffling in real-world applications.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>26</mn></msup></mrow><annotation encoding="application/x-tex">^{26}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">26</span></span></span></span></span></span></span></span></span></span></span></span></span> This is attributed to a "contrastive effect", where the benefit of strategically placing relevant passages is counterbalanced by the unintended placement of highly distracting passages in those same favoured positions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, some LLMs, particularly those with high closed-book accuracy, may exhibit a "parametric bias", relying more on their pre-trained knowledge than on the provided context, especially when relevant passages are not in preferential positions. This can negatively influence their ability to effectively read and utilize external information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580459646fcd5a2a23040" class="notion-text notion-text__content notion-semantic-string">The "lost-in-the-middle" effect and positional bias are not simple, direct inhibitors in RAG but rather complex phenomena whose impact is modulated by the simultaneous presence of both relevant and distracting information. This suggests that merely reordering chunks to "trick" the LLM into overcoming positional bias is often ineffective. A more fundamental solution lies in improving the <em>quality</em> of retrieved content and enhancing the LLM's inherent robustness to distraction. Initial research on positional bias often used simplified setups, leading to conclusions that LLMs heavily ignore middle content.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> However, in practical RAG systems, where retrievers often fetch <em>both</em> relevant and highly distracting passages <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span>, the impact of positional bias becomes less pronounced. This is because the bias penalizes both beneficial and detrimental information, creating a complex interplay. Therefore, simply trying to place the "best" chunks at the beginning or end is not a guaranteed solution, as highly distracting chunks might also end up in those favored positions, negating the intended benefit.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>27</mn></msup></mrow><annotation encoding="application/x-tex">^{27}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">27</span></span></span></span></span></span></span></span></span></span></span></span></span> This shifts the focus from <em>where</em> to place chunks to <em>what</em> chunks are retrieved in the first place, and how resilient the LLM is to imperfect retrieval.</p><span class="notion-heading__anchor" id="21740d70fdf5804cb009d3d8e8e002a5"></span><h2 id="block-21740d70fdf5804cb009d3d8e8e002a5" class="notion-heading notion-semantic-string"><strong>4.2 The Detrimental Impact of Irrelevant and Distracting Information</strong></h2><p id="block-21740d70fdf58004ad5fd2eeaf65e692" class="notion-text notion-text__content notion-semantic-string">A well-documented issue in Retrieval-Augmented Generation (RAG) is the negative influence of irrelevant and distracting information. Irrelevant passages are defined as those that do not provide useful information for answering the query. A particularly problematic subset, "distracting passages", contains information that is irrelevant yet semantically related to the query, which can actively mislead the LLM.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580ba9bdfd3bff3ab2e3e" class="notion-text notion-text__content notion-semantic-string">The presence of distracting passages can cause LLMs to generate incorrect responses, significantly degrading accuracy even when a truly relevant document is also present in the prompt.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> Studies have shown that "hard distracting passages", those with a high quantifiable distracting effect, cause a larger accuracy drop (ranging from 6 to 11 percentage points) compared to "weak" ones, and this detrimental effect persists even in larger LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span>
Paradoxically, "stronger" retrievers, while designed to maximize the recall of relevant information, can inadvertently deliver <em>more harmful distractors</em>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This occurs because these retrievers are highly effective at finding semantically similar content, which can include misleading but related information. Reranking, while generally beneficial, can also amplify this problem by increasing the average distracting effect of irrelevant passages that end up in top positions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> Researchers are exploring methods for generating synthetic distracting passages (e.g., related topics, hypothetical scenarios, negations) to improve LLM robustness to such noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580f7923dca2a57ce5607" class="notion-text notion-text__content notion-semantic-string">The very act of retrieval, especially with "stronger" retrievers, presents a "double-edged sword." While it aims to increase the recall of relevant information, it simultaneously increases the likelihood of introducing highly distracting, semantically similar but ultimately unhelpful information. This means that RAG system design must prioritize not just recall, but also robust filtering and LLM resilience to noise. RAG's core purpose is to provide relevant external knowledge.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">^4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span> However, no retriever is perfect, and they often return irrelevant or "distracting" passages.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> The critical observation here is that <em>stronger</em> retrievers, which are designed to find more relevant information, also tend to retrieve <em>more harmful</em> distracting passages.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This creates a paradox: improving the retriever's primary function (recall) can exacerbate the problem of distraction. Therefore, simply optimizing retrieval for "relevance" (as traditionally defined) is insufficient. RAG systems must also incorporate mechanisms, such as robust reranking or LLM fine-tuning with hard negative examples, that specifically address the <em>distracting effect</em> to ensure true performance gains, especially for multi-step tasks where a single misleading piece of information can derail the entire reasoning chain.</p><span class="notion-heading__anchor" id="21740d70fdf580e2a4f9c97d9434d4cb"></span><h2 id="block-21740d70fdf580e2a4f9c97d9434d4cb" class="notion-heading notion-semantic-string"><strong>4.3 Cognitive Load and Context Window Management</strong></h2><p id="block-21740d70fdf580d28441d48edb42e729" class="notion-text notion-text__content notion-semantic-string">Large Language Models are fundamentally constrained by the knowledge encoded in their parameters and the fixed context window available during inference.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> The concept of "cognitive load", analogous to human information processing, is highly relevant here. Cognitive Load Theory (CLT) categorizes load into intrinsic (content complexity), extraneous (poor instruction design), and germane (schema construction).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>31</mn></msup></mrow><annotation encoding="application/x-tex">^{31}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">31</span></span></span></span></span></span></span></span></span></span></span></span></span> For LLMs, the inherent "content complexity" of the input is a dominant factor influencing their processing efficiency.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>31</mn></msup></mrow><annotation encoding="application/x-tex">^{31}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">31</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580139869c3006b44178b" class="notion-text notion-text__content notion-semantic-string">Presenting an LLM with an excessive number of tool descriptions or a large volume of irrelevant information can saturate its context window, thereby increasing its "cognitive load".<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> This overload can lead to reduced selection accuracy and an increase in hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> Conversely, supplying <em>only</em> the most relevant context, for instance, through mechanisms like RAG-MCP for tool selection, significantly reduces prompt size and complexity. This mitigation of "prompt bloat" directly lowers the LLM's cognitive load.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> By narrowing the choices and freeing up context space for task-specific reasoning, especially in multi-turn dialogues, the LLM's decision-making capabilities are markedly improved.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58011839fd05b5333a270" class="notion-text notion-text__content notion-semantic-string">While long context windows offer the appealing prospect of easy information input, simply pulling in too many chunks can be counterproductive. Beyond a certain point, the inclusion of excessive irrelevant or distracting information can confuse the model, causing performance to decline.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> The key lies in identifying the "sweet spot" for context length, where sufficient information is provided to maximize recall without overwhelming the model with unnecessary noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5809dbe10fa8f1c49222f" class="notion-text notion-text__content notion-semantic-string">The concept of "cognitive load" in LLMs highlights that simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference. Instead, it introduces a critical trade-off where the <em>quality and conciseness</em> of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy. This implies a need for highly precise retrieval and filtering mechanisms. While LLMs are capable of handling long contexts <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span>, the evidence suggests a point of diminishing returns or even negative impact when too much information, particularly irrelevant or distracting content, is included.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This is framed in terms of "cognitive load".<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> If the LLM is forced to "sift through hundreds of distractors" <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span>, it consumes computational resources and can lead to errors. This directly impacts multi-step reasoning, which requires focused attention on relevant facts. Therefore, effective RAG design is not just about <em>what</em> to retrieve, but <em><strong>how much</strong></em> and <em><strong>how clean</strong></em> that retrieved information is, to ensure the LLM can efficiently process and reason over it without being overwhelmed. This reinforces the importance of advanced reranking and filtering techniques that go beyond simple relevance.
<strong>
Table 2: Factors Affecting LLM Performance in Long Contexts</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Factor</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Description</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Impact on Multi-Step Inference</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Interaction with Chunk Order</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Mitigation Strategies</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Positional Bias</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLMs weigh information differently based on its position (e.g., "lost-in-the-middle" effect).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Can cause LLMs to ignore relevant info or be misled by distractors in middle positions.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reordering by relevance alone is ineffective; original document order (DOS RAG) can implicitly mitigate by maintaining flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Improve retrieval quality, LLM robustness to distraction, avoid simple rearrangement.<!-- --></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">25</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Irrelevant/Distracting Information</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Passages that are semantically similar but do not contain the answer or mislead the LLM.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Significantly degrades accuracy, even when relevant info is present; can derail reasoning chain.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Strong retrievers can inadvertently bring more harmful distractors to top ranks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Robust reranking, LLM fine-tuning with hard negatives, query rewriters, chunk filtering.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Cognitive Load/Context Window Overload</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLM struggles to process excessive or noisy information within its limited context.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Reduces selection accuracy, increases hallucinations, hinders efficient reasoning.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Too many chunks (even if somewhat relevant) can overwhelm the model.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Supplying only relevant context, precise chunking, adaptive streaming, efficient filtering.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Lack of Narrative Continuity</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Disjointed or shuffled presentation of information.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Impairs sequential reasoning, makes it harder for LLM to build a coherent understanding.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Direct result of relevance-only sorting; addressed by DOS RAG.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Preserving original document structure (DOS RAG) or logical flow.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">17</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf580e4a606e6f4424b31e5"></span><h1 id="block-21740d70fdf580e4a606e6f4424b31e5" class="notion-heading notion-semantic-string"><strong>5. Empirical Evidence and Performance Analysis</strong></h1><p id="block-21740d70fdf58093a210cb9d25926543" class="notion-text notion-text__content notion-semantic-string">Empirical studies provide concrete evidence regarding the impact of chunk retrieval sequence on the multi-step inference capabilities of LLMs within RAG systems. This section synthesizes key findings from various benchmarks and case studies.</p><span class="notion-heading__anchor" id="21740d70fdf580ad845cdef42ba8a4e7"></span><h2 id="block-21740d70fdf580ad845cdef42ba8a4e7" class="notion-heading notion-semantic-string"><strong>5.1 Case Studies on Chunk Order and Multi-Step Question Answering</strong></h2><p id="block-21740d70fdf580948a38d453a10fe70d" class="notion-text notion-text__content notion-semantic-string">Comparative studies between DOS RAG and Vanilla RAG consistently demonstrate the superior performance of DOS RAG across a range of benchmarks, including Bench, QuALITY, and NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This performance advantage is particularly notable when the retrieval budget is expanded to tens of thousands of tokens.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> For example, on the Bench dataset, DOS RAG achieved an accuracy of 93.1% at 30K tokens, significantly outperforming Vanilla RAG, which reached 87.8%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This consistent empirical outperformance of DOS RAG provides strong evidence that LLMs' multi-step reasoning capabilities are profoundly tied to the 
<em>narrative and structural coherence</em> of the input context, rather than merely the presence of highly relevant, but potentially fragmented, information. This validates the theoretical arguments for sequential processing preference.</p><p id="block-21740d70fdf58044bc06dbd3566e29b3" class="notion-text notion-text__content notion-semantic-string">Furthermore, these studies reveal that complex multi-stage RAG pipelines, such as ReadAgent and RAPTOR, often underperform simpler methods like DOS RAG, especially at moderate token budgets.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that the added complexity of multi-stage approaches yields diminishing returns when long-context LLMs can effectively incorporate relevant context in a single, well-ordered pass.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>20</mn></msup></mrow><annotation encoding="application/x-tex">^{20}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">20</span></span></span></span></span></span></span></span></span></span></span></span></span> However, there is a "sweet spot" for context length: DOS RAG's performance tends to plateau and even decline beyond a certain retrieval budget (e.g., 30K tokens).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span> This indicates that simply expanding the context window with more chunks can eventually introduce too much noise or irrelevant information, underscoring the importance of balancing recall with precision and effective filtering.</p><p id="block-21740d70fdf580c58320e68a6859b88f" class="notion-text notion-text__content notion-semantic-string">Research on premise order in reasoning tasks further supports the importance of sequence. Studies demonstrate that permuting the order of premises in deductive reasoning tasks can lead to a performance drop of over 30% in LLMs.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> LLMs consistently perform best when premises are aligned with the sequential steps of a ground truth proof.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">^{18}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">18</span></span></span></span></span></span></span></span></span></span></span></span></span> In the context of multi-hop question answering, reranking also plays a crucial role. Inverted context ordering, where the most relevant chunks are placed first, can lead to improvements in <code class="code language-none">correctness</code>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> When rerankers like BGE-M3 are combined with higher <code class="code language-none">retrieval@k</code> values, more "gold documents" (highly relevant chunks) are retained in the reranked set, enhancing performance for multi-hop questions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span> However, increasing <code class="code language-none">rerank@k</code> with a fixed <code class="code language-none">retrieval@k</code> can introduce higher variation in <code class="code language-none">correctness</code> scores, ranging from 1% to 25%.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>24</mn></msup></mrow><annotation encoding="application/x-tex">^{24}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">24</span></span></span></span></span></span></span></span></span></span></span></span></span></p><span class="notion-heading__anchor" id="21740d70fdf580889f7bc0535e95d69a"></span><h2 id="block-21740d70fdf580889f7bc0535e95d69a" class="notion-heading notion-semantic-string"><strong>5.2 Evaluation Benchmarks and Metrics</strong></h2><p id="block-21740d70fdf5806baa0cd10113df6c6c" class="notion-text notion-text__content notion-semantic-string">The field of RAG and LLM evaluation is maturing, evidenced by the proliferation of specialized benchmarks designed to assess complex reasoning capabilities. The MTI Bench, for instance, is specifically tailored to analyze Multi-Task Inference, distinguishing between tasks with sequential dependencies (Multi-Step subset) and those without (Multi-Part subset).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span> This benchmark has shown that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, can achieve significantly better performance (up to 12.4%) and speed (1.46 times faster) with Multi-Task Inference compared to Single-Task Inference, particularly for stronger models.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580cf802debf13bd62d5a" class="notion-text notion-text__content notion-semantic-string">Another important benchmark, ProcBench, is designed to evaluate multi-step reasoning by challenging LLMs with explicit instructions and questions that require strict adherence to provided steps.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> Its focus is on assessing the ability to follow step-by-step procedures, a critical skill for applications like automated decision-making and planning.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span> DataMorgana offers a novel approach for generating customizable synthetic benchmarks with single-hop and multi-hop QA pairs, utilized in challenges such as LiveRAG 2025.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span> For evaluating multi-modal RAG systems (spanning text, tables, and knowledge graphs), mmRAG provides a modular benchmark that assesses components beyond just generation, including query routing and retrieval <code class="code language-none">accuracy</code>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>35</mn></msup></mrow><annotation encoding="application/x-tex">^{35}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">35</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, RAGChecker is a fine-grained evaluation framework that incorporates diagnostic metrics for both retrieval and generation modules, demonstrating better correlations with human judgments.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>36</mn></msup></mrow><annotation encoding="application/x-tex">^{36}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">36</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf580d5978af3c7e30f6305" class="notion-text notion-text__content notion-semantic-string">These specialized benchmarks employ a variety of evaluation metrics. <code class="code language-none">Accuracy</code> is a common metric, used for instance in the evaluation of ChunkRAG on the PopQA dataset.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> For more nuanced assessments, metrics like <code class="code language-none">F1</code>, <code class="code language-none">BLEU-1</code>, <code class="code language-none">BLEU-4</code>, <code class="code language-none">ROUGE-L</code>, and <code class="code language-none">METEOR</code> are employed, particularly for tasks like NarrativeQA.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span> In challenges like LiveRAG, correctness and faithfulness scores are critical for evaluating the quality of generated answers.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> The proliferation of these specialized benchmarks signifies a maturing research field that recognizes the inadequacy of general QA metrics for assessing complex reasoning in RAG. This indicates a growing understanding that multi-step inference requires specific, granular evaluation beyond simple end-to-end accuracy, driving innovation in context organization. The evolution from general QA benchmarks to highly specialized ones, which distinguish sequential tasks <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>10</mn></msup></mrow><annotation encoding="application/x-tex">^{10}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span></span></span></span></span></span></span></span></span>, step-by-step procedure following <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>11</mn></msup></mrow><annotation encoding="application/x-tex">^{11}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">11</span></span></span></span></span></span></span></span></span></span></span></span></span>, and multi-hop questions <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>15</mn></msup></mrow><annotation encoding="application/x-tex">^{15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">15</span></span></span></span></span></span></span></span></span></span></span></span></span>, demonstrates that the research community is moving towards a more nuanced understanding of LLM capabilities within RAG. This shift implies that the design of RAG systems, particularly regarding chunk retrieval sequence, must now be optimized not just for general relevance, but for the specific demands of these complex reasoning tasks. The emphasis on metrics like "<code class="code language-none">correctness</code>" and "<!-- --><code class="code language-none">faithfulness</code>" further underscores the need for precise and contextually appropriate information delivery, which is directly influenced by chunk order.
<!-- --></p><p id="block-21740d70fdf580c2b190c8654f824624" class="notion-text notion-text__content notion-semantic-string"><strong>
Table 3: Overview of Benchmarks for RAG Multi-Step QA Evaluation</strong></p><div class="notion-table__wrapper"><table class="notion-table"><tbody><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Benchmark</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Primary Focus</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Key Features Relevant to Chunk Order/Multi-Step QA</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Key Findings Related to Chunk Order</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Relevant Snippets</strong></span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>MTI Bench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-Task Inference (sequential &amp; non-sequential sub-tasks)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Evaluates LLMs' ability to handle multiple instructions in one call; distinguishes Multi-Step (sequential) from Multi-Part (non-sequential) tasks.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Stronger LLMs show better performance (up to 12.4%) and speed (x1.46 faster) with Multi-Task Inference vs. Single-Task.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">10</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>ProcBench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-Step Reasoning &amp; Procedure Following</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Dataset designed to challenge LLMs with explicit instructions, requiring reliance solely on provided steps; various complexity levels.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Highlights critical gap in current assessments focusing exclusively on multi-step inference.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">11</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>Bench</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Evaluates performance under varying retrieval token budgets (1.5K to 40K tokens).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">DOS RAG consistently outperforms Vanilla RAG and multi-stage methods (e.g., 93.1% vs. 87.8% at 30K tokens). Performance plateaus/declines beyond 30K tokens.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>QuALITY</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering (narrative understanding)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Requires understanding underlying narrative rather than shallow pattern matching.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Full-document baseline outperforms all methods for shorter documents (6k-8k tokens); DOS RAG highest for retrieval-augmented methods up to 8K.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>NarrativeQA</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Long-Context Question Answering (narrative understanding)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Questions require understanding the underlying narrative.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">DOS RAG achieves superior results compared to ReadAgent and RAPTOR, often using fewer tokens. Consistent across multiple metrics (F1, BLEU, ROUGE, METEOR).</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">19</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>DataMorgana</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">QA-pair Generation (single-hop &amp; multi-hop)</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Creates highly customizable synthetic benchmarks; used in LiveRAG Challenge.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Used to evaluate impact of inverted context ordering and reranking on multi-hop QA performance.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">15</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>mmRAG</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Multi-modal RAG Evaluation</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Modular benchmark for text, tables, KGs; evaluates query routing and retrieval accuracy beyond generation.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Provides relevance labels to evaluate retrieval accuracy and dataset-level relevance for query routing.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">35</span></div></td></tr><tr style="color:var(--color-text-default)"><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string"><strong>ChunkRAG</strong></span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">LLM-driven Chunk Filtering</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Enhances RAG by evaluating and filtering retrieved information at the chunk level using LLM-based relevance scoring.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">Outperforms existing RAG models by significantly reducing hallucinations and improving factual accuracy on PopQA.</span></div></td><td style="min-width:120px;max-width:240px"><div class="notion-table__cell"><span class="notion-semantic-string">14</span></div></td></tr></tbody></table></div><span class="notion-heading__anchor" id="21740d70fdf580e0a67ddf16d23ac95b"></span><h1 id="block-21740d70fdf580e0a67ddf16d23ac95b" class="notion-heading notion-semantic-string"><strong>6. Optimizing Chunk Retrieval Sequence for Enhanced Multi-Step Reasoning</strong></h1><p id="block-21740d70fdf580bdab4cf02a41144b18" class="notion-text notion-text__content notion-semantic-string">Translating the empirical findings and observations into actionable strategies is essential for designing RAG systems that excel in multi-step inference tasks. Optimization requires a multi-faceted approach, considering chunking, reordering, and mitigation of detrimental factors.</p><span class="notion-heading__anchor" id="21740d70fdf58085858fd4791e4f101b"></span><h2 id="block-21740d70fdf58085858fd4791e4f101b" class="notion-heading notion-semantic-string"><strong>6.1 Best Practices for Chunking and Reordering</strong></h2><p id="block-21740d70fdf58011a43df811886b1920" class="notion-text notion-text__content notion-semantic-string">To optimize chunk retrieval sequence, a primary focus must be placed on <strong>prioritizing contextual coherence</strong>. For multi-step reasoning, chunking strategies should aim to preserve logical units and narrative flow, rather than simply adhering to fixed sizes. Recursive-based chunking, sentence-based chunking, and particularly document structure-based chunking (as exemplified by DOS RAG) are highly beneficial for maintaining this crucial context.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>13</mn></msup></mrow><annotation encoding="application/x-tex">^{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">13</span></span></span></span></span></span></span></span></span></span></span></span></span> Given its consistent outperformance across various benchmarks, <strong>adopting DOS RAG as a baseline</strong> is strongly recommended, especially when working with long-context LLMs and tasks that demand narrative understanding.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>19</mn></msup></mrow><annotation encoding="application/x-tex">^{19}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">19</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf5800bb60debab93527a57" class="notion-text notion-text__content notion-semantic-string">While initial retrieval provides a set of relevant chunks, a <strong>strategic reranking</strong> step is indispensable for refining the order and reducing noise. Cross-encoders offer high precision in this regard, while multi-vector rerankers provide a balance between performance and efficiency. For deeper relevance scoring, fine-tuned LLM rerankers and LLM-as-a-judge approaches can be employed, though their associated latency must be carefully considered.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>22</mn></msup></mrow><annotation encoding="application/x-tex">^{22}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">22</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, implementing <strong>inverted context ordering</strong>, where the most relevant (reranked) chunks are placed immediately before the query, has been shown to improve correctness in multi-hop QA tasks.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58049bce1f1a47e2d8051" class="notion-text notion-text__content notion-semantic-string">Optimizing chunk retrieval sequence is not a standalone step but requires a holistic approach, integrating intelligent chunking, robust retrieval, and strategic reranking. The most effective practice involves a dynamic balance between preserving the original document structure for narrative flow and leveraging reranking for query-specific relevance. The various studies present different techniques for chunking and reordering. The key understanding is that these techniques are not mutually exclusive but rather complementary. For instance, while DOS RAG emphasizes maintaining the original structure <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>20</mn></msup></mrow><annotation encoding="application/x-tex">^{20}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">20</span></span></span></span></span></span></span></span></span></span></span></span></span>, effective reranking can still improve the 
<em>selection</em> of which chunks to include and their final placement within that structure (e.g., inverted context ordering for the most relevant ones).<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span> This suggests that a truly optimized system might involve chunking based on logical units, retrieving a larger initial set, applying a reranker, and then finally reordering the top-K chunks according to their original document sequence or a query-specific optimal order. This integrated view highlights the need for a pipeline approach rather than isolated optimization efforts.</p><span class="notion-heading__anchor" id="21740d70fdf580e49c36f4912bd6ac93"></span><h2 id="block-21740d70fdf580e49c36f4912bd6ac93" class="notion-heading notion-semantic-string"><strong>6.2 Strategies for Mitigating Positional Bias and Distraction</strong></h2><p id="block-21740d70fdf580e895f8f04cd3bdbc49" class="notion-text notion-text__content notion-semantic-string">To effectively mitigate positional bias and the detrimental impact of distracting information, RAG systems must focus on proactive measures. First, efforts should concentrate on developing <strong>retrievers that not only maximize recall but also minimize the retrieval of highly distracting passages</strong>.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> This is crucial because stronger retrievers can inadvertently bring more harmful distractors into the context. Second, <strong>robust LLM fine-tuning</strong> with carefully selected "hard distracting passages" can significantly increase the LLM's accuracy and resilience against noise.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span><strong> </strong>Third, implementing <strong>LLM-driven chunk filtering</strong> (e.g., ChunkRAG) is a powerful strategy to evaluate and filter retrieved information at the chunk level, ensuring that only pertinent chunks are utilized. This directly reduces hallucinations and improves factual accuracy.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Fourth, for complex multi-step queries, <strong>query rewriting or decomposition</strong> into simpler sub-queries can improve retrieval accuracy and reduce the likelihood of fetching irrelevant information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Finally, active <strong>context window management</strong> is vital to avoid overload. Providing only the most relevant context reduces the cognitive load on the LLM, enhancing selection accuracy and reducing hallucinations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>30</mn></msup></mrow><annotation encoding="application/x-tex">^{30}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">30</span></span></span></span></span></span></span></span></span></span></span></span></span> Identifying the "sweet spot" for context length, where recall is maximized without introducing excessive noise, is also paramount.<!-- --><span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>17</mn></msup></mrow><annotation encoding="application/x-tex">^{17}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">17</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p id="block-21740d70fdf58025a2fec1c13e30b145" class="notion-text notion-text__content notion-semantic-string">Mitigating positional bias and the distracting effect shifts the focus from merely reacting to retrieved chunks to proactively ensuring the <em>quality and focus</em> of the context before it reaches the LLM. This implies that pre-processing and intelligent filtering are as crucial as the retrieval itself. The studies indicate that positional bias and distracting information are inherent challenges.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">^{25}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span></span> Simply reordering <em>after</em> retrieval is often insufficient to address these issues.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>26</mn></msup></mrow><annotation encoding="application/x-tex">^{26}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">26</span></span></span></span></span></span></span></span></span></span></span></span></span> Therefore, the solution must involve proactive measures. This includes improving the <em>initial retrieval</em> to be less prone to fetching distractors, and then employing <em>strong filtering</em> (like ChunkRAG) to eliminate noise before it ever reaches the LLM's context window.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>14</mn></msup></mrow><annotation encoding="application/x-tex">^{14}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">14</span></span></span></span></span></span></span></span></span></span></span></span></span> Furthermore, making the LLM itself more robust through fine-tuning with challenging examples <span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>28</mn></msup></mrow><annotation encoding="application/x-tex">^{28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span> creates a defense-in-depth strategy. This multi-layered approach is essential for reliable multi-step inference.</p><span class="notion-heading__anchor" id="21740d70fdf580b3bb55f18aef19ec66"></span><h2 id="block-21740d70fdf580b3bb55f18aef19ec66" class="notion-heading notion-semantic-string"><strong>6.3 Advanced Techniques for Multi-Hop Reasoning</strong></h2><p id="block-21740d70fdf580f4b7a0d4c4313da49c" class="notion-text notion-text__content notion-semantic-string">Addressing multi-step reasoning effectively in RAG necessitates moving beyond simple retrieve-and-generate pipelines towards more dynamic, iterative, and potentially graph-aware architectures. For multi-hop reasoning, which intrinsically requires connecting information across multiple sources or steps, <strong>iterative retrieval</strong> becomes crucial. This involves employing multi-round question refinement processes, decomposing main questions into sub-queries, generating answers for each, and iteratively retrieving additional context as needed.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">^{16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">16</span></span></span></span></span></span></span></span></span></span></span></span></span><strong> Adaptive retrieval</strong> mechanisms that dynamically determine retrieval necessity and balance performance gains with inference speed also represent a significant advancement.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>38</mn></msup></mrow><annotation encoding="application/x-tex">^{38}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">38</span></span></span></span></span></span></span></span></span></span></span></span></span> The integration of structured knowledge, such as <strong>graph-based RAG</strong> (e.g., knowledge graphs), can enrich the learning context, particularly for complex reasoning over heterogeneous knowledge sources.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>35</mn><mo separator="true">,</mo><mn>39</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{35,39}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">35</span><span class="mpunct mtight">,</span><span class="mord mtight">39</span></span></span></span></span></span></span></span></span></span></span></span></span> This approach facilitates multi-hop reasoning by explicitly modeling relationships between entities, which is often difficult to capture through purely semantic similarity. Finally, the use of <strong>prompt-based reasoning chains</strong> like Chain-of-Thought (CoT), Tree-of-Thought (ToT), or Graph-of-Thought (GoT) can explicitly model logical chains and guide the LLM's reasoning process step-by-step, enhancing its ability to perform complex deductions.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">^6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span></span> These architectural advancements demonstrate a recognition that multi-step reasoning demands a more sophisticated and interactive approach to information access and organization.<!-- --></p><span class="notion-heading__anchor" id="21740d70fdf58057bc2bc6c92d18067c"></span><h1 id="block-21740d70fdf58057bc2bc6c92d18067c" class="notion-heading notion-semantic-string"><strong>7. Conclusion and Future Directions</strong></h1><p id="block-21740d70fdf580c9a5c9cbbe9a3723c1" class="notion-text notion-text__content notion-semantic-string">The analysis presented in this report underscores that the chunk retrieval sequence is a critical determinant of a Large Language Model's multi-step inference performance within Retrieval-Augmented Generation systems. The findings consistently highlight the significant benefits of preserving the original document structure, as demonstrated by DOS RAG, which often outperforms relevance-based sorting by maintaining narrative continuity crucial for complex reasoning. The nuanced role of reranking is also evident, as it refines relevance but must be balanced against computational overhead. Furthermore, the pervasive challenges of positional bias and the detrimental impact of distracting information necessitate proactive mitigation strategies.</p><p id="block-21740d70fdf580059d5be7ecaceed316" class="notion-text notion-text__content notion-semantic-string">The implications for RAG system design are clear: a holistic approach is required. This involves considering not only semantic relevance but also contextual coherence, the cognitive load imposed on the LLM, and robustness to noise. Simply increasing the context window size or the quantity of retrieved information does not guarantee improved multi-step inference; instead, the quality and conciseness of the retrieved context directly impact the LLM's processing efficiency and reasoning accuracy.</p><p id="block-21740d70fdf580ca8329d34ae124a4ac" class="notion-text notion-text__content notion-semantic-string">Future research and development should focus on several promising directions:</p><ul class="notion-bulleted-list"><li id="block-21740d70fdf580bb9f60cd579f054e8f" class="notion-list-item notion-semantic-string"><strong>Adaptive Retrieval Architectures:</strong> Further development of systems that can dynamically adjust retrieval strategies and context presentation based on the complexity of the query and the current state of the LLM.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58072ab0fc215757eb48b" class="notion-list-item notion-semantic-string"><strong>Real-time Retrieval Integration:</strong> Enhancing the seamless and low-latency integration of retrieval within LLM inference loops to support more interactive and dynamic applications.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">^9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf5802cb5a5e4c8136d32b4" class="notion-list-item notion-semantic-string"><strong>Structured Reasoning over Multi-Hop Evidence:</strong> Continued investigation into how RAG systems can better facilitate complex, multi-hop reasoning, potentially through explicit graph-based representations or advanced prompting techniques that guide logical derivations.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>6</mn><mo separator="true">,</mo><mn>39</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{6,39}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">6</span><span class="mpunct mtight">,</span><span class="mord mtight">39</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580438856f1a03608dfd0" class="notion-list-item notion-semantic-string"><strong>Robustness to Adversarial Inputs:</strong> Developing RAG systems that are more resilient to noisy or adversarial retrieved content, ensuring reliable performance in challenging environments.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>8</mn><mo separator="true">,</mo><mn>28</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{8,28}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span><span class="mpunct mtight">,</span><span class="mord mtight">28</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf58012ab78f973a8a870ee" class="notion-list-item notion-semantic-string"><strong>Cross-Modal and Multi-Lingual RAG:</strong> Expanding research to encompass multi-modal data (e.g., images, audio, video) and multi-lingual contexts, as current benchmarks are largely single-modal and English-centric.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mn>4</mn><mo separator="true">,</mo><mn>35</mn><mo separator="true">,</mo><mn>40</mn></mrow></msup></mrow><annotation encoding="application/x-tex">^{4,35,40}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mpunct mtight">,</span><span class="mord mtight">35</span><span class="mpunct mtight">,</span><span class="mord mtight">40</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li id="block-21740d70fdf580ba9d25c88f6cfa1e49" class="notion-list-item notion-semantic-string"><strong>Evaluation Methodologies:</strong> Continued refinement of evaluation frameworks and benchmarks to more accurately capture the nuances of multi-step inference and the quality of contextual information.<span id="" class="notion-equation notion-equation__inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">^9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p id="block-21740d70fdf580aa9d37d06095234106" class="notion-text notion-text__content notion-semantic-string">These future directions underscore the ongoing evolution of RAG systems, moving towards more intelligent, adaptive, and robust architectures capable of supporting increasingly sophisticated LLM applications.</p><div id="block-21740d70fdf580388756d009905ff072" class="notion-toggle closed notion-toggle-heading-1"><div class="notion-toggle__summary"><div class="notion-toggle__trigger"><div class="notion-toggle__trigger_icon"><span></span></div></div><span class="notion-heading__anchor" id="21740d70fdf580388756d009905ff072"></span><h1 id="block-21740d70fdf580388756d009905ff072" class="notion-heading toggle notion-semantic-string"><strong>References</strong></h1></div><div class="notion-toggle__content"><ol type="1" class="notion-numbered-list"><li id="block-21740d70fdf580cbb88acbb7fa5d007e" class="notion-list-item notion-semantic-string">IBM. (n.d.). 
<em>What Are Large Language Models (LLMs)?</em>. Retrieved from https://www.ibm.com/think/topics/large-language-models#:~:text=LLMs%20consist%20of%20multiple%20layers,specific%20parts%20of%20data%20sets.</li><li id="block-21740d70fdf580a2af76f5bd3e1d8ed2" class="notion-list-item notion-semantic-string">Kasneci, E., et al. (2023). 
<em>Editorial  The Use of Large Language Models in Science: Opportunities and Challenges.</em> PMC. Retrieved from https://pmc.ncbi.nlm.nih.gov/articles/PMC10485814/.</li><li id="block-21740d70fdf580dcba69e2d2b53e2344" class="notion-list-item notion-semantic-string">IBM. (2023, November 2). 
<em>What are Large Language Models (LLMs)?</em>. Retrieved from https://www.ibm.com/think/topics/large-language-models.</li><li id="block-21740d70fdf58009bdead36180daf5d1" class="notion-list-item notion-semantic-string">Google Cloud. (n.d.). 
<em>What is Retrieval-Augmented Generation (RAG)?</em>. Retrieved from https://cloud.google.com/use-cases/retrieval-augmented-generation.</li><li id="block-21740d70fdf580ff9eb4c885d46d800a" class="notion-list-item notion-semantic-string">Zhang, Y., et al. (2025). 
<em>Empowering LLMs with Logical Reasoning: A Comprehensive Survey</em>. arXiv. Retrieved from https://arxiv.org/html/2502.15652v2.</li><li id="block-21740d70fdf58036811defbe9753cd2e" class="notion-list-item notion-semantic-string">Zhang, Y., et al. (2025). 
<em>Empowering LLMs with Logical Reasoning: A Comprehensive Survey</em>. arXiv. Retrieved from https://arxiv.org/html/2502.15652v3.</li><li id="block-21740d70fdf5806da33bea329cf410a0" class="notion-list-item notion-semantic-string">AWS. (n.d.). 
<em>What is RAG (Retrieval-Augmented Generation)?</em>. Retrieved from https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=Retrieval%2DAugmented%20Generation%20(RAG),sources%20before%20generating%20a%20response.</li><li id="block-21740d70fdf580a6be25eaf64652eaae" class="notion-list-item notion-semantic-string">Sharma, C. (2025). 
<em>Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers</em>. arXiv. Retrieved from https://arxiv.org/html/2506.00054v1.</li><li id="block-21740d70fdf5804dbeeeffd1f4ae88c4" class="notion-list-item notion-semantic-string">Sharma, C. (2025). 
<em>Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers</em>. arXiv. Retrieved from <a href="https://arxiv.org/abs/2506.00054" class="notion-link link" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2506.00054</a>.</li><li id="block-21740d70fdf580259622f329546dfb2f" class="notion-list-item notion-semantic-string">Xu, K., et al. (2024). 
<em>Multi-Task Inference: Do LLMs Hold the Capability to Handle Multiple Instructions Simultaneously?</em>. arXiv. Retrieved from https://arxiv.org/html/2402.11597v2.</li><li id="block-21740d70fdf58095905ac7ad4b41ae74" class="notion-list-item notion-semantic-string">About AI. (2024, May 22). 
<em>How Well Do Large Language Models (LLMs) Actually Handle Multi-Step Reasoning?</em>. Medium. Retrieved from https://medium.com/about-ai/how-well-do-large-language-models-llms-actually-handle-multi-step-reasoning-83ce37f7fc32.</li><li id="block-21740d70fdf5807ab0f4d6304119f55e" class="notion-list-item notion-semantic-string">F22 Labs. (n.d.). 
<em>7 Chunking Strategies in RAG You Need To Know</em>. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/#:~:text=Retrieval%20Augmented%20Generation%20(RAG)%20enhances,for%20faster%20retrieval%20and%20processing.</li><li id="block-21740d70fdf580679df9f9f2652d4f26" class="notion-list-item notion-semantic-string">F22 Labs. (n.d.). 
<em>7 Chunking Strategies in RAG You Need To Know</em>. Retrieved from https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/.</li><li id="block-21740d70fdf580c684aaf266ad8e32a5" class="notion-list-item notion-semantic-string">Mallen, E., et al. (2024). 
<em>ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems</em>. arXiv. Retrieved from https://arxiv.org/html/2410.19572v2.</li><li id="block-21740d70fdf5807d9c4dfa847b6cef28" class="notion-list-item notion-semantic-string">Filice, S., et al. (2025). 
<em>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</em>. arXiv. Retrieved from https://arxiv.org/html/2506.14412v1.</li><li id="block-21740d70fdf58086b271ef77f650390a" class="notion-list-item notion-semantic-string">Filice, S., et al. (2025). 
<em>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</em>. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.14412.</li><li id="block-21740d70fdf58078b78cdffcd8fd875e" class="notion-list-item notion-semantic-string">SuperAnnotate. (n.d.). 
<em>RAG vs. Long Context LLMs</em>. Retrieved from https://www.superannotate.com/blog/rag-vs-long-context-llms.</li><li id="block-21740d70fdf58059a4e7c03e14bd6215" class="notion-list-item notion-semantic-string">Liu, Y., et al. (2024). 
<em>The Impact of Premise Order on LLM Reasoning</em>. arXiv. Retrieved from https://arxiv.org/html/2402.08939v1.</li><li id="block-21740d70fdf580d08492fb9b92c328c8" class="notion-list-item notion-semantic-string">Cuconasu, F., et al. (2025, June 4). 
<em>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</em>. arXiv. Retrieved from https://www.arxiv.org/pdf/2506.03989.</li><li id="block-21740d70fdf5808ab1dbe83434ff1e1f" class="notion-list-item notion-semantic-string">Cuconasu, F., et al. (2025). 
<em>Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models</em>. arXiv. Retrieved from https://arxiv.org/html/2506.03989v1.</li><li id="block-21740d70fdf5809ba01edf97c0736c92" class="notion-list-item notion-semantic-string">DataCamp. (n.d.). 
<em>Advanced RAG Techniques</em>. Retrieved from https://www.datacamp.com/blog/rag-advanced.</li><li id="block-21740d70fdf580f999e2db1523518c77" class="notion-list-item notion-semantic-string">Analytics Vidhya. (2025, March 28). 
<em>Comprehensive Guide on Reranker for RAG</em>. Retrieved from https://www.analyticsvidhya.com/blog/2025/03/reranker-for-rag/.</li><li id="block-21740d70fdf5807ba8dbdede8f9371bd" class="notion-list-item notion-semantic-string">Galileo AI. (n.d.). 
<em>Mastering RAG: How to Select a Reranking Model</em>. Retrieved from https://galileo.ai/blog/mastering-rag-how-to-select-a-reranking-model.</li><li id="block-21740d70fdf580ffbfd5e1c2b853d383" class="notion-list-item notion-semantic-string">Filice, S., et al. (2025). 
<em>RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition</em>. arXiv. Retrieved from https://arxiv.org/html/2506.14412.</li><li id="block-21740d70fdf580b1aa1afd59d3f6ac19" class="notion-list-item notion-semantic-string">Cuconasu, F., et al. (2025). 
<em>Do RAG Systems Suffer From Positional Bias?</em>. arXiv. Retrieved from https://arxiv.org/html/2505.15561v1.</li><li id="block-21740d70fdf58020b6c2f23ce562d30d" class="notion-list-item notion-semantic-string">Cuconasu, F., et al. (2025). 
<em>Do RAG Systems Suffer From Positional Bias?</em>. arXiv. Retrieved from https://arxiv.org/abs/2505.15561.</li><li id="block-21740d70fdf58084b30fdb94d2ea101e" class="notion-list-item notion-semantic-string">Cuconasu, F., et al. (2025). 
<em>Investigate positional bias in RAG systems, including the 'lost-in-the-middle' effect and impact of distracting passages</em>. arXiv. Retrieved from https://arxiv.org/pdf/2505.15561.</li><li id="block-21740d70fdf58077896bcd067145878b" class="notion-list-item notion-semantic-string">Amiraz, C., et al. (2025, May 11). 
<em>Describe the 'distracting effect' of irrelevant passages in RAG and its impact on LLM accuracy</em>. arXiv. Retrieved from https://arxiv.org/abs/2505.06914.</li><li id="block-21740d70fdf58073ae6dd2cc789c3e2f" class="notion-list-item notion-semantic-string">Amiraz, C., et al. (2025). 
<em>The Distracting Effect: Understanding Irrelevant Passages in RAG</em>. arXiv. Retrieved from https://arxiv.org/html/2505.06914v1.</li><li id="block-21740d70fdf580968828d188c0c8ee1d" class="notion-list-item notion-semantic-string">Li, Y., et al. (2025). 
<em>RAG-MCP: Retrieval-Augmented Generation for Model Context Protocol</em>. arXiv. Retrieved from https://arxiv.org/html/2505.03275v1.</li><li id="block-21740d70fdf5807897acec3822f47bf9" class="notion-list-item notion-semantic-string">Zhang, M., et al. (2025). 
<em>Cognitive-Aware LLM Streaming</em>. arXiv. Retrieved from https://arxiv.org/html/2504.17999v1.</li><li id="block-21740d70fdf5802486baf24d75884d9a" class="notion-list-item notion-semantic-string">Li, Y., et al. (2025). 
<em>How does supplying only relevant context in RAG reduce cognitive load and improve LLM decision making?</em>. arXiv. Retrieved from https://arxiv.org/pdf/2505.03275.</li><li id="block-21740d70fdf5809e9c01f527c2c10ef8" class="notion-list-item notion-semantic-string">Wang, Y., et al. (2024). 
<em>InfLLM: Enabling LLMs to Understand Extremely Long Sequences Without Any Fine-tuning</em>. NeurIPS. Retrieved from https://neurips.cc/virtual/2024/poster/94480.</li><li id="block-21740d70fdf5800facf1d89be0a5874c" class="notion-list-item notion-semantic-string">Xu, J., et al. (2025). 
<em>Long Context vs. RAG for LLMs: An Evaluation and Revisits</em>. arXiv. Retrieved from https://arxiv.org/html/2501.01880v1.</li><li id="block-21740d70fdf580c89d4df57ac8cd0fd7" class="notion-list-item notion-semantic-string">Chen, C., et al. (2025). 
<em>mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs</em>. arXiv. Retrieved from https://arxiv.org/html/2505.11180v1.</li><li id="block-21740d70fdf58026936ddc81616067a1" class="notion-list-item notion-semantic-string">Zhang, Z., et al. (2024). 
<em>RAGChecker: A Fine-grained Evaluation Framework for Retrieval-Augmented Generation</em>. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html.</li><li id="block-21740d70fdf580e3a8fae2409c074f7a" class="notion-list-item notion-semantic-string">Diamant, N. (n.d.). 
<em>RAG_Techniques</em>. GitHub. Retrieved from https://github.com/NirDiamant/RAG_Techniques.</li><li id="block-21740d70fdf580449387e309ff20589a" class="notion-list-item notion-semantic-string">Mallen, E., et al. (2024). 
<em>Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source Large Language Models</em>. ACL Anthology. Retrieved from https://aclanthology.org/2024.findings-emnlp.831/.</li><li id="block-21740d70fdf5802b9c93c4c6f0a88a9b" class="notion-list-item notion-semantic-string">Li, H., et al. (2024). 
<em>RAGRAPH: A General Retrieval-Augmented Graph Learning Framework</em>. NeurIPS. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2024/file/34d6c7090bc5af0b96aeaf92fa074899-Paper-Conference.pdf.</li><li id="block-21740d70fdf5800a8c63e200b143bcc2" class="notion-list-item notion-semantic-string">Reddit. (2024, March 17). 
<em>Advanced Chunking/Retrieving Strategies for Legal Documents</em>. Retrieved from https://www.reddit.com/r/Rag/comments/1jdi4sg/advanced_chunkingretrieving_strategies_for_legal/.</li></ol></div></div><div id="block-21740d70fdf580fe8ae4f3d4e38c4ec2" class="notion-text"></div></article></main>